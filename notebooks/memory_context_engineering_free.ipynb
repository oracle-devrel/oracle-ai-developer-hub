{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "159c65b0",
   "metadata": {},
   "source": [
    "# Memory and Context Engineering for AI Agents with Oracle AI Database, Langchain and Tavily\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5340125d",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://img.shields.io/badge/Open%20in-Colab-F9AB00?style=flat-square&logo=googlecolab)](https://colab.research.google.com/github/oracle-devrel/oracle-ai-developer-hub/blob/main/notebooks/memory_context_engineering_free.ipynb)\n",
    "\n",
    "In this notebook, you'll learn how to engineer memory systems that give AI agents the ability to remember, learn, and adapt across conversations. \n",
    "Moving beyond simple RAG, we implement a complete **Memory Manager** with six distinct memory typesâ€”each serving a specific cognitive function.\n",
    "\n",
    "> **Note:** This is the **Oracle Database Free** edition of the notebook. It uses a local Docker container running [Oracle Database Free](https://www.oracle.com/database/free/get-started/) â€” no cloud account or wallet required. For the Oracle Autonomous Database (ATP) version, see `memory_context_engineering_agents.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d6ba7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "| Memory Type | Purpose | Storage |\n",
    "|-------------|---------|---------|\n",
    "| **Conversational** | Chat history per thread | SQL Table |\n",
    "| **Knowledge Base** | Searchable documents & facts | Vector Store |\n",
    "| **Workflow** | Learned action patterns | Vector Store |\n",
    "| **Toolbox** | Dynamic tool definitions | Vector Store |\n",
    "| **Entity** | People, places, systems extracted from context | Vector Store |\n",
    "| **Summary** | Compressed context for long conversations | Vector Store |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65027f9",
   "metadata": {},
   "source": [
    "\n",
    "## Key Concepts Covered\n",
    "\n",
    "- **Memory Engineering**: Design patterns for agent memory systems\n",
    "- **Context Engineering**: Techniques for optimizing what goes into the LLM context\n",
    "- **Context Window Management**: Monitor usage, auto-summarize at thresholds\n",
    "- **Just-in-Time Retrieval**: Compact summaries with on-demand expansion\n",
    "- **Dynamic Tool Calling**: Semantic tool discovery and execution\n",
    "- **Entity Extraction**: LLM-powered entity recognition and storage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8775610",
   "metadata": {},
   "source": [
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.10+\n",
    "- Docker Desktop or Docker Engine\n",
    "- Oracle Database Free container (`ghcr.io/gvenzl/oracle-free:23.26.0`)\n",
    "- HuggingFace account (free) â€” for LLM inference via HF Inference API\n",
    "- Tavily API key (for web search tool)\n",
    "- *(No HF token?)* The notebook auto-downloads a lightweight Qwen3-0.6B model from HuggingFace and runs it locally\n",
    "\n",
    "### How to get a free HuggingFace token\n",
    "\n",
    "1. Sign up at [huggingface.co/join](https://huggingface.co/join) (free, no credit card)\n",
    "2. Go to [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "3. Click **\"Create new token\"** â†’ select **\"Read\"** access â†’ copy the token\n",
    "\n",
    "### How to get a Tavily API key\n",
    "\n",
    "1. Sign up at [app.tavily.com](https://app.tavily.com/) (free tier available)\n",
    "2. Copy your API key from the dashboard\n",
    "\n",
    "> **No cloud account required.** This notebook runs entirely locally using Oracle Database Free in Docker.\n",
    "\n",
    "## By the End\n",
    "You'll have a reusable `MemoryLayer` class and agent loop that demonstrates how modern AI agents maintain context, learn from interactions, and manage information across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b243d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU langchain-oracledb sentence-transformers langchain-openai langchain tavily-python huggingface_hub openai transformers torch sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a1b2c3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’» Local environment â€” connecting to Oracle DB at 127.0.0.1\n",
      "   DSN: 127.0.0.1:1521/FREEPDB1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "IN_COLAB = \"COLAB_RELEASE_TAG\" in os.environ\n",
    "\n",
    "# ============================================================\n",
    "# DATABASE CONNECTION CONFIGURATION\n",
    "# ============================================================\n",
    "# If running in Colab, point to your remote Oracle Database Free\n",
    "# instance. If running locally, use 127.0.0.1 (default).\n",
    "#\n",
    "# To use a remote instance you need:\n",
    "#   1. Oracle Database Free container running on a server\n",
    "#   2. Port 1521 open in the server's firewall / security list\n",
    "#   3. The VECTOR user created (run setup() locally first)\n",
    "# ============================================================\n",
    "\n",
    "if IN_COLAB:\n",
    "    ORACLE_HOST = \"129.213.76.89\"   # <-- Replace with your server's public IP\n",
    "    print(f\"\\u2601\\ufe0f  Colab detected \\u2014 connecting to remote Oracle DB at {ORACLE_HOST}\")\n",
    "else:\n",
    "    ORACLE_HOST = \"127.0.0.1\"       # Local Docker container\n",
    "    print(f\"\\U0001f4bb Local environment \\u2014 connecting to Oracle DB at {ORACLE_HOST}\")\n",
    "\n",
    "ORACLE_DSN = f\"{ORACLE_HOST}:1521/FREEPDB1\"\n",
    "print(f\"   DSN: {ORACLE_DSN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393bf345",
   "metadata": {},
   "source": [
    "# Local Installation of Oracle AI Database via Docker [Memory Core]\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792a6485",
   "metadata": {},
   "source": [
    "This section walks you through setting up **Oracle Database Free** locally using Docker. Oracle Database Free is a converged database that combines relational, document, graph, and vector data in a single engineâ€”making it ideal for AI applications that need semantic search, embeddings storage, and vector similarity queries.\n",
    "\n",
    "Before running the setup code, ensure you have Docker installed and pull the Oracle Free container image with vector search capabilities. This image includes Oracle AI Database Free with built-in vector operations.\n",
    "\n",
    "```bash\n",
    "docker pull ghcr.io/gvenzl/oracle-free:23.26.0\n",
    "docker run -d --name oracle-free -p 1521:1521 -p 5500:5500 -e ORACLE_PASSWORD='OraclePwd_2025' ghcr.io/gvenzl/oracle-free:23.26.0\n",
    "docker logs -f oracle-free\n",
    "```\n",
    "\n",
    "First-time startup may take a few minutes as Oracle initializes the database files. Once ready, the code below will establish a connection to the running container.\n",
    "\n",
    "**What you'll do:**\n",
    "1. Pull and run the Oracle Database Docker container\n",
    "2. Establish a connection from Python using `oracledb`\n",
    "3. Create a dedicated user for vector operations\n",
    "\n",
    "This local setup gives you a fully functional Oracle database for development and testing without needing cloud infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58676655",
   "metadata": {},
   "source": [
    "### Installing Oracle AI Database via Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c745a8",
   "metadata": {},
   "source": [
    "For this notebook we will be using a local installation of [Oracle AI Database](https://www.oracle.com/database/free/get-started/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92b14d9",
   "metadata": {},
   "source": [
    "1. Install & start Docker. Docker Desktop (Mac/Windows) or Docker Engine (Linux). Make sure it's running.\n",
    "    - If installed with Docker Engine, run from terminal ```open /Applications/Docker.app```\n",
    "2. Pull the [gvenzl Oracle Free image](https://github.com/gvenzl/oci-oracle-free)\n",
    "3. Run a container with the oracle image\n",
    "\n",
    "    ```\n",
    "      docker pull ghcr.io/gvenzl/oracle-free:23.26.0\n",
    "      docker run -d \\\n",
    "        --name oracle-free \\\n",
    "        -p 1521:1521 -p 5500:5500 \\\n",
    "        -e ORACLE_PASSWORD=OraclePwd_2025 \\\n",
    "        ghcr.io/gvenzl/oracle-free:23.26.0\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95d25a7",
   "metadata": {},
   "source": [
    "> ðŸš« **Troubleshoot**  \n",
    "> If you see the error:  \n",
    "> *`docker: Error response from daemon: Conflict. The container name \"/oracle-full\" is already in use by container ... You have to remove (or rename) that container to be able to reuse that name.`*  \n",
    ">\n",
    "> ðŸ§© **Fix:**  \n",
    "> - Remove the existing container:  \n",
    ">   ```bash\n",
    ">   docker rm oracle-free\n",
    ">   ```  \n",
    "> - Then re-run your Docker command from **Step 3** to start a new container.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88463dc3",
   "metadata": {},
   "source": [
    "### ðŸš€ One-Click Database Setup\n",
    "\n",
    "The cell below handles **everything automatically**:\n",
    "- âœ… Checks if Docker is running\n",
    "- âœ… Checks if Oracle container exists and is healthy\n",
    "- âœ… Waits for database to be ready (with progress indicator)\n",
    "- âœ… Fixes the listener for ARM Macs (Apple Silicon)\n",
    "- âœ… Creates the VECTOR user with proper privileges\n",
    "- âœ… Tests the connection\n",
    "\n",
    "**Just run the cell below and wait for the âœ… success message!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fb7ae8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import sys\n",
    "import oracledb\n",
    "\n",
    "IMAGE = \"ghcr.io/gvenzl/oracle-free:23.26.0\"\n",
    "CONTAINER = \"oracle-free\"\n",
    "PDB = \"FREEPDB1\"\n",
    "PORT = 1521\n",
    "\n",
    "def sh(*cmd, check=True, capture=True, text=True):\n",
    "    return subprocess.run(cmd, check=check, capture_output=capture, text=text)\n",
    "\n",
    "def ensure_running(container=CONTAINER, image=IMAGE, oracle_pwd=None):\n",
    "    \"\"\"Ensure the Oracle Free container is running, creating it if needed.\"\"\"\n",
    "    # Docker reachable?\n",
    "    r = sh(\"docker\", \"info\", check=False)\n",
    "    if r.returncode != 0:\n",
    "        raise RuntimeError(\"Docker is not running. Please start Docker and try again.\")\n",
    "\n",
    "    # Container exists?\n",
    "    r = sh(\"docker\", \"ps\", \"-a\", \"--filter\", f\"name=^{container}$\", \"--format\", \"{{.Names}}\", check=False)\n",
    "    if r.stdout.strip() == container:\n",
    "        r2 = sh(\"docker\", \"inspect\", \"-f\", \"{{.State.Running}}\", container, check=False)\n",
    "        if r2.stdout.strip().lower() != \"true\":\n",
    "            print(f\"Starting existing container '{container}'...\")\n",
    "            sh(\"docker\", \"start\", container)\n",
    "        else:\n",
    "            print(f\"Container '{container}' is already running.\")\n",
    "        return\n",
    "\n",
    "    # Otherwise create it\n",
    "    if not oracle_pwd:\n",
    "        raise ValueError(\"Set oracle_pwd (SYS/SYSTEM password) to create a fresh container.\")\n",
    "    print(f\"Creating new container '{container}' from {image}...\")\n",
    "    r = sh(\n",
    "        \"docker\", \"run\", \"-d\",\n",
    "        \"--name\", container,\n",
    "        \"-p\", f\"{PORT}:1521\",\n",
    "        \"-p\", \"5500:5500\",\n",
    "        \"--shm-size=1g\",\n",
    "        \"-e\", f\"ORACLE_PASSWORD={oracle_pwd}\",\n",
    "        image,\n",
    "        check=False\n",
    "    )\n",
    "    if r.returncode != 0:\n",
    "        err = (r.stderr or \"\").strip()\n",
    "        if \"port is already allocated\" in err or \"address already in use\" in err:\n",
    "            raise RuntimeError(\n",
    "                f\"Port {PORT} is already in use. Another container may be running on that port.\\n\"\n",
    "                f\"Try: docker ps  (to see what is using the port)\\n\"\n",
    "                f\"     docker stop <name> && docker rm <name>  (to free the port)\"\n",
    "            )\n",
    "        elif \"is already in use\" in err:\n",
    "            raise RuntimeError(\n",
    "                f\"Container name '{container}' is already in use by a stopped container.\\n\"\n",
    "                f\"Try: docker rm {container}  (then re-run this cell)\"\n",
    "            )\n",
    "        else:\n",
    "            raise RuntimeError(f\"docker run failed (exit {r.returncode}):\\n{err}\")\n",
    "    print(f\"Container '{container}' created.\")\n",
    "\n",
    "def wait_ready(container=CONTAINER, pdb=PDB, timeout_s=600):\n",
    "    \"\"\"Deterministic readiness check: instance OPEN + PDB READ WRITE.\"\"\"\n",
    "    print(\"Waiting for database to be ready (this may take a few minutes on first start)...\")\n",
    "    deadline = time.time() + timeout_s\n",
    "    cmd = (\n",
    "        \"export ORACLE_SID=FREE; \"\n",
    "        \"sqlplus -s / as sysdba <<'SQL'\\n\"\n",
    "        \"set heading off feedback off pages 0 verify off echo off\\n\"\n",
    "        \"select status from v$instance;\\n\"\n",
    "        f\"select open_mode from v$pdbs where name='{pdb}';\\n\"\n",
    "        \"exit\\n\"\n",
    "        \"SQL\"\n",
    "    )\n",
    "    while time.time() < deadline:\n",
    "        r = sh(\"docker\", \"exec\", container, \"bash\", \"-lc\", cmd, check=False)\n",
    "        out = (r.stdout or \"\").upper()\n",
    "        if \"OPEN\" in out and \"READ WRITE\" in out:\n",
    "            print(\"Database is ready!\")\n",
    "            return\n",
    "        time.sleep(3)\n",
    "    raise TimeoutError(f\"DB not ready after {timeout_s}s. Try: docker logs -f {container}\")\n",
    "\n",
    "def get_vector_memory_size(container=CONTAINER) -> int:\n",
    "    \"\"\"Return vector_memory_size in bytes (0 if unset).\"\"\"\n",
    "    cmd = (\n",
    "        \"export ORACLE_SID=FREE; \"\n",
    "        \"sqlplus -s / as sysdba <<'SQL'\\n\"\n",
    "        \"set heading off feedback off pages 0 verify off echo off\\n\"\n",
    "        \"select value from v$parameter where name='vector_memory_size';\\n\"\n",
    "        \"exit\\n\"\n",
    "        \"SQL\"\n",
    "    )\n",
    "    r = sh(\"docker\", \"exec\", container, \"bash\", \"-lc\", cmd, check=False)\n",
    "    raw = (r.stdout or \"\").strip()\n",
    "    tokens = [t for t in raw.split() if t.isdigit()]\n",
    "    return int(tokens[-1]) if tokens else 0\n",
    "\n",
    "def configure_vector_memory(container=CONTAINER, size=\"1G\", timeout_s=600):\n",
    "    \"\"\"\n",
    "    Ensure Vector Pool is enabled for HNSW/ANN operations by setting VECTOR_MEMORY_SIZE\n",
    "    (SPFILE) and restarting the DB if needed.\n",
    "    \"\"\"\n",
    "    current = get_vector_memory_size(container)\n",
    "    if current and current > 0:\n",
    "        return False  # no restart needed\n",
    "\n",
    "    sql = f\"\"\"\n",
    "alter system set vector_memory_size={size} scope=spfile sid='*';\n",
    "shutdown immediate;\n",
    "startup;\n",
    "\"\"\"\n",
    "    sh(\"docker\", \"exec\", container, \"bash\", \"-lc\", f\"export ORACLE_SID=FREE; sqlplus -s / as sysdba <<'SQL'\\n{sql}\\nSQL\")\n",
    "    wait_ready(container=container, pdb=PDB, timeout_s=timeout_s)\n",
    "    return True  # restart happened\n",
    "\n",
    "def create_vector_user(container=CONTAINER, vector_pwd=\"VectorPwd_2025\"):\n",
    "    sql = f\"\"\"\n",
    "ALTER SESSION SET CONTAINER = {PDB};\n",
    "BEGIN\n",
    "  EXECUTE IMMEDIATE 'CREATE USER VECTOR IDENTIFIED BY \"{vector_pwd}\"';\n",
    "EXCEPTION\n",
    "  WHEN OTHERS THEN\n",
    "    IF SQLCODE != -01920 AND SQLCODE != -01921 AND SQLCODE != -01918 AND SQLCODE != -00955 THEN\n",
    "      RAISE;\n",
    "    END IF;\n",
    "END;\n",
    "/\n",
    "GRANT CREATE SESSION, CREATE TABLE, CREATE SEQUENCE, CREATE VIEW TO VECTOR;\n",
    "GRANT UNLIMITED TABLESPACE TO VECTOR;\n",
    "\"\"\"\n",
    "    sh(\"docker\", \"exec\", container, \"bash\", \"-lc\",\n",
    "       f'export ORACLE_SID=FREE; echo \"{sql}\" | sqlplus -s / as sysdba')\n",
    "\n",
    "def test_connection(vector_pwd=\"VectorPwd_2025\"):\n",
    "    conn = oracledb.connect(user=\"VECTOR\", password=vector_pwd, dsn=f\"127.0.0.1:{PORT}/{PDB}\")\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"select sys_context('userenv','con_name') from dual\")\n",
    "        con_name = cur.fetchone()[0]\n",
    "    conn.close()\n",
    "    return con_name\n",
    "\n",
    "def setup(oracle_pwd=None, vector_pwd=\"VectorPwd_2025\", vector_memory_size=\"1G\"):\n",
    "    ensure_running(oracle_pwd=oracle_pwd)\n",
    "    wait_ready()\n",
    "\n",
    "    restarted = configure_vector_memory(size=vector_memory_size)\n",
    "    if restarted:\n",
    "        print(f\"\\U0001f9e0 Enabled Vector Pool (VECTOR_MEMORY_SIZE={vector_memory_size}) and restarted DB\")\n",
    "\n",
    "    create_vector_user(vector_pwd=vector_pwd)\n",
    "    con_name = test_connection(vector_pwd=vector_pwd)\n",
    "    print(f\"\\u2705 Ready. Connected as VECTOR to container: {con_name}\")\n",
    "    print(f\"DSN: 127.0.0.1:{PORT}/{PDB}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bacd1640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container 'oracle-free' is already running.\n",
      "Waiting for database to be ready (this may take a few minutes on first start)...\n",
      "Database is ready!\n",
      "âœ… Ready. Connected as VECTOR to container: FREEPDB1\n",
      "DSN: 127.0.0.1:1521/FREEPDB1\n"
     ]
    }
   ],
   "source": [
    "# Run this cell after starting your Docker container\n",
    "# It handles everything: waits for ready, configures vector memory, creates user, tests connection\n",
    "setup(oracle_pwd=\"OraclePwd_2025\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b1afd2",
   "metadata": {},
   "source": [
    "### Connection Helper Function\n",
    "\n",
    "In the code below we have a reusable function that connects to Oracle Database with automatic retry logic and helpful error messages.\n",
    "\n",
    "**What it does:**\n",
    "1. Attempts to connect using the `oracledb` Python driver\n",
    "2. Retries up to 3 times if the connection fails (useful when the database is still starting)\n",
    "3. Prints the Oracle version banner on successful connection. This will also include the version you are running\n",
    "4. Provides troubleshooting hints for common connection errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "322bb9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import oracledb\n",
    "import time\n",
    "\n",
    "def connect_to_oracle(max_retries=3, retry_delay=5, user=\"sys\", password=\"OraclePwd_2025\", dsn=ORACLE_DSN, program=\"langchain_oracledb_deep_research_demo\"):\n",
    "    \"\"\"\n",
    "    Connect to Oracle database with retry logic and better error handling.\n",
    "    \n",
    "    Args:\n",
    "        max_retries: Maximum number of connection attempts\n",
    "        retry_delay: Seconds to wait between retries\n",
    "    \"\"\"\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(f\"Connection attempt {attempt}/{max_retries}...\")\n",
    "            conn = oracledb.connect(\n",
    "                user=user,\n",
    "                password=password,\n",
    "                dsn=dsn,\n",
    "                program=program\n",
    "            )\n",
    "            print(\"âœ“ Connected successfully!\")\n",
    "            \n",
    "            # Test the connection\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(\"SELECT banner FROM v$version WHERE banner LIKE 'Oracle%'\")\n",
    "                banner = cur.fetchone()[0]\n",
    "                # Banner should include the version you are running\n",
    "                print(f\"\\n{banner}\")\n",
    "            \n",
    "            return conn\n",
    "            \n",
    "        except oracledb.OperationalError as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"âœ— Connection failed (attempt {attempt}/{max_retries})\")\n",
    "            \n",
    "            if \"DPY-4011\" in error_msg or \"Connection reset by peer\" in error_msg:\n",
    "                print(\"  â†’ This usually means:\")\n",
    "                print(\"    1. Database is still starting up (wait 2-3 minutes)\")\n",
    "                print(\"    2. Listener configuration issue\")\n",
    "                print(\"    3. Container is not running\")\n",
    "                \n",
    "                if attempt < max_retries:\n",
    "                    print(f\"\\n  Waiting {retry_delay} seconds before retry...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                else:\n",
    "                    print(\"\\n  ðŸ’¡ Try running: setup(oracle_pwd='OraclePwd_2025')\")\n",
    "                    print(\"     This will fix the listener and verify the connection.\")\n",
    "                    raise\n",
    "            else:\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Unexpected error: {e}\")\n",
    "            raise\n",
    "    \n",
    "    raise ConnectionError(\"Failed to connect after all retries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8bacbe",
   "metadata": {},
   "source": [
    "Ensure you have your Docker Engine running before going through the next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03562c6",
   "metadata": {},
   "source": [
    "Connect as the `VECTOR` user dedicated schema for storing embeddings and vector data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f3aa1368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection attempt 1/3...\n",
      "âœ“ Connected successfully!\n",
      "\n",
      "Oracle Database 23ai Free Release 23.0.0.0.0 - Develop, Learn, and Run for Free\n",
      "Using user: VECTOR\n"
     ]
    }
   ],
   "source": [
    "vector_conn = connect_to_oracle(\n",
    "    user=\"VECTOR\",\n",
    "    password=\"VectorPwd_2025\",\n",
    "    dsn=ORACLE_DSN,\n",
    "    program=\"langchain_oracledb_deep_research_demo\",\n",
    ")\n",
    "\n",
    "print(\"Using user:\", vector_conn.username)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365bcafb",
   "metadata": {},
   "source": [
    "âœ… **Setup complete!** You now have Oracle Database Free running locally in Docker with an active connection.\n",
    "\n",
    "Next, we'll create vector stores using **LangChain's Oracle integration** to store embeddings and metadata for semantic search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f8ccd5",
   "metadata": {},
   "source": [
    "# Vector Search With Langchain and Oracle AI Database\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa782a",
   "metadata": {},
   "source": [
    "This section demonstrates how to use **LangChain's Oracle Vector Store (OracleVS)** to store and search documents using semantic similarity. \n",
    "\n",
    "Vector search enables finding documents based on meaning rather than exact keyword matches.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| **1. Initialize Embeddings** | Load a HuggingFace embedding model to convert text into vectors |\n",
    "| **2. Create Vector Store** | Set up an Oracle-backed vector store with distance strategy |\n",
    "| **3. Create Index** | Build an HNSW (Hierarchical Navigable Small World) index for fast similarity search |\n",
    "| **4. Add Documents** | Store text with metadata in the vector database |\n",
    "| **5. Query** | Search for similar documents using natural language |\n",
    "| **6. Filter Results** | Use metadata filters to narrow down search results |\n",
    "\n",
    "## Key Components\n",
    "\n",
    "- **`OracleVS`**: LangChain's Oracle vector store integration\n",
    "- **`HuggingFaceEmbeddings`**: Converts text to 768-dimensional vectors\n",
    "- **`DistanceStrategy.EUCLIDEAN_DISTANCE`**: Measures similarity between vectors\n",
    "- **HNSW Index**: Speeds up searches on large datasets using hierarchical graph-based nearest neighbor search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e5cbeb",
   "metadata": {},
   "source": [
    "## Creating Vector Stores with Langchain OracleVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "340bffc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 199/199 [00:00<00:00, 1372.31it/s, Materializing param=pooler.dense.weight]                        \n",
      "\u001b[1mMPNetModel LOAD REPORT\u001b[0m from: sentence-transformers/paraphrase-mpnet-base-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_oracledb.vectorstores import OracleVS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_oracledb.vectorstores.oraclevs import create_index\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# Initialize the vector store\n",
    "vector_store = OracleVS(\n",
    "    client=vector_conn, \n",
    "    embedding_function=embedding_model,\n",
    "    table_name=\"VECTOR_SEARCH_DEMO\",\n",
    "    distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "64e24aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to clean stale internal vector tables and safely create indexes\n",
    "\n",
    "def cleanup_stale_vector_tables(conn):\n",
    "    \"\"\"Drop stale internal vector index tables (VECTOR$...) left by failed index operations.\"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"SELECT table_name FROM user_tables WHERE table_name LIKE 'VECTOR$%' ORDER BY table_name\")\n",
    "        stale_tables = cur.fetchall()\n",
    "    if not stale_tables:\n",
    "        return\n",
    "    print(f\"  \\U0001f9f9 Found {len(stale_tables)} internal vector table(s) \\u2014 cleaning up orphans...\")\n",
    "    for (tbl,) in stale_tables:\n",
    "        try:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(f'DROP TABLE \"{tbl}\" PURGE')\n",
    "            conn.commit()\n",
    "            print(f\"     Dropped orphan: {tbl}\")\n",
    "        except Exception as e:\n",
    "            err = str(e)\n",
    "            if \"ORA-51903\" in err:\n",
    "                pass\n",
    "            else:\n",
    "                print(f\"     Could not drop {tbl}: {err[:80]}\")\n",
    "\n",
    "\n",
    "def _drop_existing_vector_indexes(conn, table_name):\n",
    "    \"\"\"Drop any existing vector (DOMAIN) indexes on the given table.\n",
    "    \n",
    "    Vector indexes in Oracle 23ai are DOMAIN indexes. The standard\n",
    "    user_ind_columns view does not reliably contain column info for\n",
    "    them, so we match on table_name + index_type instead.\n",
    "    \"\"\"\n",
    "    bare_table = table_name.strip('\"').upper()\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"SELECT index_name FROM user_indexes \"\n",
    "            \"WHERE table_name = :1 AND index_type = 'DOMAIN'\",\n",
    "            [bare_table]\n",
    "        )\n",
    "        rows = cur.fetchall()\n",
    "    for (old_idx,) in rows:\n",
    "        try:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(f'DROP INDEX \"{old_idx}\"')\n",
    "            print(f\"     Dropped old index: {old_idx}\")\n",
    "        except Exception as e:\n",
    "            print(f\"     Could not drop {old_idx}: {str(e)[:80]}\")\n",
    "\n",
    "\n",
    "def safe_create_index(conn, vs, idx_name):\n",
    "    \"\"\"Create HNSW index, dropping any prior vector index on the same table first.\n",
    "    \n",
    "    Important: HNSW (INMEMORY NEIGHBOR GRAPH) indexes in Oracle 23ai Free\n",
    "    do NOT support DML. Only create on tables that are done receiving writes.\n",
    "    \"\"\"\n",
    "    # Check if this exact index already exists\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"SELECT index_name FROM user_indexes WHERE index_name = :1\",\n",
    "            [idx_name.upper()]\n",
    "        )\n",
    "        if cur.fetchone():\n",
    "            print(f\"  \\u23ed\\ufe0f Index already exists: {idx_name} (skipped)\")\n",
    "            return\n",
    "\n",
    "    # Drop any existing vector index on the same table (avoids ORA-01408)\n",
    "    _drop_existing_vector_indexes(conn, vs.table_name)\n",
    "\n",
    "    # Map LangChain distance strategy to Oracle distance function\n",
    "    dist_map = {\n",
    "        \"EUCLIDEAN_DISTANCE\": \"EUCLIDEAN\",\n",
    "        \"COSINE\": \"COSINE\",\n",
    "        \"DOT_PRODUCT\": \"DOT\",\n",
    "        \"MAX_INNER_PRODUCT\": \"DOT\",\n",
    "    }\n",
    "    dist_name = vs.distance_strategy.name if hasattr(vs.distance_strategy, 'name') else str(vs.distance_strategy)\n",
    "    oracle_dist = dist_map.get(dist_name, \"COSINE\")\n",
    "\n",
    "    ddl = (\n",
    "        f\"CREATE VECTOR INDEX {idx_name} ON {vs.table_name}(embedding) \"\n",
    "        f\"ORGANIZATION INMEMORY NEIGHBOR GRAPH \"\n",
    "        f\"WITH TARGET ACCURACY 95 \"\n",
    "        f\"DISTANCE {oracle_dist} \"\n",
    "        f\"PARAMETERS (type HNSW, neighbors 32, efConstruction 200) \"\n",
    "        f\"PARALLEL 8\"\n",
    "    )\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(ddl)\n",
    "        print(f\"  \\u2705 Created HNSW index: {idx_name}\")\n",
    "    except Exception as e:\n",
    "        err = str(e)\n",
    "        if \"ORA-00955\" in err:\n",
    "            print(f\"  \\u23ed\\ufe0f Index already exists: {idx_name} (skipped)\")\n",
    "        elif \"ORA-51956\" in err:\n",
    "            print(f\"  \\u26a0\\ufe0f VECTOR_MEMORY_SIZE not configured. Run setup() first.\")\n",
    "        else:\n",
    "            print(f\"  \\u26a0\\ufe0f Could not create index {idx_name}: {err[:120]}\")\n",
    "            print(f\"     \\u2192 Vector search still works without an index (full scan)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "146cb8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Suppress langchain_oracledb logging, remove this if you want to see the debug logs\n",
    "logging.getLogger(\"langchain_oracledb\").setLevel(logging.CRITICAL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc1b4b9",
   "metadata": {},
   "source": [
    "## Ingesting Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4926e267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add texts to the vector database\n",
    "texts = [\"A tablespace can be online (accessible) or offline (not accessible) whenever the database is open.\\nA tablespace is usually online so that its data is available to users. The SYSTEM tablespace and temporary tablespaces cannot be taken offline.\", \"The database stores LOBs differently from other data types. Creating a LOB column implicitly creates a LOB segment and a LOB index. \"]\n",
    "metadata = [\n",
    "    {\"id\": \"100\", \"link\": \"Document Example Test 1\"},\n",
    "    {\"id\": \"101\", \"link\": \"Document Example Test 2\"},\n",
    "]\n",
    "\n",
    "# Simple Ingestion\n",
    "vector_store.add_texts(\n",
    "    texts=texts, # This is the text embeddings will be generated from\n",
    "    metadatas=metadata # This is the metadata that will be stored with the text\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061c994",
   "metadata": {},
   "source": [
    "## Querying the Vector Store\n",
    "\n",
    "Search for documents similar to a natural language query. \n",
    "\n",
    "The vector store converts queries to an embedding and finds the closest matches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78909ea2",
   "metadata": {},
   "source": [
    "Basic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3196205a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Result 1 ---\n",
      "Text: A tablespace can be online (accessible) or offline (not accessible) whenever the database is open.\n",
      "A tablespace is usually online so that its data is available to users.\n",
      "Metadata: {'id': '100', 'link': 'Document Example Test 1'}\n",
      "--- Result 2 ---\n",
      "Text: The database stores LOBs differently from other data types. Creating a LOB column implicitly creates a LOB segment and a LOB index.\n",
      "Metadata: {'id': '101', 'link': 'Document Example Test 2'}\n"
     ]
    }
   ],
   "source": [
    "query = \"How does Oracle handle tablespaces?\"\n",
    "\n",
    "results = vector_store.similarity_search(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(results, start=1):\n",
    "    print(f\"--- Result {i} ---\")\n",
    "    print(\"Text:\", doc.page_content)\n",
    "    print(\"Metadata:\", doc.metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ece922",
   "metadata": {},
   "source": [
    "Search With Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "56e523e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 3.180044961373964\n",
      "Text : A tablespace can be online (accessible) or offline (not accessible) whenever the database is open.\n",
      "A tablespace is usually online so that its data is available to users.\n",
      "Meta : {'id': '100', 'link': 'Document Example Test 1'}\n",
      "------\n",
      "Score: 3.602375651267878\n",
      "Text : The database stores LOBs differently from other data types. Creating a LOB column implicitly creates a LOB segment and a LOB index.\n",
      "Meta : {'id': '101', 'link': 'Document Example Test 2'}\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search_with_score(query, k=3)\n",
    "\n",
    "for doc, score in results:\n",
    "    print(\"Score:\", score)\n",
    "    print(\"Text :\", doc.page_content)\n",
    "    print(\"Meta :\", doc.metadata)\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10609d0",
   "metadata": {},
   "source": [
    "Filter by exact match on a metadata field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "081c7f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: A tablespace can be online (accessible) or offline (not accessible) whenever the database is open.\n",
      "A tablespace is usual ...\n",
      "Meta: {'id': '100', 'link': 'Document Example Test 1'}\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "query = \"How are tablespaces made available to users?\"\n",
    "\n",
    "# This will only return docs where metadata.link == \"Document Example Test 1\".\n",
    "docs = vector_store.similarity_search(\n",
    "    query, k=3,\n",
    "    filter={\"link\": {\"$eq\": \"Document Example Test 1\"}},\n",
    ")\n",
    "\n",
    "for doc in docs:\n",
    "    print(\"Text:\", doc.page_content[:120], \"...\")\n",
    "    print(\"Meta:\", doc.metadata)\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90290170",
   "metadata": {},
   "source": [
    "Filter by id list ($in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e0d8de15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='100', metadata={'id': '100', 'link': 'Document Example Test 1'}, page_content='A tablespace can be online (accessible) or offline (not accessible) whenever the database is open.\\nA tablespace is usually online so that its data is available to users.')]\n"
     ]
    }
   ],
   "source": [
    "docs = vector_store.similarity_search(\n",
    "    query=\"Explain database storage concepts\",\n",
    "    k=5,\n",
    "    filter={\"id\": {\"$in\": [\"100\"]}},  # only id 100\n",
    ")\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§¹ Found 17 internal vector table(s) â€” cleaning up orphans...\n",
      "  âš ï¸ Could not create index oravs_hnsw: ORA-01408: such column list already indexed\n",
      "Help: https://docs.oracle.com/error-help/db/ora-01408/\n",
      "     â†’ Vector search still works without an index (full scan)\n"
     ]
    }
   ],
   "source": [
    "# Create HNSW index AFTER data loading (Oracle 23ai Free HNSW does not support DML,\n",
    "# so the index must be created after all inserts are complete)\n",
    "cleanup_stale_vector_tables(vector_conn)\n",
    "safe_create_index(vector_conn, vector_store, \"oravs_hnsw\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82623c94",
   "metadata": {},
   "source": [
    "# Memory Engineering and Agent Memory\n",
    "--------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3ff4cf",
   "metadata": {},
   "source": [
    "\n",
    "**`Agent Memory`** is the exocortex that augments an LLMâ€”capturing, encoding, storing, linking, and retrieving information beyond the modelâ€™s parametric and contextual limits. \n",
    "It provides the persistence and structure required for long-horizon reasoning and reliable behaviour.\n",
    "\n",
    "**`Memory Engineering`** is the scaffolding and control harness that we design to move information optimally and efficiently into, through, and across all components of an AI system(databases, LLMs, applications etc). It ensures that data is captured, transformed, organized, and retrieved in the right way at the right timeâ€”so agents can behave reliably, believably, and capabaly.\n",
    "\n",
    "This is the core section of the notebook where we build a complete **`Memory Manager`** for AI agents. \n",
    "\n",
    "Just like humans have different types of memory (short-term, long-term, procedural), AI agents benefit from specialized memory systems.\n",
    "\n",
    "## Why Memory Engineering Matters\n",
    "\n",
    "Without memory, agents:\n",
    "- Forget previous conversations\n",
    "- Can't learn from past interactions\n",
    "- Repeat the same mistakes\n",
    "- Lack context for complex tasks\n",
    "\n",
    "With proper memory engineering, agents can:\n",
    "- Maintain context across sessions\n",
    "- Learn and improve over time\n",
    "- Access relevant knowledge when needed\n",
    "- Execute complex multi-step workflows\n",
    "\n",
    "## Memory Types We'll Implement\n",
    "\n",
    "| Memory Type | Human Analogy | Purpose | Storage |\n",
    "|-------------|---------------|---------|---------|\n",
    "| **Conversational** | Short-term memory | Chat history per thread | SQL Table |\n",
    "| **Knowledge Base** | Long-term semantic memory | Facts, documents, search results | Vector Store |\n",
    "| **Workflow** | Procedural memory | Learned action patterns | Vector Store |\n",
    "| **Toolbox** | Skill memory | Available tools & capabilities | Vector Store |\n",
    "| **Entity** | Episodic memory | People, places, systems mentioned | Vector Store |\n",
    "| **Summary** | Compressed memory | Condensed context for long conversations | Vector Store |\n",
    "\n",
    "## Steps in This Section\n",
    "\n",
    "1. **Define table names** for each memory type\n",
    "2. **Create SQL table** for conversational history\n",
    "3. **Create vector stores** for semantic memories\n",
    "4. **Build indexes** for fast similarity search\n",
    "5. **Implement MemoryLayer class** with read/write methods for each memory type\n",
    "6. **Initialize the memory manager** with all storage backends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b4fa8",
   "metadata": {},
   "source": [
    "## Define Memory Tables and Stores\n",
    "First, we define table names for each memory type. \n",
    "\n",
    "These tables will be created in Oracle Database to persist agent memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8e0ee2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table names for each memory type\n",
    "CONVERSATIONAL_TABLE   = \"CONVERSATIONAL_MEMORY\" # Episodic memory\n",
    "KNOWLEDGE_BASE_TABLE   = \"SEMANTIC_MEMORY\" # Semantic memory\n",
    "WORKFLOW_TABLE = \"WORKFLOW_MEMORY\" # Procedural memory\n",
    "TOOLBOX_TABLE    = \"TOOLBOX_MEMORY\" # Procedural memory\n",
    "ENTITY_TABLE = \"ENTITY_MEMORY\" # Semantic memory\n",
    "SUMMARY_TABLE = \"SUMMARY_MEMORY\" # Semanatic memory\n",
    "\n",
    "ALL_TABLES = [CONVERSATIONAL_TABLE, KNOWLEDGE_BASE_TABLE, WORKFLOW_TABLE, TOOLBOX_TABLE, ENTITY_TABLE, SUMMARY_TABLE]\n",
    "\n",
    "# Drop existing tables to start fresh\n",
    "for table in ALL_TABLES:\n",
    "    try:\n",
    "        with vector_conn.cursor() as cur:\n",
    "            cur.execute(f\"DROP TABLE {table} PURGE\")\n",
    "    except Exception as e:\n",
    "        if \"ORA-00942\" in str(e):\n",
    "            print(f\"  - {table} (not exists)\")\n",
    "        else:\n",
    "            print(f\"  âœ— {table}: {e}\")\n",
    "            \n",
    "vector_conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "29c9b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model token limits (for context management)\n",
    "MODEL_TOKEN_LIMITS = {\n",
    "    \"Qwen/Qwen2.5-72B-Instruct\": 131072,\n",
    "    \"Qwen/Qwen3-0.6B\": 32768,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded028dc",
   "metadata": {},
   "source": [
    "### Create Conversational Memory Table\n",
    "\n",
    "This function below creates a SQL table to store chat history. \n",
    "\n",
    "Unlike vector stores, conversational memory uses a traditional table because we need exact retrieval by thread ID (not similarity search).\n",
    "\n",
    "**What it does:**\n",
    "- Creates a table with columns: `id`, `thread_id`, `role`, `content`, `timestamp`, `metadata`\n",
    "- Adds an index on `thread_id` for fast conversation lookups\n",
    "- Adds an index on `timestamp` for chronological ordering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "54032e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversational_history_table(conn, table_name: str = \"CONVERSATIONAL_MEMORY\"):\n",
    "    \"\"\"\n",
    "    Create a table to store conversational history.\n",
    "\n",
    "    Args:\n",
    "        conn: Oracle database connection\n",
    "        table_name: Name of the table to create\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        # Drop table if exists\n",
    "        try:\n",
    "            cur.execute(f\"DROP TABLE {table_name}\")\n",
    "        except:\n",
    "            pass  # Table doesn't exist\n",
    "        \n",
    "        # Create table with proper schema\n",
    "        cur.execute(f\"\"\"\n",
    "            CREATE TABLE {table_name} (\n",
    "                id VARCHAR2(100) DEFAULT SYS_GUID() PRIMARY KEY,\n",
    "                thread_id VARCHAR2(100) NOT NULL,\n",
    "                role VARCHAR2(50) NOT NULL,\n",
    "                content CLOB NOT NULL,\n",
    "                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                metadata CLOB,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                summary_id VARCHAR2(100) DEFAULT NULL\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create index on thread_id for faster lookups\n",
    "        cur.execute(f\"\"\"\n",
    "            CREATE INDEX idx_{table_name.lower()}_thread_id ON {table_name}(thread_id)\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create index on timestamp for ordering\n",
    "        cur.execute(f\"\"\"\n",
    "            CREATE INDEX idx_{table_name.lower()}_timestamp ON {table_name}(timestamp)\n",
    "        \"\"\")\n",
    "        \n",
    "    conn.commit()\n",
    "    print(f\"Table {table_name} created successfully with indexes\")\n",
    "    return table_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "24745bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table CONVERSATIONAL_MEMORY created successfully with indexes\n"
     ]
    }
   ],
   "source": [
    "# Create the table\n",
    "CONVERSATION_HISTORY_TABLE = create_conversational_history_table(vector_conn, CONVERSATIONAL_TABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4539b694",
   "metadata": {},
   "source": [
    "### Create Vector Stores for Each Memory Type\n",
    "\n",
    "Here we create 5 separate vector storesâ€”one for each memory type. \n",
    "\n",
    "Each vector store is backed by its own Oracle table and uses the same embedding model for consistency.\n",
    "\n",
    "| Vector Store | Purpose |\n",
    "|--------------|---------|\n",
    "| `knowledge_base_vs` | Store documents, facts, and search results |\n",
    "| `workflow_vs` | Store learned action patterns and tool sequences |\n",
    "| `toolbox_vs` | Store tool definitions for semantic tool discovery |\n",
    "| `entity_vs` | Store extracted entities (people, places, systems) |\n",
    "| `summary_vs` | Store compressed summaries for long conversations |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "01fffb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_vs = OracleVS(\n",
    "    client=vector_conn,\n",
    "    embedding_function=embedding_model,\n",
    "    table_name=KNOWLEDGE_BASE_TABLE,\n",
    "    distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE,\n",
    ")\n",
    "\n",
    "workflow_vs = OracleVS(\n",
    "    client=vector_conn,\n",
    "    embedding_function=embedding_model,\n",
    "    table_name=WORKFLOW_TABLE,\n",
    "    distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE,\n",
    ")\n",
    "\n",
    "toolbox_vs = OracleVS(\n",
    "    client=vector_conn,\n",
    "    embedding_function=embedding_model,\n",
    "    table_name=TOOLBOX_TABLE,\n",
    "    distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE,\n",
    ")\n",
    "\n",
    "entity_vs = OracleVS(\n",
    "    client=vector_conn,\n",
    "    embedding_function=embedding_model,\n",
    "    table_name=ENTITY_TABLE,\n",
    "    distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE,\n",
    ")\n",
    "\n",
    "summary_vs = OracleVS(\n",
    "    client=vector_conn,\n",
    "    embedding_function=embedding_model,\n",
    "    table_name=SUMMARY_TABLE,\n",
    "    distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75548e90",
   "metadata": {},
   "source": [
    "Then we create indexes for each of the vector stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "fe0100e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ§¹ Found 2 internal vector table(s) â€” cleaning up orphans...\n",
      "Creating HNSW vector indexes...\n",
      "  âœ… Created HNSW index: knowledge_base_vs_hnsw\n",
      "  âœ… Created HNSW index: workflow_vs_hnsw\n",
      "  âœ… Created HNSW index: toolbox_vs_hnsw\n",
      "  âœ… Created HNSW index: entity_vs_hnsw\n",
      "  âœ… Created HNSW index: summary_vs_hnsw\n",
      "All HNSW indexes created!\n"
     ]
    }
   ],
   "source": [
    "# Create HNSW indexes on memory tables.\n",
    "# The MemoryManager._safe_add_texts() method handles the Oracle 23ai Free\n",
    "# limitation (HNSW does not support DML) by dropping and recreating indexes\n",
    "# around each write operation.\n",
    "cleanup_stale_vector_tables(vector_conn)\n",
    "\n",
    "print(\"Creating HNSW vector indexes...\")\n",
    "safe_create_index(vector_conn, knowledge_base_vs, \"knowledge_base_vs_hnsw\")\n",
    "safe_create_index(vector_conn, workflow_vs, \"workflow_vs_hnsw\")\n",
    "safe_create_index(vector_conn, toolbox_vs, \"toolbox_vs_hnsw\")\n",
    "safe_create_index(vector_conn, entity_vs, \"entity_vs_hnsw\")\n",
    "safe_create_index(vector_conn, summary_vs, \"summary_vs_hnsw\")\n",
    "print(\"All HNSW indexes created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929d5ff9",
   "metadata": {},
   "source": [
    "## Programmatic vs Agentic Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5678a3",
   "metadata": {},
   "source": [
    "A key design decision in memory engineering is determining which operations should be **programmatic** (executed automatically by code) versus **agentic** (decided by the LLM at runtime).\n",
    "\n",
    "| Operation | Programmatic | Agentic |\n",
    "|-----------|:------------:|:-------:|\n",
    "| `read_conversational_memory()` | âœ… | âŒ |\n",
    "| `read_knowledge_base()` | âœ… | âŒ |\n",
    "| `read_workflow()` | âœ… | âŒ |\n",
    "| `read_entity()` | âœ… | âŒ |\n",
    "| `read_summary_context()` | âœ… | âŒ |\n",
    "| `write_conversational_memory()` | âœ… | âŒ |\n",
    "| `write_workflow()` | âœ… | âŒ |\n",
    "| `write_entity()` | âœ… | âŒ |\n",
    "| `search_tavily()` | âŒ | âœ… |\n",
    "| `expand_summary()` | âŒ | âœ… |\n",
    "| `summarize_and_store()` | âŒ | âœ… |\n",
    "\n",
    "### Why Memory Reads are Programmatic\n",
    "\n",
    "Memory retrieval operations are **always executed** at the start of each agent loop because:\n",
    "\n",
    "1. **Context is essential** â€” The agent needs memory to understand the conversation and avoid repeating mistakes. Without this, every interaction starts from scratch.\n",
    "\n",
    "2. **The agent can't know what it doesn't know** â€” If the agent had to decide whether to check memory, it would need to already know what's in memoryâ€”a chicken-and-egg problem.\n",
    "\n",
    "3. **Consistency** â€” Always loading memory ensures the agent has a predictable, complete view of its knowledge.\n",
    "\n",
    "### Why Memory Writes are Programmatic\n",
    "\n",
    "Storing conversations, workflows, and entities happens automatically because:\n",
    "\n",
    "1. **Reliability** â€” We don't want the agent to \"forget\" to save important information. Conversation history must be persisted consistently.\n",
    "\n",
    "2. **Completeness** â€” Every interaction should be recorded. Selective saving would create gaps in memory.\n",
    "\n",
    "3. **Reduced cognitive load** â€” Letting the agent focus on the task rather than memory management leads to better responses.\n",
    "\n",
    "### Why Tool Calls are Agentic\n",
    "\n",
    "External actions like web search and summary expansion are left to the agent's discretion because:\n",
    "\n",
    "1. **Intent matters** â€” Only the agent knows if it needs more information. Automatically searching for every query would be wasteful.\n",
    "\n",
    "2. **Cost considerations** â€” External API calls have latency and may incur costs. The agent should only call them when genuinely needed.\n",
    "\n",
    "3. **Judgment required** â€” Deciding *what* to search for or *which* summary to expand requires understanding the user's intentâ€”something the LLM excels at."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f3f57d",
   "metadata": {},
   "source": [
    "## Memory Manager Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85d87a8",
   "metadata": {},
   "source": [
    "The `MemoryManager` class is the central abstraction that unifies all memory operations. It provides a clean interface for reading and writing to different memory types, hiding the complexity of SQL queries and vector store operations.\n",
    "\n",
    "### What We're Building\n",
    "\n",
    "A single class that manages 6 types of memory with consistent read/write patterns:\n",
    "\n",
    "| Memory Type | Storage | Write Method | Read Method |\n",
    "|-------------|---------|--------------|-------------|\n",
    "| **Conversational** | SQL Table | `write_conversational_memory()` | `read_conversational_memory()` |\n",
    "| **Knowledge Base** | Vector Store | `write_knowledge_base()` | `read_knowledge_base()` |\n",
    "| **Workflow** | Vector Store | `write_workflow()` | `read_workflow()` |\n",
    "| **Toolbox** | Vector Store | `write_toolbox()` | `read_toolbox()` |\n",
    "| **Entity** | Vector Store | `write_entity()` | `read_entity()` |\n",
    "| **Summary** | Vector Store | `write_summary()` | `read_summary_memory()`, `read_summary_context()` |\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Thread-based conversations** â€” Messages are organized by `thread_id` for multi-conversation support\n",
    "- **Semantic search** â€” Vector stores enable finding relevant content by meaning, not just keywords\n",
    "- **Metadata filtering** â€” Workflows filter by `num_steps > 0`, summaries filter by `id`\n",
    "- **LLM-powered entity extraction** â€” Automatically extracts people, places, and systems from text\n",
    "- **Formatted context output** â€” Each read method returns formatted text ready for the LLM context\n",
    "\n",
    "### Alternative: Memory Manager Frameworks\n",
    "\n",
    "There are existing frameworks that abstract memory management for AI agents:\n",
    "\n",
    "| Framework | Description |\n",
    "|-----------|-------------|\n",
    "| **LangChain Memory** | Built-in memory classes (ConversationBufferMemory, VectorStoreRetrieverMemory) |\n",
    "| **Mem0** | Dedicated memory layer for AI agents with automatic memory management |\n",
    "| **LlamaIndex** | Document-based memory with various storage backends |\n",
    "| **Zep** | Long-term memory service for AI assistants |\n",
    "\n",
    "### Pros and Cons of Building Your Own\n",
    "\n",
    "| Approach | Pros | Cons |\n",
    "|----------|------|------|\n",
    "| **Custom (what we're doing)** | Full control, tailored to your needs, deeper understanding, no external dependencies | More code to maintain, need to handle edge cases yourself |\n",
    "| **Using a framework** | Faster to implement, battle-tested, community support, handles edge cases | Less control, may not fit your exact use case, additional dependency |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bb7a2f",
   "metadata": {},
   "source": [
    "> **For learning purposes**, building your own memory manager (as we do here) gives you a deep understanding of how memory engineering works. \n",
    "> \n",
    "> **For production**, you might consider using or extending an existing framework. \n",
    ">\n",
    "> For example, this simple notebook only illustrates reads and writes, but not deletion and updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "27aacfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json as json_lib\n",
    "from datetime import datetime\n",
    "\n",
    "class MemoryManager:\n",
    "    \"\"\"\n",
    "    A simplified memory manager for AI agents using Oracle AI Database.\n",
    "    \n",
    "    Manages 5 types of memory:\n",
    "    - Conversational: Chat history per thread (SQL table)\n",
    "    - Knowledge Base: Searchable documents (Vector store)\n",
    "    - Workflow: Execution patterns (Vector store)\n",
    "    - Toolbox: Available tools (Vector store)\n",
    "    - Entity: People, places, systems (Vector store)\n",
    "    - Summary: Storing compressed context window\n",
    "    \n",
    "    HNSW indexes in Oracle 23ai Free do not support DML. This class\n",
    "    handles that by dropping the HNSW index before writes and\n",
    "    recreating it afterward, keeping indexes active for reads.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, conn, conversation_table: str, knowledge_base_vs,\n",
    "                 workflow_vs, toolbox_vs, entity_vs, summary_vs,\n",
    "                 hnsw_index_names: dict = None):\n",
    "        self.conn = conn\n",
    "        self.conversation_table = conversation_table\n",
    "        self.knowledge_base_vs = knowledge_base_vs\n",
    "        self.workflow_vs = workflow_vs\n",
    "        self.toolbox_vs = toolbox_vs\n",
    "        self.entity_vs = entity_vs\n",
    "        self.summary_vs = summary_vs\n",
    "        \n",
    "        # Map table_name (uppercase, unquoted) -> HNSW index name (uppercase)\n",
    "        self._hnsw_indexes = {}\n",
    "        if hnsw_index_names:\n",
    "            for table, idx in hnsw_index_names.items():\n",
    "                self._hnsw_indexes[table.strip('\"').upper()] = idx.upper()\n",
    "    \n",
    "    # ==================== HNSW INDEX MANAGEMENT ====================\n",
    "    \n",
    "    def _safe_add_texts(self, vs, texts, metadatas=None):\n",
    "        \"\"\"Insert into a vector store, handling HNSW drop/recreate.\n",
    "        \n",
    "        Oracle 23ai Free HNSW indexes block DML. This method:\n",
    "        1. Drops the known HNSW index (by name, no catalog query needed)\n",
    "        2. Performs the insert\n",
    "        3. Recreates the HNSW index\n",
    "        \"\"\"\n",
    "        table_key = vs.table_name.strip('\"').upper()\n",
    "        idx_name = self._hnsw_indexes.get(table_key)\n",
    "        \n",
    "        # Drop the HNSW index if we know about one on this table\n",
    "        if idx_name:\n",
    "            self.conn.rollback()  # Clear any pending transaction before DDL\n",
    "            try:\n",
    "                with self.conn.cursor() as cur:\n",
    "                    cur.execute(f\"DROP INDEX {idx_name}\")\n",
    "            except Exception:\n",
    "                pass  # Index may not exist yet\n",
    "        \n",
    "        # Perform the insert\n",
    "        vs.add_texts(texts, metadatas)\n",
    "        \n",
    "        # Recreate the HNSW index\n",
    "        if idx_name:\n",
    "            dist_map = {\n",
    "                \"EUCLIDEAN_DISTANCE\": \"EUCLIDEAN\", \"COSINE\": \"COSINE\",\n",
    "                \"DOT_PRODUCT\": \"DOT\", \"MAX_INNER_PRODUCT\": \"DOT\",\n",
    "            }\n",
    "            dist_name = vs.distance_strategy.name if hasattr(vs.distance_strategy, 'name') else str(vs.distance_strategy)\n",
    "            oracle_dist = dist_map.get(dist_name, \"COSINE\")\n",
    "            try:\n",
    "                ddl = (\n",
    "                    f\"CREATE VECTOR INDEX {idx_name} ON {vs.table_name}(embedding) \"\n",
    "                    f\"ORGANIZATION INMEMORY NEIGHBOR GRAPH \"\n",
    "                    f\"WITH TARGET ACCURACY 95 \"\n",
    "                    f\"DISTANCE {oracle_dist} \"\n",
    "                    f\"PARAMETERS (type HNSW, neighbors 32, efConstruction 200) \"\n",
    "                    f\"PARALLEL 8\"\n",
    "                )\n",
    "                with self.conn.cursor() as cur:\n",
    "                    cur.execute(ddl)\n",
    "            except Exception:\n",
    "                pass  # Best-effort; search still works via full scan\n",
    "    \n",
    "    # ==================== CONVERSATIONAL MEMORY (SQL) ====================\n",
    "    \n",
    "    def write_conversational_memory(self, content: str, role: str, thread_id: str) -> str:\n",
    "        \"\"\"Store a message in conversation history.\"\"\"\n",
    "        thread_id = str(thread_id)\n",
    "        with self.conn.cursor() as cur:\n",
    "            id_var = cur.var(str)\n",
    "            cur.execute(f\"\"\"\n",
    "                INSERT INTO {self.conversation_table} (thread_id, role, content, metadata, timestamp)\n",
    "                VALUES (:thread_id, :role, :content, :metadata, CURRENT_TIMESTAMP)\n",
    "                RETURNING id INTO :id\n",
    "            \"\"\", {\"thread_id\": thread_id, \"role\": role, \"content\": content, \"metadata\": \"{}\", \"id\": id_var})\n",
    "            record_id = id_var.getvalue()[0] if id_var.getvalue() else None\n",
    "        self.conn.commit()\n",
    "        return record_id\n",
    "    \n",
    "    def read_conversational_memory(self, thread_id: str, limit: int = 10) -> str:\n",
    "        \"\"\"Read conversation history for a thread (excludes summarized messages).\"\"\"\n",
    "        thread_id = str(thread_id)\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(f\"\"\"\n",
    "                SELECT role, content, timestamp FROM {self.conversation_table}\n",
    "                WHERE thread_id = :thread_id AND summary_id IS NULL\n",
    "                ORDER BY timestamp ASC\n",
    "                FETCH FIRST :limit ROWS ONLY\n",
    "            \"\"\", {\"thread_id\": thread_id, \"limit\": limit})\n",
    "            results = cur.fetchall()\n",
    "        \n",
    "        messages = [f\"[{ts.strftime('%H:%M:%S')}] [{role}] {content}\" for role, content, ts in results]\n",
    "        messages_formatted = '\\n'.join(messages)\n",
    "        return f\"\"\"## Conversation Memory: This is the conversation history for the current thread\n",
    "### How to use: Use the conversation history to answer the question\n",
    "\n",
    "{messages_formatted}\"\"\"\n",
    "    \n",
    "    def mark_as_summarized(self, thread_id: str, summary_id: str):\n",
    "        \"\"\"Mark all unsummarized messages in a thread as summarized.\"\"\"\n",
    "        thread_id = str(thread_id)\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(f\"\"\"\n",
    "                UPDATE {self.conversation_table}\n",
    "                SET summary_id = :summary_id\n",
    "                WHERE thread_id = :thread_id AND summary_id IS NULL\n",
    "            \"\"\", {\"summary_id\": summary_id, \"thread_id\": thread_id})\n",
    "        self.conn.commit()\n",
    "        print(f\"  \\U0001f4e6 Marked messages as summarized (summary_id: {summary_id})\")\n",
    "    \n",
    "    # ==================== KNOWLEDGE BASE (Vector Store) ====================\n",
    "    \n",
    "    def write_knowledge_base(self, text: str, metadata: dict):\n",
    "        \"\"\"Store text in knowledge base with metadata.\"\"\"\n",
    "        self._safe_add_texts(self.knowledge_base_vs, [text], [metadata])\n",
    "    \n",
    "    def read_knowledge_base(self, query: str, k: int = 3) -> str:\n",
    "        \"\"\"Search knowledge base for relevant content.\"\"\"\n",
    "        results = self.knowledge_base_vs.similarity_search(query, k=k)\n",
    "        content = \"\\n\".join([doc.page_content for doc in results])\n",
    "        return f\"\"\"## Knowledge Base Memory: This are general information that is relevant to the question\n",
    "### How to use: Use the knowledge base as background information that can help answer the question\n",
    "\n",
    "{content}\"\"\"\n",
    "    \n",
    "    \n",
    "    # ==================== WORKFLOW (Vector Store) ====================\n",
    "    \n",
    "    def write_workflow(self, query: str, steps: list, final_answer: str, success: bool = True):\n",
    "        \"\"\"Store a completed workflow pattern for future reference.\"\"\"\n",
    "        steps_text = \"\\n\".join([f\"Step {i+1}: {s}\" for i, s in enumerate(steps)])\n",
    "        text = f\"Query: {query}\\nSteps:\\n{steps_text}\\nAnswer: {final_answer[:200]}\"\n",
    "        \n",
    "        metadata = {\n",
    "            \"query\": query,\n",
    "            \"success\": success,\n",
    "            \"num_steps\": len(steps),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        self._safe_add_texts(self.workflow_vs, [text], [metadata])\n",
    "    \n",
    "    def read_workflow(self, query: str, k: int = 3) -> str:\n",
    "        \"\"\"Search for similar past workflows with at least 1 step.\"\"\"\n",
    "        results = self.workflow_vs.similarity_search(\n",
    "            query, \n",
    "            k=k, \n",
    "            filter={\"num_steps\": {\"$gt\": 0}}\n",
    "        )\n",
    "        if not results:\n",
    "            return \"## Workflow Memory\\nNo relevant workflows found.\"\n",
    "        content = \"\\n---\\n\".join([doc.page_content for doc in results])\n",
    "        return f\"\"\"## Workflow Memory: This are the past workflows that are relevant to the question\n",
    "### How to use: Use the steps and use them to answer the question, especially when using tools and external sources\n",
    "\n",
    "{content}\"\"\"\n",
    "    \n",
    "    # ==================== TOOLBOX (Vector Store) ====================\n",
    "    \n",
    "    def write_toolbox(self, text: str, metadata: dict):\n",
    "        \"\"\"Store a tool definition in the toolbox.\"\"\"\n",
    "        self._safe_add_texts(self.toolbox_vs, [text], [metadata])\n",
    "    \n",
    "    def read_toolbox(self, query: str, k: int = 3) -> list[dict]:\n",
    "        \"\"\"Find relevant tools and return OpenAI-compatible schemas.\"\"\"\n",
    "        results = self.toolbox_vs.similarity_search(query, k=k)\n",
    "        tools = []\n",
    "        for doc in results:\n",
    "            meta = doc.metadata\n",
    "            stored_params = meta.get(\"parameters\", {})\n",
    "            properties = {}\n",
    "            required = []\n",
    "            \n",
    "            for param_name, param_info in stored_params.items():\n",
    "                param_type = param_info.get(\"type\", \"string\")\n",
    "                type_mapping = {\n",
    "                    \"<class 'str'>\": \"string\",\n",
    "                    \"<class 'int'>\": \"integer\", \n",
    "                    \"<class 'float'>\": \"number\",\n",
    "                    \"<class 'bool'>\": \"boolean\",\n",
    "                    \"str\": \"string\",\n",
    "                    \"int\": \"integer\",\n",
    "                    \"float\": \"number\",\n",
    "                    \"bool\": \"boolean\"\n",
    "                }\n",
    "                json_type = type_mapping.get(param_type, \"string\")\n",
    "                properties[param_name] = {\"type\": json_type}\n",
    "                \n",
    "                if \"default\" not in param_info:\n",
    "                    required.append(param_name)\n",
    "            \n",
    "            tools.append({\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": meta.get(\"name\", \"tool\"),\n",
    "                    \"description\": meta.get(\"description\", \"\"),\n",
    "                    \"parameters\": {\"type\": \"object\", \"properties\": properties, \"required\": required}\n",
    "                }\n",
    "            })\n",
    "        return tools\n",
    "\n",
    "    # ==================== ENTITY (Vector Store) ====================\n",
    "    \n",
    "    def extract_entities(self, text: str, llm_client) -> list[dict]:\n",
    "        \"\"\"Use LLM to extract entities (people, places, systems) from text.\"\"\"\n",
    "        if not text or len(text.strip()) < 5:\n",
    "            return []\n",
    "        \n",
    "        prompt = f'''Extract entities from: \"{text[:500]}\"\n",
    "Return JSON: [{{\"name\": \"X\", \"type\": \"PERSON|PLACE|SYSTEM\", \"description\": \"brief\"}}]\n",
    "If none: []'''\n",
    "\n",
    "        try:\n",
    "            response = call_llm(\n",
    "                model=None,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.0,\n",
    "                max_tokens=300\n",
    "            )\n",
    "            result = response.choices[0].message.content.strip()\n",
    "            \n",
    "            start, end = result.find(\"[\"), result.rfind(\"]\")\n",
    "            if start == -1 or end == -1:\n",
    "                return []\n",
    "            \n",
    "            parsed = json_lib.loads(result[start:end+1])\n",
    "            return [{\"name\": e[\"name\"], \"type\": e.get(\"type\", \"UNKNOWN\"), \"description\": e.get(\"description\", \"\")} \n",
    "                    for e in parsed if isinstance(e, dict) and e.get(\"name\")]\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def write_entity(self, name: str, entity_type: str, description: str, llm_client=None, text: str = None):\n",
    "        \"\"\"Store an entity OR extract and store entities from text.\"\"\"\n",
    "        if text and llm_client:\n",
    "            entities = self.extract_entities(text, llm_client)\n",
    "            for e in entities:\n",
    "                self._safe_add_texts(\n",
    "                    self.entity_vs,\n",
    "                    [f\"{e['name']} ({e['type']}): {e['description']}\"],\n",
    "                    [{\"name\": e['name'], \"type\": e['type'], \"description\": e['description']}]\n",
    "                )\n",
    "            return entities\n",
    "        else:\n",
    "            self._safe_add_texts(\n",
    "                self.entity_vs,\n",
    "                [f\"{name} ({entity_type}): {description}\"],\n",
    "                [{\"name\": name, \"type\": entity_type, \"description\": description}]\n",
    "            )\n",
    "    \n",
    "    def read_entity(self, query: str, k: int = 5) -> str:\n",
    "        \"\"\"Search for relevant entities.\"\"\"\n",
    "        results = self.entity_vs.similarity_search(query, k=k)\n",
    "        if not results:\n",
    "            return \"## Entity Memory\\nNo entities found.\"\n",
    "        \n",
    "        entities = [f\"\\u2022 {doc.metadata.get('name', '?')}: {doc.metadata.get('description', '')}\" \n",
    "                    for doc in results if hasattr(doc, 'metadata')]\n",
    "        entities_formatted = '\\n'.join(entities)\n",
    "        return f\"\"\"## Entity Memory: This are the entities that are relevant to the question\n",
    "### How to use: Use the entities to answer the question, especially when having long conversations\n",
    "\n",
    "{entities_formatted}\"\"\"\n",
    "    \n",
    "    # ==================== SUMMARY (Vector Store) ====================\n",
    "    \n",
    "    def write_summary(self, summary_id: str, full_content: str, summary: str, description: str):\n",
    "        \"\"\"Store a summary with its original content.\"\"\"\n",
    "        self._safe_add_texts(\n",
    "            self.summary_vs,\n",
    "            [f\"{summary_id}: {description}\"],\n",
    "            [{\"id\": summary_id, \"full_content\": full_content, \"summary\": summary, \"description\": description}]\n",
    "        )\n",
    "        return summary_id\n",
    "    \n",
    "    def read_summary_memory(self, summary_id: str) -> str:\n",
    "        \"\"\"Retrieve a specific summary by ID (just-in-time retrieval).\"\"\"\n",
    "        results = self.summary_vs.similarity_search(\n",
    "            summary_id, \n",
    "            k=5, \n",
    "            filter={\"id\": summary_id}\n",
    "        )\n",
    "        if not results:\n",
    "            return f\"Summary {summary_id} not found.\"\n",
    "        doc = results[0]\n",
    "        return doc.metadata.get('summary', 'No summary content.')\n",
    "    \n",
    "    def read_summary_context(self, query: str = \"\", k: int = 10) -> str:\n",
    "        \"\"\"Get available summaries for context window (IDs + descriptions only).\"\"\"\n",
    "        results = self.summary_vs.similarity_search(query or \"summary\", k=k)\n",
    "        if not results:\n",
    "            return \"## Summary Memory\\nNo summaries available.\"\n",
    "        \n",
    "        lines = [\"## Summary Memory\", \"Use expand_summary(id) to get full content:\"]\n",
    "        for doc in results:\n",
    "            sid = doc.metadata.get('id', '?')\n",
    "            desc = doc.metadata.get('description', 'No description')\n",
    "            lines.append(f\"  \\u2022 [ID: {sid}] {desc}\")\n",
    "        return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b775acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MemoryManager instance\n",
    "# hnsw_index_names maps table names to their HNSW index names so that\n",
    "# _safe_add_texts can drop/recreate them around writes (Oracle 23ai Free\n",
    "# HNSW does not support DML).\n",
    "memory_manager = MemoryManager(\n",
    "    conn=vector_conn,\n",
    "    conversation_table=CONVERSATION_HISTORY_TABLE,\n",
    "    knowledge_base_vs=knowledge_base_vs,\n",
    "    workflow_vs=workflow_vs,\n",
    "    toolbox_vs=toolbox_vs,\n",
    "    entity_vs=entity_vs,\n",
    "    summary_vs=summary_vs,\n",
    "    hnsw_index_names={\n",
    "        KNOWLEDGE_BASE_TABLE: \"knowledge_base_vs_hnsw\",\n",
    "        WORKFLOW_TABLE: \"workflow_vs_hnsw\",\n",
    "        TOOLBOX_TABLE: \"toolbox_vs_hnsw\",\n",
    "        ENTITY_TABLE: \"entity_vs_hnsw\",\n",
    "        SUMMARY_TABLE: \"summary_vs_hnsw\",\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HNSW Write Pattern: Drop â†’ Insert â†’ Recreate\n",
    "\n",
    "Oracle 23ai Free's HNSW indexes (`ORGANIZATION INMEMORY NEIGHBOR GRAPH`) **do not support DML** â€” any `INSERT`, `UPDATE`, or `DELETE` on a table with an HNSW index raises `ORA-51928`. This is a known limitation of the in-memory graph structure in the Free edition.\n",
    "\n",
    "The `MemoryManager._safe_add_texts()` method works around this by cycling the index on every write:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     _safe_add_texts() Flow                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  1. ROLLBACK         conn.rollback()                               â”‚\n",
    "â”‚     â””â”€ Clear any pending transaction so DDL can execute             â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  2. DROP INDEX       DROP INDEX TOOLBOX_VS_HNSW                    â”‚\n",
    "â”‚     â””â”€ Remove the HNSW index by its known name                     â”‚\n",
    "â”‚     â””â”€ If the index doesn't exist yet, the exception is caught     â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  3. INSERT           vs.add_texts(texts, metadatas)                â”‚\n",
    "â”‚     â””â”€ Now succeeds because the table has no HNSW index            â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  4. CREATE INDEX     CREATE VECTOR INDEX TOOLBOX_VS_HNSW           â”‚\n",
    "â”‚                        ON TOOLBOX_MEMORY(embedding)                â”‚\n",
    "â”‚                        ORGANIZATION INMEMORY NEIGHBOR GRAPH        â”‚\n",
    "â”‚                        WITH TARGET ACCURACY 95                     â”‚\n",
    "â”‚                        DISTANCE COSINE                             â”‚\n",
    "â”‚                        PARAMETERS (type HNSW, neighbors 32,        â”‚\n",
    "â”‚                                    efConstruction 200)             â”‚\n",
    "â”‚                        PARALLEL 8                                  â”‚\n",
    "â”‚     â””â”€ Rebuild the graph index over all rows (old + new)           â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Concrete example â€” `toolbox.register_tool()` calls `write_toolbox()`:**\n",
    "\n",
    "```python\n",
    "# 1. Toolbox registers a tool â†’ calls write_toolbox()\n",
    "memory_manager.write_toolbox(\n",
    "    \"search_tavily Search the web using Tavily API...\",\n",
    "    {\"name\": \"search_tavily\", \"description\": \"...\", ...}\n",
    ")\n",
    "\n",
    "# 2. write_toolbox() delegates to _safe_add_texts()\n",
    "def write_toolbox(self, text, metadata):\n",
    "    self._safe_add_texts(self.toolbox_vs, [text], [metadata])\n",
    "\n",
    "# 3. _safe_add_texts() executes the cycle:\n",
    "#    a) self.conn.rollback()                    â† clear pending txn\n",
    "#    b) DROP INDEX TOOLBOX_VS_HNSW              â† remove HNSW\n",
    "#    c) self.toolbox_vs.add_texts([text], ...)  â† insert row\n",
    "#    d) CREATE VECTOR INDEX TOOLBOX_VS_HNSW ... â† rebuild HNSW\n",
    "```\n",
    "\n",
    "The same cycle applies to all five vector-backed memory types:\n",
    "\n",
    "| Write Method | Vector Store | HNSW Index Name |\n",
    "|---|---|---|\n",
    "| `write_knowledge_base()` | `SEMANTIC_MEMORY` | `KNOWLEDGE_BASE_VS_HNSW` |\n",
    "| `write_workflow()` | `WORKFLOW_MEMORY` | `WORKFLOW_VS_HNSW` |\n",
    "| `write_toolbox()` | `TOOLBOX_MEMORY` | `TOOLBOX_VS_HNSW` |\n",
    "| `write_entity()` | `ENTITY_MEMORY` | `ENTITY_VS_HNSW` |\n",
    "| `write_summary()` | `SUMMARY_MEMORY` | `SUMMARY_VS_HNSW` |\n",
    "\n",
    "**Trade-off:** Each write incurs the cost of a full HNSW rebuild. This is acceptable for this notebook's workload (infrequent writes, small tables), but for high-write-throughput scenarios you'd want IVF indexes instead (which support DML natively)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1879e58",
   "metadata": {},
   "source": [
    "## Creating the Agent's Toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0b03d9",
   "metadata": {},
   "source": [
    "### The Scalability Problem with Tools\n",
    "\n",
    "As your AI system grows, you might have **hundreds of tools** availableâ€”APIs, database queries, calculators, search engines, and more. However, passing all tools to the LLM at inference time creates serious problems:\n",
    "\n",
    "| Problem | Impact |\n",
    "|---------|--------|\n",
    "| **Context bloat** | Tool definitions consume tokens, leaving less room for actual content |\n",
    "| **Tool selection failure** | LLMs struggle to choose the right tool when presented with too many options |\n",
    "| **Increased latency** | More tokens = slower inference |\n",
    "| **Higher costs** | More tokens = higher API costs |\n",
    "\n",
    "Model providers like OpenAI and Anthropic typically recommend limiting the number of tools exposed to an LLM (often 10-20 max for reliable selection).\n",
    "\n",
    "### The Solution: Semantic Tool Retrieval\n",
    "\n",
    "The `Toolbox` class solves this by treating tools as a **searchable memory**:\n",
    "\n",
    "1. **Register hundreds of tools** â€” Store all available tools with their descriptions and embeddings\n",
    "2. **Retrieve only relevant tools** â€” At inference time, use vector search to find tools semantically relevant to the current query\n",
    "3. **Pass a focused toolset** â€” Only the retrieved tools (typically 3-5) are passed to the LLM\n",
    "\n",
    "This approach means your system can **scale to hundreds of tools** while the LLM only sees the most relevant ones for each query.\n",
    "\n",
    "### How the Code Works\n",
    "\n",
    "The `Toolbox` class uses **docstrings as the retrieval key**:\n",
    "\n",
    "```\n",
    "User Query â†’ Embed Query â†’ Vector Search â†’ Find tools with similar docstrings â†’ Return relevant tools\n",
    "```\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| `get_embedding()` | Converts tool description to a vector |\n",
    "| `ToolMetadata` | Pydantic model storing tool name, description, signature, parameters |\n",
    "| `_augment_docstring()` | Uses LLM to improve the docstring for better retrieval |\n",
    "| `_generate_queries()` | Creates synthetic queries that would trigger this tool |\n",
    "| `register_tool()` | Decorator that stores tool with its embedding in the toolbox |\n",
    "\n",
    "When you call `memory_manager.read_toolbox(query)`, it performs a similarity search to find tools whose docstrings are semantically similar to the query.\n",
    "\n",
    "### The Intersection of Three Engineering Disciplines\n",
    "\n",
    "This implementation combines techniques from **memory engineering**, **context engineering**, and **prompt engineering**:\n",
    "\n",
    "| Discipline | Technique Used | How It Helps |\n",
    "|------------|----------------|--------------|\n",
    "| **Memory Engineering** | Toolbox as procedural memory | Tools are stored and retrieved like learned skills |\n",
    "| **Memory Engineering** | Docstring augmentation | LLM improves docstrings for better semantic retrieval |\n",
    "| **Memory Engineering** | Synthetic query generation | Creates example queries to improve tool discoverability |\n",
    "| **Context Engineering** | Selective tool retrieval | Only relevant tools enter the context, reducing bloat |\n",
    "| **Context Engineering** | Context offloading | Tool results can be summarized to save context space |\n",
    "| **Prompt Engineering** | Role setting | \"You are a technical writer\" improves docstring quality |\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "The `augment=True` flag in `@toolbox.register_tool(augment=True)` triggers:\n",
    "1. **Docstring augmentation** â€” LLM rewrites the docstring to be clearer and more searchable\n",
    "2. **Synthetic query generation** â€” LLM generates example queries that would need this tool\n",
    "3. **Rich embedding** â€” Combines name + augmented docstring + signature + queries for better retrieval\n",
    "\n",
    "This means a simple one-line docstring like `\"Search the web\"` becomes a rich, detailed description that's much more likely to be retrieved when the user asks something like `\"What's the latest news about AI?\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8d418dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import uuid\n",
    "from typing import Callable, Optional, Union\n",
    "from pydantic import BaseModel\n",
    "\n",
    "def get_embedding(text: str) -> list[float]:\n",
    "    \"\"\"\n",
    "    Get the embedding for a text using the configured embedding model.\n",
    "    \"\"\"\n",
    "    return embedding_model.embed_query(text)\n",
    "\n",
    "\n",
    "class ToolMetadata(BaseModel):\n",
    "    \"\"\"Metadata for a registered tool.\"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    signature: str\n",
    "    parameters: dict\n",
    "    return_type: str\n",
    "\n",
    "\n",
    "class Toolbox:\n",
    "    \"\"\"\n",
    "    A toolbox for registering, storing, and retrieving tools with LLM-powered augmentation.\n",
    "    \n",
    "    Tools are stored with embeddings for semantic retrieval, allowing the agent to\n",
    "    find relevant tools based on natural language queries.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, memory_manager, llm_client, model: str = \"Qwen/Qwen2.5-72B-Instruct\"):\n",
    "        \"\"\"\n",
    "        Initialize the Toolbox.\n",
    "        \n",
    "        Args:\n",
    "            memory_manager: MemoryManager instance for storing tools\n",
    "            llm_client: OpenAI client for LLM augmentation\n",
    "            model: Model to use for augmentation\n",
    "        \"\"\"\n",
    "        self.memory_manager = memory_manager\n",
    "        self.llm_client = llm_client\n",
    "        self.model = model\n",
    "        self._tools: dict[str, Callable] = {}  # Maps tool_id -> callable\n",
    "        self._tools_by_name: dict[str, Callable] = {}  # Maps function_name -> callable for execution\n",
    "    \n",
    "    def _augment_docstring(self, docstring: str) -> str:\n",
    "        \"\"\"\n",
    "        Use LLM to improve and expand a tool's docstring.\n",
    "        \n",
    "        Takes a basic docstring and returns an enhanced version with:\n",
    "        - Clearer description of what the tool does\n",
    "        - Better formatted parameters and return values\n",
    "        - Usage examples and edge cases\n",
    "        \n",
    "        Args:\n",
    "            docstring: The original docstring to augment\n",
    "            \n",
    "        Returns:\n",
    "            An improved, more detailed docstring\n",
    "        \"\"\"\n",
    "        if not docstring.strip():\n",
    "            return \"No description provided.\"\n",
    "\n",
    "\n",
    "        # NOTE: The role description of a technical writer below is a prompt engineering technique that is used to improve the quality of the docstring\n",
    "        # Athough there are research that suggest that role description doesn't realy affect the quality of the LLM's output, it is still a useful technique\n",
    "        #Â and it is a good [prompt engineering] technique to know.\n",
    "        prompt = f\"\"\"You are a technical writer. Improve the following function docstring to be more clear, \n",
    "            comprehensive, and useful. Include:\n",
    "            1. A clear concise summary\n",
    "            2. Detailed description of what the function does\n",
    "            3. When to use this function\n",
    "            4. Any important notes or caveats\n",
    "\n",
    "            Original docstring:\n",
    "            {docstring}\n",
    "\n",
    "            Return ONLY the improved docstring, no other text.\n",
    "        \"\"\"\n",
    "\n",
    "        response = call_llm(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()\n",
    "    \n",
    "    def _generate_queries(self, docstring: str, num_queries: int = 5) -> list[str]:\n",
    "        \"\"\"\n",
    "        Generate synthetic example queries that would lead to using this tool.\n",
    "        \n",
    "        These queries are used to improve retrieval - by embedding both the tool\n",
    "        description AND example queries, we increase the chances of finding the\n",
    "        right tool when the user asks a related question.\n",
    "        \n",
    "        Args:\n",
    "            docstring: The tool's docstring (ideally augmented)\n",
    "            num_queries: Number of example queries to generate\n",
    "            \n",
    "        Returns:\n",
    "            List of example natural language queries\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"Based on the following tool description, generate {num_queries} diverse example queries \n",
    "            that a user might ask when they need this tool. Make them natural and varied.\n",
    "\n",
    "            Tool description:\n",
    "            {docstring}\n",
    "\n",
    "            Return ONLY a JSON array of strings, like: [\"query1\", \"query2\", ...]\n",
    "        \"\"\"\n",
    "\n",
    "        response = call_llm(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            import json\n",
    "            queries = json.loads(response.choices[0].message.content.strip())\n",
    "            return queries if isinstance(queries, list) else []\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback: extract queries from text\n",
    "            return [response.choices[0].message.content.strip()]\n",
    "    \n",
    "    def _get_tool_metadata(self, func: Callable) -> ToolMetadata:\n",
    "        \"\"\"\n",
    "        Extract metadata from a function for storage and retrieval.\n",
    "        \n",
    "        Args:\n",
    "            func: The function to extract metadata from\n",
    "            \n",
    "        Returns:\n",
    "            ToolMetadata object with function details\n",
    "        \"\"\"\n",
    "        sig = inspect.signature(func)\n",
    "        \n",
    "        # Extract parameter info\n",
    "        parameters = {}\n",
    "        for name, param in sig.parameters.items():\n",
    "            param_info = {\"name\": name}\n",
    "            if param.annotation != inspect.Parameter.empty:\n",
    "                param_info[\"type\"] = str(param.annotation)\n",
    "            if param.default != inspect.Parameter.empty:\n",
    "                param_info[\"default\"] = str(param.default)\n",
    "            parameters[name] = param_info\n",
    "        \n",
    "        # Extract return type\n",
    "        return_type = \"Any\"\n",
    "        if sig.return_annotation != inspect.Signature.empty:\n",
    "            return_type = str(sig.return_annotation)\n",
    "        \n",
    "        return ToolMetadata(\n",
    "            name=func.__name__,\n",
    "            description=func.__doc__ or \"No description\",\n",
    "            signature=str(sig),\n",
    "            parameters=parameters,\n",
    "            return_type=return_type\n",
    "        )\n",
    "    \n",
    "    def register_tool(\n",
    "        self, func: Optional[Callable] = None, augment: bool = False\n",
    "    ) -> Union[str, Callable]:\n",
    "        \"\"\"\n",
    "        Register a function as a tool in the toolbox.\n",
    "\n",
    "        Can be used as a decorator or called directly:\n",
    "        \n",
    "            @toolbox.register_tool\n",
    "            def my_tool(): ...\n",
    "            \n",
    "            @toolbox.register_tool(augment=True)\n",
    "            def my_enhanced_tool(): ...\n",
    "            \n",
    "            tool_id = toolbox.register_tool(some_function)\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        func : Callable, optional\n",
    "            The function to register as a tool. If None, returns a decorator.\n",
    "        augment : bool, optional\n",
    "            Whether to augment the tool docstring and generate synthetic queries\n",
    "            using the configured LLM provider.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        Union[str, Callable]\n",
    "            If func is provided, returns the tool ID. Otherwise returns a decorator.\n",
    "        \"\"\"\n",
    "\n",
    "        def decorator(f: Callable) -> str:\n",
    "            docstring = f.__doc__ or \"\"\n",
    "            signature = str(inspect.signature(f))\n",
    "            object_id = uuid.uuid4()\n",
    "            object_id_str = str(object_id)\n",
    "\n",
    "            # NOTE: Augmentation is a technique that is used to improve the quality of the tool's docstring\n",
    "            #Â by using the LLM to enhance the tool's discoverability and retrieval this is a [memory engineering] technique\n",
    "            if augment:\n",
    "                # Use LLM to enhance the tool's discoverability\n",
    "                augmented_docstring = self._augment_docstring(docstring)\n",
    "                queries = self._generate_queries(augmented_docstring)\n",
    "                \n",
    "                # Create rich embedding text combining all information\n",
    "                embedding_text = f\"{f.__name__} {augmented_docstring} {signature} {' '.join(queries)}\"\n",
    "                embedding = get_embedding(embedding_text)\n",
    "                \n",
    "                tool_data = self._get_tool_metadata(f)\n",
    "                tool_data.description = augmented_docstring  # Use augmented description\n",
    "\n",
    "                tool_dict = {\n",
    "                    \"_id\": object_id_str,  # Use string, not UUID object\n",
    "                    \"embedding\": embedding,\n",
    "                    \"queries\": queries,\n",
    "                    \"augmented\": True,\n",
    "                    **tool_data.model_dump(),\n",
    "                }\n",
    "            else:\n",
    "                # Basic registration without augmentation\n",
    "                embedding = get_embedding(f\"{f.__name__} {docstring} {signature}\")\n",
    "                tool_data = self._get_tool_metadata(f)\n",
    "\n",
    "                tool_dict = {\n",
    "                    \"_id\": object_id_str,  # Use string, not UUID object\n",
    "                    \"embedding\": embedding,\n",
    "                    \"augmented\": False,\n",
    "                    **tool_data.model_dump(),\n",
    "                }\n",
    "\n",
    "            # Store the tool in the toolbox memory for retrieval\n",
    "            # The embedding enables semantic search to find relevant tools\n",
    "            self.memory_manager.write_toolbox(\n",
    "                f\"{f.__name__} {docstring} {signature}\", \n",
    "                tool_dict\n",
    "            )\n",
    "            \n",
    "            # Keep reference to the callable for execution\n",
    "            self._tools[object_id_str] = f\n",
    "            self._tools_by_name[f.__name__] = f  # Also store by name for easy lookup\n",
    "            return object_id_str\n",
    "\n",
    "        if func is None:\n",
    "            return decorator\n",
    "        return decorator(func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b6de020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Function to securely get and set environment variables\n",
    "def set_env_securely(var_name, prompt):\n",
    "    value = getpass.getpass(prompt)\n",
    "    os.environ[var_name] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7eb6bacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using HuggingFace Inference API (Qwen/Qwen2.5-72B-Instruct)\n",
      "   Provider: huggingface_api | Model: Qwen/Qwen2.5-72B-Instruct\n"
     ]
    }
   ],
   "source": [
    "import os, json as _json\n",
    "from openai import OpenAI\n",
    "\n",
    "# ============================================================\n",
    "# LLM CONFIGURATION\n",
    "# ============================================================\n",
    "# This notebook uses HuggingFace for all LLM inference:\n",
    "#\n",
    "#   WITH HF token  â†’ HF Inference API (Qwen2.5-72B, fast, cloud)\n",
    "#   WITHOUT token  â†’ Local Qwen3-0.6B via HuggingFace transformers\n",
    "#\n",
    "# Getting a free HF token (recommended):\n",
    "#   1. Sign up at https://huggingface.co/join (free, no credit card)\n",
    "#   2. Go to https://huggingface.co/settings/tokens\n",
    "#   3. Create a token with \"Read\" access\n",
    "#\n",
    "# Without a token the notebook still works â€” it downloads\n",
    "# Qwen3-0.6B (~500 MB) and runs it locally on CPU.\n",
    "# This is slower but fully functional.\n",
    "# ============================================================\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\").strip()\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    try:\n",
    "        HF_TOKEN = getpass.getpass(\n",
    "            \"HuggingFace Token (press Enter to skip â†’ uses local Qwen3): \"\n",
    "        ).strip()\n",
    "    except:\n",
    "        HF_TOKEN = \"\"\n",
    "\n",
    "# ---- Models ----\n",
    "HF_API_MODEL = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "LOCAL_MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# ================================================================\n",
    "# Local HF model wrapper (mimics OpenAI client interface)\n",
    "# ================================================================\n",
    "class _Obj:\n",
    "    \"\"\"Simple namespace for dot-access.\"\"\"\n",
    "    def __init__(self, **kw):\n",
    "        self.__dict__.update(kw)\n",
    "\n",
    "class LocalHFClient:\n",
    "    \"\"\"Wraps a local HuggingFace model to look like OpenAI's client.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name=LOCAL_MODEL_NAME):\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "        import torch\n",
    "        print(f\"   Downloading {model_name} from HuggingFace Hub...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, dtype=torch.float32, device_map=\"cpu\"\n",
    "        )\n",
    "        self.model_name = model_name\n",
    "        self._torch = torch\n",
    "        # Mimic OpenAI client structure: client.chat.completions.create(...)\n",
    "        self.chat = _Obj(completions=_Obj(create=self._create))\n",
    "\n",
    "    def _create(self, model=None, messages=None, tools=None,\n",
    "                tool_choice=None, temperature=0.4, max_tokens=500, **kw):\n",
    "        import re as _re, uuid as _uuid\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tools=tools if tools else None,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=False,\n",
    "        )\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        with self._torch.no_grad():\n",
    "            output = self.model.generate(\n",
    "                **inputs, max_new_tokens=max_tokens,\n",
    "                temperature=max(temperature, 0.01),\n",
    "                do_sample=True,\n",
    "            )\n",
    "        raw = self.tokenizer.decode(\n",
    "            output[0][inputs[\"input_ids\"].shape[1]:],\n",
    "            skip_special_tokens=False,\n",
    "        )\n",
    "        # Strip end tokens\n",
    "        raw = raw.replace(\"<|im_end|>\", \"\").replace(\"<|endoftext|>\", \"\").strip()\n",
    "\n",
    "        # Parse tool calls if present\n",
    "        tool_calls = None\n",
    "        content = raw\n",
    "        tc_match = _re.findall(r\"<tool_call>\\s*(\\{.*?\\})\\s*</tool_call>\", raw, _re.DOTALL)\n",
    "        if tc_match:\n",
    "            tool_calls = []\n",
    "            for i, tc_json in enumerate(tc_match):\n",
    "                try:\n",
    "                    parsed = _json.loads(tc_json)\n",
    "                    tool_calls.append(_Obj(\n",
    "                        id=f\"call_{_uuid.uuid4().hex[:8]}\",\n",
    "                        type=\"function\",\n",
    "                        function=_Obj(\n",
    "                            name=parsed[\"name\"],\n",
    "                            arguments=_json.dumps(parsed.get(\"arguments\", {})),\n",
    "                        ),\n",
    "                    ))\n",
    "                except:\n",
    "                    pass\n",
    "            if tool_calls:\n",
    "                content = None  # Tool calls = no text content\n",
    "\n",
    "        message = _Obj(\n",
    "            content=content,\n",
    "            tool_calls=tool_calls if tool_calls else None,\n",
    "            role=\"assistant\",\n",
    "        )\n",
    "        return _Obj(\n",
    "            choices=[_Obj(message=message, finish_reason=\"stop\")],\n",
    "            model=model or self.model_name,\n",
    "        )\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Initialize the LLM client\n",
    "# ================================================================\n",
    "llm_provider = None\n",
    "client = None\n",
    "MODEL = None\n",
    "\n",
    "if HF_TOKEN:\n",
    "    try:\n",
    "        hf_client = OpenAI(\n",
    "            base_url=\"https://router.huggingface.co/v1\",\n",
    "            api_key=HF_TOKEN,\n",
    "        )\n",
    "        test = hf_client.chat.completions.create(\n",
    "            model=HF_API_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"hi\"}],\n",
    "            max_tokens=5,\n",
    "        )\n",
    "        client = hf_client\n",
    "        MODEL = HF_API_MODEL\n",
    "        llm_provider = \"huggingface_api\"\n",
    "        print(f\"\\u2705 Using HuggingFace Inference API ({HF_API_MODEL})\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\u26a0\\ufe0f  HF Inference API failed: {str(e)[:120]}\")\n",
    "        print(\"   Falling back to local model...\")\n",
    "\n",
    "if client is None:\n",
    "    try:\n",
    "        local_client = LocalHFClient(LOCAL_MODEL_NAME)\n",
    "        client = local_client\n",
    "        MODEL = LOCAL_MODEL_NAME\n",
    "        llm_provider = \"huggingface_local\"\n",
    "        print(f\"\\u2705 Using local HuggingFace model ({LOCAL_MODEL_NAME})\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            f\"No LLM available.\\n\"\n",
    "            f\"  \\u2022 Set HF_TOKEN for HuggingFace Inference API (free at huggingface.co)\\n\"\n",
    "            f\"  \\u2022 Or ensure PyTorch + transformers are installed for local inference\\n\"\n",
    "            f\"  Error: {e}\"\n",
    "        )\n",
    "\n",
    "print(f\"   Provider: {llm_provider} | Model: {MODEL}\")\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# call_llm() â€” resilient wrapper with auto-fallback\n",
    "# ================================================================\n",
    "def call_llm(messages, tools=None, model=None, temperature=0.4, max_tokens=1000):\n",
    "    \"\"\"Call LLM with automatic fallback from HF API to local model on rate limit.\"\"\"\n",
    "    global client, MODEL, llm_provider\n",
    "\n",
    "    use_model = model or MODEL\n",
    "    kwargs = {\n",
    "        \"model\": use_model, \"messages\": messages,\n",
    "        \"temperature\": temperature, \"max_tokens\": max_tokens,\n",
    "    }\n",
    "    if tools:\n",
    "        kwargs[\"tools\"] = tools\n",
    "        kwargs[\"tool_choice\"] = \"auto\"\n",
    "\n",
    "    try:\n",
    "        return client.chat.completions.create(**kwargs)\n",
    "    except Exception as e:\n",
    "        err = str(e).lower()\n",
    "        if llm_provider == \"huggingface_api\" and (\n",
    "            \"rate\" in err or \"429\" in err or \"limit\" in err or \"credit\" in err\n",
    "        ):\n",
    "            print(f\"\\n\\u26a0\\ufe0f  HF rate limit reached \\u2014 switching to local {LOCAL_MODEL_NAME}...\")\n",
    "            try:\n",
    "                local_fallback = LocalHFClient(LOCAL_MODEL_NAME)\n",
    "                kwargs[\"model\"] = LOCAL_MODEL_NAME\n",
    "                result = local_fallback.chat.completions.create(**kwargs)\n",
    "                client = local_fallback\n",
    "                MODEL = LOCAL_MODEL_NAME\n",
    "                llm_provider = \"huggingface_local\"\n",
    "                print(f\"   Now using local model ({LOCAL_MODEL_NAME})\")\n",
    "                return result\n",
    "            except Exception as e2:\n",
    "                print(f\"   \\u274c Local fallback failed: {e2}\")\n",
    "                raise\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fb27c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Toolbox with the configured LLM client\n",
    "toolbox = Toolbox(memory_manager=memory_manager, llm_client=client, model=MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305bc1bd",
   "metadata": {},
   "source": [
    "# Context Engineering Techniques\n",
    "\n",
    "--------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4ac84b",
   "metadata": {},
   "source": [
    "> **Context engineering** refers to the set of strategies for curating and maintaining the optimal set of tokens (information) during LLM inference, including all the other information that may land there outside of the prompts.\n",
    "> \n",
    "> â€” *Anthropic*\n",
    "\n",
    "While memory engineering focuses on *what to store and retrieve*, context engineering focuses on *how to manage what's in the context window right now*. This includes monitoring usage, compressing information, and providing just-in-time access to details.\n",
    "\n",
    "## What This Section Covers\n",
    "\n",
    "| Step | Function | Purpose |\n",
    "|------|----------|---------|\n",
    "| **1. Calculate Usage** | `calculate_context_usage()` | Monitor what % of the context window is used |\n",
    "| **2. Summarize** | `summarise_context_window()` | Compress long content into summaries using LLM |\n",
    "| **3. Offload** | `offload_to_summary()` | Auto-trigger summarization when usage exceeds threshold |\n",
    "| **4. Just-in-Time Retrieval** | `expand_summary()` tool | Let agent expand summaries on demand |\n",
    "\n",
    "**`Just-In-Time (JIT)`** retrieval is the process of fetching only the information needed at the exact moment the agent requires it, based on the current task, query, or reasoning step. Instead of loading pre-computed or pre-cached context upfront, the system dynamically retrieves the minimal, most relevant data on demand, ensuring efficiency and reducing context overload. In the context of agent memory JIT is a retrieval-control strategy where memory access is triggered by the agentâ€™s current goal, query, or reasoning step. Rather than preloading large histories or the full knowledge base, the system dynamically filters, ranks, and injects only the information that materially influences the next token. This reduces context saturation, improves attention allocation, and increases reasoning fidelity.\n",
    "\n",
    "## The Context Management Flow\n",
    "\n",
    "```\n",
    "Context built â†’ Check usage % â†’ If >80%: Summarize & offload â†’ Store summary with ID\n",
    "                                                              â†“\n",
    "Agent sees: [Summary ID: abc123] Brief description â† Agent can call expand_summary(\"abc123\") if needed\n",
    "```\n",
    "\n",
    "This approach keeps the context lean while giving the agent access to full details when required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d39643d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context window calculator - returns percentage used\n",
    "def calculate_context_usage(context: str, model: str = None) -> dict:\n",
    "    \"\"\"Calculate context window usage as percentage.\"\"\"\n",
    "    estimated_tokens = len(context) // 4  # ~4 chars per token\n",
    "    max_tokens = MODEL_TOKEN_LIMITS.get(model or MODEL, 128000)\n",
    "    percentage = (estimated_tokens / max_tokens) * 100\n",
    "    return {\"tokens\": estimated_tokens, \"max\": max_tokens, \"percent\": round(percentage, 1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ea6db760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context summariser - calls LLM and stores summary\n",
    "import uuid\n",
    "\n",
    "def summarise_context_window(content: str, memory_manager, llm_client, model: str = None) -> dict:\n",
    "    \"\"\"Summarise content using LLM and store in summary memory.\"\"\"\n",
    "    # Call LLM to summarise\n",
    "    response = call_llm(\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Summarise this in 2-3 sentences:\\n{content[:3000]}\"}],\n",
    "        model=model,\n",
    "        max_tokens=200\n",
    "    )\n",
    "    summary = response.choices[0].message.content\n",
    "    \n",
    "    # Generate one-liner description\n",
    "    desc_response = call_llm(\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Write a 10-word label for: {summary}\"}],\n",
    "        model=model,\n",
    "        max_tokens=30\n",
    "    )\n",
    "    description = desc_response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Store in memory\n",
    "    summary_id = str(uuid.uuid4())[:8]\n",
    "    memory_manager.write_summary(summary_id, content, summary, description)\n",
    "    \n",
    "    return {\"id\": summary_id, \"description\": description, \"summary\": summary}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b1a7538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context offloader - replaces content with summary reference\n",
    "def offload_to_summary(context: str, memory_manager, llm_client, threshold_percent: float = 80.0) -> tuple:\n",
    "    \"\"\"If context exceeds threshold, summarise and return compacted version.\"\"\"\n",
    "    usage = calculate_context_usage(context)\n",
    "    \n",
    "    if usage['percent'] < threshold_percent:\n",
    "        return context, []  # No offload needed\n",
    "    \n",
    "    # Summarise the context\n",
    "    result = summarise_context_window(context, memory_manager, llm_client)\n",
    "    \n",
    "    # Return compact reference instead of full content\n",
    "    compact = f\"[Summary ID: {result['id']}] {result['description']}\"\n",
    "    return compact, [result]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c1efd3",
   "metadata": {},
   "source": [
    "### Summary Tools & Conversation Compaction\n",
    "\n",
    "Below we register the `expand_summary` and `summarize_and_store` functions as tools the agent can call.\n",
    "\n",
    "#### Design Logic: Why Mark Instead of Delete?\n",
    "\n",
    "When conversation history grows large, we need to reduce context window usage. We had two choices:\n",
    "\n",
    "| Approach | Pros | Cons |\n",
    "|----------|------|------|\n",
    "| **Delete summarized messages** | Simple, immediate space savings | Permanent data loss, can't audit or recover |\n",
    "| **Mark as summarized (our choice)** | Preserves history, reversible, auditable | Slightly more complex queries |\n",
    "\n",
    "**Our intuition:** Memory should be *compressed*, or *forgotten* not *erased*. By marking messages with a `summary_id` instead of deleting them:\n",
    "\n",
    "1. **Full history is preserved** â€” Original messages remain in the database for auditing, debugging, or reprocessing\n",
    "2. **Linkage is maintained** â€” Each summary knows which messages it represents (via `summary_id`)\n",
    "3. **Reversible** â€” If a summary is deleted, you could \"unsummarize\" by clearing the `summary_id`\n",
    "\n",
    "#### The Flow\n",
    "\n",
    "```\n",
    "Thread has 50 messages â†’ Context too large â†’ summarize_conversation(thread_id)\n",
    "                                                    â†“\n",
    "                        1. Read unsummarized messages\n",
    "                        2. LLM summarizes them\n",
    "                        3. Store summary with unique ID\n",
    "                        4. UPDATE messages SET summary_id = 'abc123'\n",
    "                                                    â†“\n",
    "                        Next read: Only new messages appear + Summary ID reference\n",
    "```\n",
    "\n",
    "This is a form of **log compaction** â€” a pattern borrowed from databases and message queues where old entries are compressed but not lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "54eb2bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary tools for the agent\n",
    "@toolbox.register_tool(augment=True)\n",
    "def expand_summary(summary_id: str) -> str:\n",
    "    \"\"\"Expand a summary reference to full content. Use when you need more details from a [Summary ID: xxx] reference.\"\"\"\n",
    "    return memory_manager.read_summary_memory(summary_id)\n",
    "\n",
    "@toolbox.register_tool(augment=True)\n",
    "def summarize_and_store(text: str, thread_id: str = None) -> str:\n",
    "    \"\"\"Summarize long text and store in memory. Returns a summary ID for later retrieval with expand_summary.\"\"\"\n",
    "    result = summarise_context_window(text, memory_manager, client)\n",
    "    # If thread_id provided, mark conversation messages as summarized\n",
    "    if thread_id:\n",
    "        memory_manager.mark_as_summarized(thread_id, result['id'])\n",
    "    return f\"Stored as [Summary ID: {result['id']}] {result['description']}\"\n",
    "\n",
    "def summarize_conversation(thread_id: str) -> dict:\n",
    "    \"\"\"\n",
    "    Summarize all unsummarized messages in a thread and mark them.\n",
    "    Call this to compact a thread's conversation history.\n",
    "    \"\"\"\n",
    "    # Read current unsummarized messages\n",
    "    conv_memory = memory_manager.read_conversational_memory(thread_id, limit=100)\n",
    "    \n",
    "    if not conv_memory or \"[]\" in conv_memory:\n",
    "        return {\"status\": \"nothing_to_summarize\"}\n",
    "    \n",
    "    # Summarize the conversation\n",
    "    result = summarise_context_window(conv_memory, memory_manager, client)\n",
    "    \n",
    "    # Mark messages as summarized\n",
    "    memory_manager.mark_as_summarized(thread_id, result['id'])\n",
    "    \n",
    "    print(f\"âœ… Conversation summarized: [Summary ID: {result['id']}]\")\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05404f5",
   "metadata": {},
   "source": [
    "# Web Access with Tavily\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cce642",
   "metadata": {},
   "source": [
    "This section demonstrates how to create an **agentic tool** that the LLM can call to search the web. \n",
    "\n",
    "We use [Tavily](https://tavily.com/), an AI-optimized search API designed for LLM applications.\n",
    "\n",
    "## What This Section Does\n",
    "\n",
    "1. **Initialize the Tavily client** â€” Set up the search API with an API key\n",
    "2. **Register `search_tavily` as a tool** â€” Use `@toolbox.register_tool(augment=True)` to make it discoverable\n",
    "3. **Implement the search-and-store pattern** â€” Results are automatically written to knowledge base memory\n",
    "4. **Test tool retrieval** â€” Verify the tool can be found via semantic search\n",
    "\n",
    "## The Search-and-Store Pattern\n",
    "\n",
    "One thing to note is that not only do we get external context that is not available to the Agent at execution, but we persists this to the knowledge base memory and the Agent can reuse this information in subsequent iteration.\n",
    "When the agent calls `search_tavily()`, it doesn't just return resultsâ€”it **persists them to the knowledge base**:\n",
    "\n",
    "```\n",
    "Agent calls search_tavily(\"latest AI news\")\n",
    "       â†“\n",
    "Tavily API returns results\n",
    "       â†“\n",
    "Each result is written to knowledge_base_vs with metadata (title, URL, timestamp)\n",
    "       â†“\n",
    "Future queries can retrieve this information without searching again\n",
    "```\n",
    "\n",
    "This pattern means the agent **learns** from its searches. Information discovered once becomes part of the agent's long-term memory, available for future conversations without additional API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "500d7836",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_env_securely(\"TAVILY_API_KEY\", \"Tavily API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6772460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "from datetime import datetime\n",
    "\n",
    "# Don't forget to set your API key!\n",
    "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "\n",
    "@toolbox.register_tool(augment=True)\n",
    "def search_tavily(query: str, max_results: int = 5):\n",
    "    \"\"\"\n",
    "    Use this function to search the web and store the results in the knowledge base.\n",
    "    \"\"\"\n",
    "    response = tavily_client.search(query=query, max_results=max_results)\n",
    "    results = response.get(\"results\", [])\n",
    "\n",
    "    # Write each result to the knowledge base\n",
    "    for result in results:\n",
    "        # Create the text content to embed\n",
    "        text = f\"Title: {result.get('title', '')}\\nContent: {result.get('content', '')}\\nURL: {result.get('url', '')}\"\n",
    "        \n",
    "        # Create metadata\n",
    "        metadata = {\n",
    "            \"title\": result.get(\"title\", \"\"),\n",
    "            \"url\": result.get(\"url\", \"\"),\n",
    "            \"score\": result.get(\"score\", 0),\n",
    "            \"source_type\": \"tavily_search\",\n",
    "            \"query\": query,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Write to knowledge base\n",
    "        memory_manager.write_knowledge_base(text, metadata)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7c7f8535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'function': {'description': '\"\"\"\\n'\n",
      "                              'Search the web for information and store the '\n",
      "                              'results in the knowledge base.\\n'\n",
      "                              '\\n'\n",
      "                              'This function performs a web search using '\n",
      "                              'predefined query parameters and stores the '\n",
      "                              'retrieved information in a specified knowledge '\n",
      "                              'base. It is particularly useful for automating '\n",
      "                              'the process of gathering and archiving data '\n",
      "                              'from the internet for further analysis or '\n",
      "                              'reference.\\n'\n",
      "                              '\\n'\n",
      "                              'When to use this function:\\n'\n",
      "                              '- When you need to automate the collection of '\n",
      "                              'web-based information.\\n'\n",
      "                              '- For updating a knowledge base with the latest '\n",
      "                              'relevant data.\\n'\n",
      "                              '- In scenarios where continuous monitoring of '\n",
      "                              'specific topics is required.\\n'\n",
      "                              '\\n'\n",
      "                              'Important notes or caveats:\\n'\n",
      "                              '- Ensure that the query parameters are '\n",
      "                              'well-defined to avoid irrelevant results.\\n'\n",
      "                              '- The function may have rate limits or '\n",
      "                              'restrictions depending on the web search '\n",
      "                              'service used.\\n'\n",
      "                              '- Data privacy and copyright laws should be '\n",
      "                              'considered when storing and using the retrieved '\n",
      "                              'information.\\n'\n",
      "                              '\"\"\"',\n",
      "               'name': 'search_tavily',\n",
      "               'parameters': {'properties': {'max_results': {'type': 'integer'},\n",
      "                                             'query': {'type': 'string'}},\n",
      "                              'required': ['query'],\n",
      "                              'type': 'object'}},\n",
      "  'type': 'function'}]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "retrieved_tools = memory_manager.read_toolbox(\"Search the internet\", k=1)\n",
    "pprint.pprint(retrieved_tools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e993b26",
   "metadata": {},
   "source": [
    "# Agent Execution\n",
    "\n",
    "--------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e94e836",
   "metadata": {},
   "source": [
    "This is where everything comes together. We build a complete **agent loop** that integrates all the memory types, context engineering, and tool calling we've implemented.\n",
    "\n",
    "## What This Section Contains\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| `AGENT_SYSTEM_PROMPT` | Instructions telling the LLM how to use memory and tools |\n",
    "| `execute_tool()` | Looks up and executes tools from the toolbox by name |\n",
    "| `call_llm()` | Resilient LLM wrapper with HuggingFace API and local model fallback |\n",
    "| `call_agent()` | The main agent loop that orchestrates everything |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "bc1e5e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json as json_lib\n",
    "\n",
    "# ==================== SYSTEM PROMPT ====================\n",
    "# Below is an example of prompt engineering technique called role description.\n",
    "# It is a technique that is used to improve the quality of the LLM's output.\n",
    "# Although there are research that suggest that role description doesn't realy affect the quality of the LLM's output, it is still a useful technique\n",
    "#Â and it is a good [prompt engineering] technique to know.\n",
    "AGENT_SYSTEM_PROMPT = \"\"\"\n",
    "# System Instructions\n",
    "You are an intelligent assistant with access to memory and tools.\n",
    "\n",
    "IMPORTANT: The user's input contains CONTEXT that has already been retrieved for you:\n",
    "- Conversation Memory: Previous conversations\n",
    "- Knowledge Base Memory: Relevant documents\n",
    "- Summary Memory: Compressed summaries with IDs\n",
    "\n",
    "## Summary Memory\n",
    "When you see [Summary ID: xxx] entries, you can call expand_summary(summary_id) to get the full content.\n",
    "Use this for just-in-time retrieval when you need more details.\n",
    "\n",
    "When answering:\n",
    "1. FIRST, use the context provided in the input\n",
    "2. If you need more detail from a summary, call expand_summary\n",
    "3. Only use search tools if context is insufficient\n",
    "\"\"\"\n",
    "\n",
    "def execute_tool(tool_name: str, tool_args: dict) -> str:\n",
    "    \"\"\"Execute a tool by looking it up in the toolbox.\"\"\"\n",
    "    \n",
    "    if tool_name not in toolbox._tools_by_name:\n",
    "        return f\"Error: Tool '{tool_name}' not found\"\n",
    "    \n",
    "    return str(toolbox._tools_by_name[tool_name](**tool_args) or \"Done\")\n",
    "\n",
    "# ==================== LLM CHAT FUNCTION ====================\n",
    "# call_llm() is defined in the LLM Configuration cell above.\n",
    "# It handles HuggingFace API -> local model fallback automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c797b",
   "metadata": {},
   "source": [
    "## The Agent Loop Flow\n",
    "\n",
    "```\n",
    "1. BUILD CONTEXT\n",
    "   â”œâ”€â”€ Read conversational memory (chat history)\n",
    "   â”œâ”€â”€ Read knowledge base (relevant documents)\n",
    "   â”œâ”€â”€ Read workflow memory (past action patterns)\n",
    "   â”œâ”€â”€ Read entity memory (people, places, systems)\n",
    "   â””â”€â”€ Read summary context (available summary IDs)\n",
    "\n",
    "2. CHECK CONTEXT USAGE\n",
    "   â””â”€â”€ If >80% used â†’ Summarize and offload\n",
    "\n",
    "3. GET TOOLS\n",
    "   â””â”€â”€ Retrieve semantically relevant tools from toolbox\n",
    "\n",
    "4. STORE USER MESSAGE\n",
    "   â””â”€â”€ Write to conversational memory + extract entities\n",
    "\n",
    "5. AGENT LOOP (up to max_iterations)\n",
    "   â”œâ”€â”€ Call LLM with context + tools\n",
    "   â”œâ”€â”€ If tool calls â†’ Execute tools, add results to messages\n",
    "   â””â”€â”€ If no tool calls â†’ Return final answer\n",
    "\n",
    "6. SAVE RESULTS\n",
    "   â”œâ”€â”€ Write workflow (if tools were used)\n",
    "   â”œâ”€â”€ Extract entities from response\n",
    "   â””â”€â”€ Store assistant response in conversational memory\n",
    "```\n",
    "\n",
    "## Key Design Decisions\n",
    "\n",
    "- **Memory is loaded programmatically** â€” The agent always has context without deciding to \"remember\"\n",
    "- **Tools are retrieved semantically** â€” Only relevant tools are passed to the LLM\n",
    "- **Context is monitored** â€” Auto-summarization prevents overflow\n",
    "- **Everything is persisted** â€” Conversations, workflows, and entities are saved for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5ddc7b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== MAIN AGENT LOOP ====================\n",
    "def call_agent(query: str, thread_id: str = \"1\", max_iterations: int = 10) -> str:\n",
    "    \"\"\"Agent loop with context window monitoring and summarization.\"\"\"\n",
    "    thread_id = str(thread_id)\n",
    "    steps = []\n",
    "    summaries = []  # Track created summaries\n",
    "    \n",
    "    # 1. Build context from memory\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ðŸ§  BUILDING CONTEXT...\")\n",
    "    \n",
    "    context = f\"# Question\\n{query}\\n\\n\"\n",
    "    context += memory_manager.read_conversational_memory(thread_id) + \"\\n\\n\"\n",
    "    context += memory_manager.read_knowledge_base(query) + \"\\n\\n\"\n",
    "    context += memory_manager.read_workflow(query) + \"\\n\\n\"\n",
    "    context += memory_manager.read_entity(query) + \"\\n\\n\"\n",
    "    context += memory_manager.read_summary_context(query) + \"\\n\\n\"  # Shows IDs + descriptions\n",
    "\n",
    "    print(\"====CONTEXT WINDOW=====\\n\")\n",
    "    print(context)\n",
    "    \n",
    "    # 2. Check context usage - summarize if >80%\n",
    "    usage = calculate_context_usage(context)\n",
    "    print(f\"ðŸ“Š Context: {usage['percent']}% ({usage['tokens']}/{usage['max']} tokens)\")\n",
    "    \n",
    "    if usage['percent'] > 80:\n",
    "        print(\"âš ï¸ Context >80% - summarizing...\")\n",
    "        context, summaries = offload_to_summary(context, memory_manager, call_llm)\n",
    "        # Add summary references to context\n",
    "        if summaries:\n",
    "            summary_section = \"\\n## Summary Memory\\n\"\n",
    "            for s in summaries:\n",
    "                summary_section += f\"[Summary ID: {s['id']}] {s['description']}\\n\"\n",
    "            context = summary_section + \"\\n\" + context\n",
    "        usage = calculate_context_usage(context)\n",
    "        print(f\"ðŸ“Š After summarization: {usage['percent']}%\")\n",
    "    \n",
    "    # 3. Get tools\n",
    "    dynamic_tools = memory_manager.read_toolbox(query, k=5)\n",
    "    print(f\"ðŸ”§ Tools: {[t['function']['name'] for t in dynamic_tools]}\")\n",
    "    \n",
    "    # 4. Store user message & extract entities\n",
    "    memory_manager.write_conversational_memory(query, \"user\", thread_id)\n",
    "    try:\n",
    "        memory_manager.write_entity(\"\", \"\", \"\", llm_client=client, text=query)\n",
    "    except: pass\n",
    "    \n",
    "    # 5. Agent loop\n",
    "    messages = [{\"role\": \"system\", \"content\": AGENT_SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": context}]\n",
    "    final_answer = \"\"\n",
    "    \n",
    "    print(\"\\nðŸ¤– AGENT LOOP\")\n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"\\n--- Iteration {iteration + 1} ---\")\n",
    "        \n",
    "        response = call_llm(messages, tools=dynamic_tools)\n",
    "        msg = response.choices[0].message\n",
    "        \n",
    "        if msg.tool_calls:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": msg.content, \"tool_calls\": [\n",
    "                {\"id\": tc.id, \"type\": \"function\", \"function\": {\"name\": tc.function.name, \"arguments\": tc.function.arguments}}\n",
    "                for tc in msg.tool_calls\n",
    "            ]})\n",
    "            \n",
    "            for tc in msg.tool_calls:\n",
    "                tool_name = tc.function.name\n",
    "                tool_args = json_lib.loads(tc.function.arguments)\n",
    "                # Format args for display (truncate long values)\n",
    "                args_display = {k: (v[:50] + '...' if isinstance(v, str) and len(v) > 50 else v) \n",
    "                               for k, v in tool_args.items()}\n",
    "                print(f\"ðŸ› ï¸ {tool_name}({args_display})\")\n",
    "                \n",
    "                try:\n",
    "                    result = execute_tool(tool_name, tool_args)\n",
    "                    steps.append(f\"{tool_name}({args_display}) â†’ success\")\n",
    "                except Exception as e:\n",
    "                    result = f\"Error: {e}\"\n",
    "                    steps.append(f\"{tool_name}({args_display}) â†’ failed\")\n",
    "                \n",
    "                print(f\"   â†’ {result[:200]}...\")\n",
    "                messages.append({\"role\": \"tool\", \"tool_call_id\": tc.id, \"content\": result})\n",
    "        else:\n",
    "            final_answer = (msg.content or \"\").replace(\"\\\\n\", \"\\n\")\n",
    "            print(f\"\\nâœ… DONE ({len(steps)} tool calls)\")\n",
    "            break\n",
    "    \n",
    "    # 6. Save workflow & entities\n",
    "    if steps:\n",
    "        memory_manager.write_workflow(query, steps, final_answer)\n",
    "    try:\n",
    "        memory_manager.write_entity(\"\", \"\", \"\", llm_client=client, text=final_answer)\n",
    "    except: pass\n",
    "    memory_manager.write_conversational_memory(final_answer, \"assistant\", thread_id)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50 + f\"\\nðŸ’¬ ANSWER:\\n{final_answer}\\n\" + \"=\"*50)\n",
    "    return final_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a1b2c3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸ§  BUILDING CONTEXT...\n",
      "====CONTEXT WINDOW=====\n",
      "\n",
      "# Question\n",
      "What are the best practices for building AI agents with Oracle Database?\n",
      "\n",
      "## Conversation Memory: This is the conversation history for the current thread\n",
      "### How to use: Use the conversation history to answer the question\n",
      "\n",
      "\n",
      "\n",
      "## Knowledge Base Memory: This are general information that is relevant to the question\n",
      "### How to use: Use the knowledge base as background information that can help answer the question\n",
      "\n",
      "\n",
      "\n",
      "## Workflow Memory\n",
      "No relevant workflows found.\n",
      "\n",
      "## Entity Memory\n",
      "No entities found.\n",
      "\n",
      "## Summary Memory\n",
      "No summaries available.\n",
      "\n",
      "\n",
      "ðŸ“Š Context: 0.1% (138/131072 tokens)\n",
      "ðŸ”§ Tools: ['search_tavily', 'summarize_and_store', 'expand_summary']\n",
      "\n",
      "ðŸ¤– AGENT LOOP\n",
      "\n",
      "--- Iteration 1 ---\n",
      "ðŸ› ï¸ search_tavily({'query': 'best practices for building AI agents with Oracle ...', 'max_results': 5})\n",
      "   â†’ [{'url': 'https://blogs.oracle.com/fusioninsider/get-started-guide-design-and-build-ai-agent-teams', 'title': 'Get started guideâ€”design and build AI agent teams - Oracle Blogs', 'content': 'Best pract...\n",
      "\n",
      "--- Iteration 2 ---\n",
      "\n",
      "âœ… DONE (1 tool calls)\n",
      "\n",
      "==================================================\n",
      "ðŸ’¬ ANSWER:\n",
      "Based on the information gathered, here are some best practices for building AI agents with Oracle Database:\n",
      "\n",
      "1. **Divide Tasks Between Different Worker Agents**:\n",
      "   - Best practice is to divide tasks between different worker agents, which take instructions from a supervisor agent. The supervisor agent reviews and coordinates the activities of the worker agents to ensure efficient and effective task execution.\n",
      "\n",
      "2. **Use Oracle Autonomous AI Database Select AI Agent**:\n",
      "   - The Select AI Agent framework enables you to build, deploy, run, and oversee AI agents fully managed by Oracle Autonomous AI Database. This framework provides a simple, secure, and scalable approach to building and managing intelligent agents within the database.\n",
      "   - **Security**: The framework ensures that data does not need to be shipped outside the database to other frameworks or agent tools, enhancing security.\n",
      "   - **Example**: Define an agent that interacts with a user to provision an Autonomous AI Database instance.\n",
      "\n",
      "3. **Connectivity and Access**:\n",
      "   - When using Generative AI Agents in a hosted region, connect to a private access database that resides in the same region or tenancy.\n",
      "   - Create a database tools connection to enable connectivity.\n",
      "   - Set up a dynamic group with access to vault secrets and Database Tools to ensure proper user access and resource management.\n",
      "\n",
      "4. **Database Table and Function Setup**:\n",
      "   - In the database, create a table with required fields and set up a database function that returns vector search results from queries.\n",
      "   - This setup is crucial for enabling Generative AI Agents to interact with the database effectively.\n",
      "\n",
      "5. **Design Custom Agents with Oracle AI Agent Studio**:\n",
      "   - Oracle AI Agent Studio allows IT admins to set up AI agents in Oracle Fusion Cloud Applications.\n",
      "   - These agents can assist users with various tasks, such as checking paid-time-off balances, pulling up customer purchase histories, processing product returns, and analyzing manufacturing equipment photos.\n",
      "   - Custom agents require AI developers, data scientists, and user interface experts for programming and systems integration.\n",
      "\n",
      "6. **Natural Language Processing (NLP)**:\n",
      "   - AI agents should be designed to understand users' organizational roles and respond to natural language prompts.\n",
      "   - Prebuilt agents can be customized using natural language instructions in an agent design studio or by selecting actions from lists.\n",
      "\n",
      "7. **Procedural Consistency**:\n",
      "   - Ensure that AI agents follow a specific sequence of actions when handling requests to maintain consistency and reliability.\n",
      "   - This can be achieved by defining clear workflows and procedures within the agent design studio.\n",
      "\n",
      "By following these best practices, you can build robust and efficient AI agents that leverage the capabilities of Oracle Database to enhance your applications and workflows.\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Based on the information gathered, here are some best practices for building AI agents with Oracle Database:\\n\\n1. **Divide Tasks Between Different Worker Agents**:\\n   - Best practice is to divide tasks between different worker agents, which take instructions from a supervisor agent. The supervisor agent reviews and coordinates the activities of the worker agents to ensure efficient and effective task execution.\\n\\n2. **Use Oracle Autonomous AI Database Select AI Agent**:\\n   - The Select AI Agent framework enables you to build, deploy, run, and oversee AI agents fully managed by Oracle Autonomous AI Database. This framework provides a simple, secure, and scalable approach to building and managing intelligent agents within the database.\\n   - **Security**: The framework ensures that data does not need to be shipped outside the database to other frameworks or agent tools, enhancing security.\\n   - **Example**: Define an agent that interacts with a user to provision an Autonomous AI Database instance.\\n\\n3. **Connectivity and Access**:\\n   - When using Generative AI Agents in a hosted region, connect to a private access database that resides in the same region or tenancy.\\n   - Create a database tools connection to enable connectivity.\\n   - Set up a dynamic group with access to vault secrets and Database Tools to ensure proper user access and resource management.\\n\\n4. **Database Table and Function Setup**:\\n   - In the database, create a table with required fields and set up a database function that returns vector search results from queries.\\n   - This setup is crucial for enabling Generative AI Agents to interact with the database effectively.\\n\\n5. **Design Custom Agents with Oracle AI Agent Studio**:\\n   - Oracle AI Agent Studio allows IT admins to set up AI agents in Oracle Fusion Cloud Applications.\\n   - These agents can assist users with various tasks, such as checking paid-time-off balances, pulling up customer purchase histories, processing product returns, and analyzing manufacturing equipment photos.\\n   - Custom agents require AI developers, data scientists, and user interface experts for programming and systems integration.\\n\\n6. **Natural Language Processing (NLP)**:\\n   - AI agents should be designed to understand users' organizational roles and respond to natural language prompts.\\n   - Prebuilt agents can be customized using natural language instructions in an agent design studio or by selecting actions from lists.\\n\\n7. **Procedural Consistency**:\\n   - Ensure that AI agents follow a specific sequence of actions when handling requests to maintain consistency and reliability.\\n   - This can be achieved by defining clear workflows and procedures within the agent design studio.\\n\\nBy following these best practices, you can build robust and efficient AI agents that leverage the capabilities of Oracle Database to enhance your applications and workflows.\""
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First query - this triggers a web search and populates the knowledge base\n",
    "call_agent(\"What are the best practices for building AI agents with Oracle Database?\", thread_id=\"0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6g7h8",
   "metadata": {},
   "source": [
    "### Multi-Turn Memory Demo\n",
    "\n",
    "The first call above triggered a **web search** using the `search_tavily` tool. The agent:\n",
    "1. Received the query with empty memory (first interaction)\n",
    "2. Decided to call `search_tavily()` to find relevant information\n",
    "3. Stored the search results in the **knowledge base** vector store\n",
    "4. Saved the conversation in **conversational memory**\n",
    "5. Extracted **entities** from both the query and the response\n",
    "\n",
    "Now, when we run the **follow-up question** below, the agent will:\n",
    "- Retrieve the stored search results from the knowledge base\n",
    "- Have access to the prior conversation via conversational memory\n",
    "- Know about relevant entities\n",
    "\n",
    "This demonstrates **persistent memory across turns** â€” the agent remembers what happened before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9dd88f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸ§  BUILDING CONTEXT...\n",
      "====CONTEXT WINDOW=====\n",
      "\n",
      "# Question\n",
      "These results are good, can you double click into the most relevant one and tell me more about it?\n",
      "\n",
      "## Conversation Memory: This is the conversation history for the current thread\n",
      "### How to use: Use the conversation history to answer the question\n",
      "\n",
      "[22:29:05] [user] What are the best practices for building AI agents with Oracle Database?\n",
      "[22:29:31] [assistant] Based on the information gathered, here are some best practices for building AI agents with Oracle Database:\n",
      "\n",
      "1. **Divide Tasks Between Different Worker Agents**:\n",
      "   - Best practice is to divide tasks between different worker agents, which take instructions from a supervisor agent. The supervisor agent reviews and coordinates the activities of the worker agents to ensure efficient and effective task execution.\n",
      "\n",
      "2. **Use Oracle Autonomous AI Database Select AI Agent**:\n",
      "   - The Select AI Agent framework enables you to build, deploy, run, and oversee AI agents fully managed by Oracle Autonomous AI Database. This framework provides a simple, secure, and scalable approach to building and managing intelligent agents within the database.\n",
      "   - **Security**: The framework ensures that data does not need to be shipped outside the database to other frameworks or agent tools, enhancing security.\n",
      "   - **Example**: Define an agent that interacts with a user to provision an Autonomous AI Database instance.\n",
      "\n",
      "3. **Connectivity and Access**:\n",
      "   - When using Generative AI Agents in a hosted region, connect to a private access database that resides in the same region or tenancy.\n",
      "   - Create a database tools connection to enable connectivity.\n",
      "   - Set up a dynamic group with access to vault secrets and Database Tools to ensure proper user access and resource management.\n",
      "\n",
      "4. **Database Table and Function Setup**:\n",
      "   - In the database, create a table with required fields and set up a database function that returns vector search results from queries.\n",
      "   - This setup is crucial for enabling Generative AI Agents to interact with the database effectively.\n",
      "\n",
      "5. **Design Custom Agents with Oracle AI Agent Studio**:\n",
      "   - Oracle AI Agent Studio allows IT admins to set up AI agents in Oracle Fusion Cloud Applications.\n",
      "   - These agents can assist users with various tasks, such as checking paid-time-off balances, pulling up customer purchase histories, processing product returns, and analyzing manufacturing equipment photos.\n",
      "   - Custom agents require AI developers, data scientists, and user interface experts for programming and systems integration.\n",
      "\n",
      "6. **Natural Language Processing (NLP)**:\n",
      "   - AI agents should be designed to understand users' organizational roles and respond to natural language prompts.\n",
      "   - Prebuilt agents can be customized using natural language instructions in an agent design studio or by selecting actions from lists.\n",
      "\n",
      "7. **Procedural Consistency**:\n",
      "   - Ensure that AI agents follow a specific sequence of actions when handling requests to maintain consistency and reliability.\n",
      "   - This can be achieved by defining clear workflows and procedures within the agent design studio.\n",
      "\n",
      "By following these best practices, you can build robust and efficient AI agents that leverage the capabilities of Oracle Database to enhance your applications and workflows.\n",
      "\n",
      "## Knowledge Base Memory: This are general information that is relevant to the question\n",
      "### How to use: Use the knowledge base as background information that can help answer the question\n",
      "\n",
      "Title: Best practices for making OCI Gen AI Agents follow strict procedural ...\n",
      "Content: Hey everyone, I'm working with OCI Gen AI Agents and trying to make them follow a specific sequence of actions when handling requests.\n",
      "URL: https://www.reddit.com/r/oracle/comments/1ova12t/best_practices_for_making_oci_gen_ai_agents/\n",
      "Title: Get started guideâ€”design and build AI agent teams - Oracle Blogs\n",
      "Content: Best practice is to divide tasks between different worker agents, which take instructions from a supervisor agent. The supervisor agent reviews\n",
      "URL: https://blogs.oracle.com/fusioninsider/get-started-guide-design-and-build-ai-agent-teams\n",
      "Title: RAG Tool Oracle Database Guidelines for Generative AI Agents\n",
      "Content: When using Generative AI Agents in a hosted region, you can connect to a private access database that resides in:. Enable the connectivity by creating a database tools connection in the region where the agent service is available. Enable the connectivity by creating a database tools connection in the tenancy where the agent service is available. In addition to giving user access to all Generative AI Agents resources as described in Adding Policies Before You Can Use the Service, you need to create a dynamic group with access to vault secrets and Database Tools. In the database that you have created to use with Generative AI Agents, create a database table with certain required fields, and set up a database function that returns vector search results from queries. :   Generative AI Agents can access an Autonomous AI Database 26ai through a private endpoint with a connection that's set up for mutual TLS (mTLS) authentication.\n",
      "URL: https://docs.oracle.com/iaas/Content/generative-ai-agents/oracle-db-guidelines.htm\n",
      "\n",
      "## Workflow Memory: This are the past workflows that are relevant to the question\n",
      "### How to use: Use the steps and use them to answer the question, especially when using tools and external sources\n",
      "\n",
      "Query: What are the best practices for building AI agents with Oracle Database?\n",
      "Steps:\n",
      "Step 1: search_tavily({'query': 'best practices for building AI agents with Oracle ...', 'max_results': 5}) â†’ success\n",
      "Answer: Based on the information gathered, here are some best practices for building AI agents with Oracle Database:\n",
      "\n",
      "1. **Divide Tasks Between Different Worker Agents**:\n",
      "   - Best practice is to divide tasks\n",
      "\n",
      "## Entity Memory: This are the entities that are relevant to the question\n",
      "### How to use: Use the entities to answer the question, especially when having long conversations\n",
      "\n",
      "â€¢ Select AI Agent: An AI agent within Oracle Autonomous AI Database designed for specific tasks\n",
      "â€¢ Oracle Database: A relational database management system developed by Oracle Corporation\n",
      "â€¢ Oracle Database: A database management system provided by Oracle Corporation\n",
      "\n",
      "## Summary Memory\n",
      "No summaries available.\n",
      "\n",
      "\n",
      "ðŸ“Š Context: 1.2% (1574/131072 tokens)\n",
      "ðŸ”§ Tools: ['expand_summary', 'search_tavily', 'summarize_and_store']\n",
      "\n",
      "ðŸ¤– AGENT LOOP\n",
      "\n",
      "--- Iteration 1 ---\n",
      "\n",
      "âœ… DONE (0 tool calls)\n",
      "\n",
      "==================================================\n",
      "ðŸ’¬ ANSWER:\n",
      "Sure, let's dive deeper into the most relevant best practice, which is the **use of Oracle Autonomous AI Database Select AI Agent**. This framework is particularly important because it provides a comprehensive and secure way to build and manage AI agents within the Oracle Database environment.\n",
      "\n",
      "### Detailed Information on Oracle Autonomous AI Database Select AI Agent\n",
      "\n",
      "#### Overview\n",
      "The Select AI Agent framework is a part of Oracle Autonomous AI Database, which allows you to build, deploy, run, and oversee AI agents that are fully managed by the database. This framework simplifies the process of creating intelligent agents and ensures that data remains secure within the database.\n",
      "\n",
      "#### Key Features and Benefits\n",
      "\n",
      "1. **Simplified Management**:\n",
      "   - **Centralized Control**: The framework provides a centralized control mechanism for managing multiple AI agents, making it easier to coordinate and monitor their activities.\n",
      "   - **Automated Deployment**: AI agents can be deployed automatically, reducing the manual effort required for setup and maintenance.\n",
      "\n",
      "2. **Enhanced Security**:\n",
      "   - **Data Integrity**: Since the data does not need to be moved outside the database, the risk of data breaches and integrity issues is minimized.\n",
      "   - **Compliance**: The framework helps in maintaining compliance with data protection regulations by keeping sensitive data within the database.\n",
      "\n",
      "3. **Scalability**:\n",
      "   - **Elastic Scaling**: The Oracle Autonomous AI Database can scale resources automatically based on the workload, ensuring that AI agents have the necessary compute power to perform efficiently.\n",
      "   - **High Availability**: The framework supports high availability configurations, ensuring that AI agents are always available to handle requests.\n",
      "\n",
      "4. **Integration**:\n",
      "   - **Seamless Integration**: AI agents can be easily integrated with other Oracle services and third-party applications, enhancing their functionality and utility.\n",
      "   - **API Support**: The framework provides APIs for interacting with AI agents, making it easy to integrate them into existing workflows and applications.\n",
      "\n",
      "#### Example Use Case\n",
      "One practical example of using the Select AI Agent framework is to create an agent that assists users in provisioning an Autonomous AI Database instance. Hereâ€™s how it works:\n",
      "\n",
      "1. **User Interaction**:\n",
      "   - The user initiates a request to provision a new Autonomous AI Database instance through a user interface or API.\n",
      "   \n",
      "2. **Agent Activation**:\n",
      "   - The Select AI Agent framework activates the appropriate agent to handle the provisioning request.\n",
      "   \n",
      "3. **Task Execution**:\n",
      "   - The agent follows a predefined workflow to create the database instance, configure settings, and ensure that all necessary resources are allocated.\n",
      "   \n",
      "4. **Status Updates**:\n",
      "   - The agent provides real-time updates to the user about the progress of the provisioning process.\n",
      "   \n",
      "5. **Completion**:\n",
      "   - Once the database instance is successfully provisioned, the agent notifies the user and provides access credentials.\n",
      "\n",
      "#### Implementation Steps\n",
      "\n",
      "1. **Define the Agent**:\n",
      "   - Use Oracle AI Agent Studio to define the agent, specifying its tasks, workflows, and interactions.\n",
      "   \n",
      "2. **Set Up Database Tables and Functions**:\n",
      "   - Create the necessary database tables and functions to support the agentâ€™s operations. For example, a table to store user requests and a function to return vector search results.\n",
      "   \n",
      "3. **Configure Connectivity**:\n",
      "   - Set up a database tools connection to enable the agent to communicate with the database. Ensure that the connection is secure and complies with all necessary policies.\n",
      "   \n",
      "4. **Test and Deploy**:\n",
      "   - Test the agent thoroughly to ensure it works as expected. Once testing is complete, deploy the agent to the production environment.\n",
      "\n",
      "5. **Monitor and Maintain**:\n",
      "   - Continuously monitor the agentâ€™s performance and make adjustments as needed to optimize its operation.\n",
      "\n",
      "By leveraging the Oracle Autonomous AI Database Select AI Agent framework, you can build robust and secure AI agents that enhance the capabilities of your Oracle Database environment. This approach not only simplifies the development and management of AI agents but also ensures that your data remains protected and compliant.\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sure, let's dive deeper into the most relevant best practice, which is the **use of Oracle Autonomous AI Database Select AI Agent**. This framework is particularly important because it provides a comprehensive and secure way to build and manage AI agents within the Oracle Database environment.\\n\\n### Detailed Information on Oracle Autonomous AI Database Select AI Agent\\n\\n#### Overview\\nThe Select AI Agent framework is a part of Oracle Autonomous AI Database, which allows you to build, deploy, run, and oversee AI agents that are fully managed by the database. This framework simplifies the process of creating intelligent agents and ensures that data remains secure within the database.\\n\\n#### Key Features and Benefits\\n\\n1. **Simplified Management**:\\n   - **Centralized Control**: The framework provides a centralized control mechanism for managing multiple AI agents, making it easier to coordinate and monitor their activities.\\n   - **Automated Deployment**: AI agents can be deployed automatically, reducing the manual effort required for setup and maintenance.\\n\\n2. **Enhanced Security**:\\n   - **Data Integrity**: Since the data does not need to be moved outside the database, the risk of data breaches and integrity issues is minimized.\\n   - **Compliance**: The framework helps in maintaining compliance with data protection regulations by keeping sensitive data within the database.\\n\\n3. **Scalability**:\\n   - **Elastic Scaling**: The Oracle Autonomous AI Database can scale resources automatically based on the workload, ensuring that AI agents have the necessary compute power to perform efficiently.\\n   - **High Availability**: The framework supports high availability configurations, ensuring that AI agents are always available to handle requests.\\n\\n4. **Integration**:\\n   - **Seamless Integration**: AI agents can be easily integrated with other Oracle services and third-party applications, enhancing their functionality and utility.\\n   - **API Support**: The framework provides APIs for interacting with AI agents, making it easy to integrate them into existing workflows and applications.\\n\\n#### Example Use Case\\nOne practical example of using the Select AI Agent framework is to create an agent that assists users in provisioning an Autonomous AI Database instance. Hereâ€™s how it works:\\n\\n1. **User Interaction**:\\n   - The user initiates a request to provision a new Autonomous AI Database instance through a user interface or API.\\n   \\n2. **Agent Activation**:\\n   - The Select AI Agent framework activates the appropriate agent to handle the provisioning request.\\n   \\n3. **Task Execution**:\\n   - The agent follows a predefined workflow to create the database instance, configure settings, and ensure that all necessary resources are allocated.\\n   \\n4. **Status Updates**:\\n   - The agent provides real-time updates to the user about the progress of the provisioning process.\\n   \\n5. **Completion**:\\n   - Once the database instance is successfully provisioned, the agent notifies the user and provides access credentials.\\n\\n#### Implementation Steps\\n\\n1. **Define the Agent**:\\n   - Use Oracle AI Agent Studio to define the agent, specifying its tasks, workflows, and interactions.\\n   \\n2. **Set Up Database Tables and Functions**:\\n   - Create the necessary database tables and functions to support the agentâ€™s operations. For example, a table to store user requests and a function to return vector search results.\\n   \\n3. **Configure Connectivity**:\\n   - Set up a database tools connection to enable the agent to communicate with the database. Ensure that the connection is secure and complies with all necessary policies.\\n   \\n4. **Test and Deploy**:\\n   - Test the agent thoroughly to ensure it works as expected. Once testing is complete, deploy the agent to the production environment.\\n\\n5. **Monitor and Maintain**:\\n   - Continuously monitor the agentâ€™s performance and make adjustments as needed to optimize its operation.\\n\\nBy leveraging the Oracle Autonomous AI Database Select AI Agent framework, you can build robust and secure AI agents that enhance the capabilities of your Oracle Database environment. This approach not only simplifies the development and management of AI agents but also ensures that your data remains protected and compliant.\""
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_agent(\"These results are good, can you double click into the most relevant one and tell me more about it?\", thread_id=\"0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf0305e",
   "metadata": {},
   "source": [
    "## Just-in-Time (JIT) Retrieval Demo\n",
    "\n",
    "The context engineering functions defined above (`summarize_conversation`, `offload_to_summary`, `expand_summary`) form the **JIT retrieval** pipeline. This section demonstrates each stage:\n",
    "\n",
    "1. **Summarize** â€” Compact conversation history into a summary stored in Oracle\n",
    "2. **Inspect** â€” See how summaries replace raw messages in the context window\n",
    "3. **Expand** â€” The agent recalls full details on demand via `expand_summary()`\n",
    "4. **Offload** â€” Automatic context compaction when usage exceeds a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0eccca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ“¦ Marked messages as summarized (summary_id: 8f0ae287)\n",
      "âœ… Conversation summarized: [Summary ID: 8f0ae287]\n",
      "Summary ID:    8f0ae287\n",
      "Description:   Best Practices: Centralized Supervision, Secure Management, Custom Agents, NLP Integration, Procedural Consistency.\n",
      "Summary text:  Best practices for building AI agents with Oracle Database include dividing tasks among worker agents supervised by a central agent, utilizing the Oracle Autonomous AI Database's Select AI Agent framework for secure and scalable management, and ensuring proper connectivity and access through private databases and dynamic groups. Additionally, setting up custom agents with Oracle AI Agent Studio, integrating NLP capabilities, and maintaining procedural consistency are crucial for effective and reliable AI agent performance.\n"
     ]
    }
   ],
   "source": [
    "# Summarize thread 0's conversation history from the two demos above\n",
    "# This exercises: summarise_context_window() â†’ write_summary() â†’ mark_as_summarized()\n",
    "\n",
    "summary_result = summarize_conversation(\"0\")\n",
    "\n",
    "print(f\"Summary ID:    {summary_result.get('id', 'N/A')}\")\n",
    "print(f\"Description:   {summary_result.get('description', 'N/A')}\")\n",
    "print(f\"Summary text:  {summary_result.get('summary', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "46d77edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SUMMARY MEMORY (what the agent sees in context):\n",
      "==================================================\n",
      "## Summary Memory\n",
      "Use expand_summary(id) to get full content:\n",
      "  â€¢ [ID: 8f0ae287] Best Practices: Centralized Supervision, Secure Management, Custom Agents, NLP Integration, Procedural Consistency.\n",
      "\n",
      "==================================================\n",
      "CONVERSATIONAL MEMORY for thread 0 (after summarization):\n",
      "==================================================\n",
      "## Conversation Memory: This is the conversation history for the current thread\n",
      "### How to use: Use the conversation history to answer the question\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect memory state after summarization\n",
    "\n",
    "# 1. Summary context now shows the [Summary ID: xxx] entry\n",
    "print(\"=\" * 50)\n",
    "print(\"SUMMARY MEMORY (what the agent sees in context):\")\n",
    "print(\"=\" * 50)\n",
    "print(memory_manager.read_summary_context())\n",
    "\n",
    "print()\n",
    "\n",
    "# 2. Conversational memory for thread 0 â€” old messages are now filtered out\n",
    "print(\"=\" * 50)\n",
    "print(\"CONVERSATIONAL MEMORY for thread 0 (after summarization):\")\n",
    "print(\"=\" * 50)\n",
    "conv = memory_manager.read_conversational_memory(\"0\")\n",
    "if \"[]\" in conv or not conv.strip():\n",
    "    print(\"(empty â€” all messages have been summarized)\")\n",
    "else:\n",
    "    print(conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6c8787",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "The original conversation messages from thread 0 are **still in the database** â€” they haven't been deleted. Instead, each message was marked with a `summary_id`, which causes `read_conversational_memory()` to filter them out (it only returns messages where `summary_id IS NULL`).\n",
    "\n",
    "The agent's context window now contains a compact `[Summary ID: xxx]` reference instead of the full conversation. When the agent needs the original details, it can call `expand_summary(summary_id)` to retrieve them on demand.\n",
    "\n",
    "```\n",
    "Before:  50 messages in context â†’ high token usage\n",
    "After:   [Summary ID: abc123] Oracle DB best practices discussion â†’ minimal tokens\n",
    "On demand: expand_summary(\"abc123\") â†’ full summary text retrieved from Oracle\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "65d4dc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸ§  BUILDING CONTEXT...\n",
      "====CONTEXT WINDOW=====\n",
      "\n",
      "# Question\n",
      "Remind me what we discussed earlier about Oracle Database best practices â€” I need the specific recommendations.\n",
      "\n",
      "## Conversation Memory: This is the conversation history for the current thread\n",
      "### How to use: Use the conversation history to answer the question\n",
      "\n",
      "\n",
      "\n",
      "## Knowledge Base Memory: This are general information that is relevant to the question\n",
      "### How to use: Use the knowledge base as background information that can help answer the question\n",
      "\n",
      "Title: Get started guideâ€”design and build AI agent teams - Oracle Blogs\n",
      "Content: Best practice is to divide tasks between different worker agents, which take instructions from a supervisor agent. The supervisor agent reviews\n",
      "URL: https://blogs.oracle.com/fusioninsider/get-started-guide-design-and-build-ai-agent-teams\n",
      "Title: Best practices for making OCI Gen AI Agents follow strict procedural ...\n",
      "Content: Hey everyone, I'm working with OCI Gen AI Agents and trying to make them follow a specific sequence of actions when handling requests.\n",
      "URL: https://www.reddit.com/r/oracle/comments/1ova12t/best_practices_for_making_oci_gen_ai_agents/\n",
      "Title: How to Create an AI Agent in 7 Steps - Oracle\n",
      "Content: AI agents are intended to understand usersâ€™ organizational roles, draw on data from business documents so that workflows remain relevant, and respond to natural language prompts instead of precoded instructions. * **AI talent on staff:** Designing custom agents requires AI developers, data scientists, and user interface experts to do the necessary programming and systems integration, whereas application administrators can work in a design studio environment to customize off-the-shelf agents. To define prebuilt agentsâ€™ workflows, admins type specific, natural language instructions into fields in an agent design studio or select actions from lists to specify how the agent should interact with users, display data, or schedule appointments. Oracle AI Agent Studio lets IT admins set up AI agents in Oracle Fusion Cloud Applications that are designed to assist users with a variety of tasks, including calling up their paid-time-off balances, pulling up customersâ€™ purchase histories, processing product returns, and analyzing manufacturing equipment photos to estimate the cost of repairs.\n",
      "URL: https://www.oracle.com/applications/fusion-ai/how-to-create-ai-agent/\n",
      "\n",
      "## Workflow Memory: This are the past workflows that are relevant to the question\n",
      "### How to use: Use the steps and use them to answer the question, especially when using tools and external sources\n",
      "\n",
      "Query: What are the best practices for building AI agents with Oracle Database?\n",
      "Steps:\n",
      "Step 1: search_tavily({'query': 'best practices for building AI agents with Oracle ...', 'max_results': 5}) â†’ success\n",
      "Answer: Based on the information gathered, here are some best practices for building AI agents with Oracle Database:\n",
      "\n",
      "1. **Divide Tasks Between Different Worker Agents**:\n",
      "   - Best practice is to divide tasks\n",
      "\n",
      "## Entity Memory: This are the entities that are relevant to the question\n",
      "### How to use: Use the entities to answer the question, especially when having long conversations\n",
      "\n",
      "â€¢ Oracle Database: A relational database management system developed by Oracle Corporation\n",
      "â€¢ Oracle Database: A database management system provided by Oracle Corporation\n",
      "â€¢ Oracle Autonomous AI Database Select AI Agent: A framework within Oracle Autonomous AI Database for building, deploying, and managing AI agents.\n",
      "â€¢ Select AI Agent: An AI agent within Oracle Autonomous AI Database designed for specific tasks\n",
      "\n",
      "## Summary Memory\n",
      "Use expand_summary(id) to get full content:\n",
      "  â€¢ [ID: 8f0ae287] Best Practices: Centralized Supervision, Secure Management, Custom Agents, NLP Integration, Procedural Consistency.\n",
      "\n",
      "\n",
      "ðŸ“Š Context: 0.7% (934/131072 tokens)\n",
      "ðŸ”§ Tools: ['expand_summary', 'search_tavily', 'summarize_and_store']\n",
      "\n",
      "ðŸ¤– AGENT LOOP\n",
      "\n",
      "--- Iteration 1 ---\n",
      "\n",
      "âœ… DONE (0 tool calls)\n",
      "\n",
      "==================================================\n",
      "ðŸ’¬ ANSWER:\n",
      "Based on our previous discussion and the information available, here are the specific recommendations for best practices when building AI agents with Oracle Database:\n",
      "\n",
      "1. **Centralized Supervision**:\n",
      "   - Use a supervisor agent to manage and review tasks performed by worker agents. This ensures that all actions are coordinated and consistent.\n",
      "\n",
      "2. **Secure Management**:\n",
      "   - Implement robust security measures to protect sensitive data and ensure compliance with regulatory requirements. This includes secure authentication, authorization, and encryption.\n",
      "\n",
      "3. **Custom Agents**:\n",
      "   - Tailor AI agents to specific tasks and workflows. Custom agents can be designed to handle specialized tasks such as data processing, customer service, or predictive analytics.\n",
      "\n",
      "4. **NLP Integration**:\n",
      "   - Integrate natural language processing (NLP) capabilities to enable AI agents to understand and respond to user queries in a more human-like manner. This improves user interaction and satisfaction.\n",
      "\n",
      "5. **Procedural Consistency**:\n",
      "   - Ensure that AI agents follow a specific sequence of actions when handling requests. This helps maintain consistency and reliability in the system.\n",
      "\n",
      "If you need more detailed information on any of these points, let me know!\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on our previous discussion and the information available, here are the specific recommendations for best practices when building AI agents with Oracle Database:\\n\\n1. **Centralized Supervision**:\\n   - Use a supervisor agent to manage and review tasks performed by worker agents. This ensures that all actions are coordinated and consistent.\\n\\n2. **Secure Management**:\\n   - Implement robust security measures to protect sensitive data and ensure compliance with regulatory requirements. This includes secure authentication, authorization, and encryption.\\n\\n3. **Custom Agents**:\\n   - Tailor AI agents to specific tasks and workflows. Custom agents can be designed to handle specialized tasks such as data processing, customer service, or predictive analytics.\\n\\n4. **NLP Integration**:\\n   - Integrate natural language processing (NLP) capabilities to enable AI agents to understand and respond to user queries in a more human-like manner. This improves user interaction and satisfaction.\\n\\n5. **Procedural Consistency**:\\n   - Ensure that AI agents follow a specific sequence of actions when handling requests. This helps maintain consistency and reliability in the system.\\n\\nIf you need more detailed information on any of these points, let me know!'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask the agent about the previous conversation â€” it will see the summary ID\n",
    "# in its context and should call expand_summary() to recall the details\n",
    "\n",
    "call_agent(\n",
    "    \"Remind me what we discussed earlier about Oracle Database best practices \"\n",
    "    \"â€” I need the specific recommendations.\",\n",
    "    thread_id=\"0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b349f",
   "metadata": {},
   "source": [
    "### Automatic Context Compaction with `offload_to_summary()`\n",
    "\n",
    "In the agent loop (`call_agent`), context is automatically compacted when it exceeds **80% of the model's token limit**. With a 128k-token model, that threshold (~384k characters) is hard to reach in a demo.\n",
    "\n",
    "Below we call `offload_to_summary()` directly with a **1% threshold** so it triggers immediately on any content. This shows the before/after token usage â€” the same mechanism the agent uses automatically at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9dd7f822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE offload:\n",
      "  Tokens:  5,800\n",
      "  Usage:   4.4%\n",
      "  Length:  23,200 chars\n",
      "\n",
      "AFTER offload:\n",
      "  Tokens:  25\n",
      "  Usage:   0.0%\n",
      "  Length:  101 chars\n",
      "  Compacted to: [Summary ID: 01a65031] Oracle AI Vector Search: Efficient Embedding Queries Enhancing Data Retrieval.\n",
      "\n",
      "Reduction: 5,775 tokens (99.6%)\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate offload_to_summary() with a low threshold to force compaction\n",
    "\n",
    "# Build a sample context\n",
    "large_context = (\n",
    "    \"Important document about AI agents and Oracle Database. \"\n",
    "    \"Oracle AI Vector Search enables similarity search on embeddings stored natively in the database. \"\n",
    "    \"Agents can use tool calling to retrieve relevant documents from vector stores. \"\n",
    ") * 100\n",
    "\n",
    "usage_before = calculate_context_usage(large_context)\n",
    "print(f\"BEFORE offload:\")\n",
    "print(f\"  Tokens:  {usage_before['tokens']:,}\")\n",
    "print(f\"  Usage:   {usage_before['percent']}%\")\n",
    "print(f\"  Length:  {len(large_context):,} chars\")\n",
    "print()\n",
    "\n",
    "# Offload with 1% threshold â€” triggers immediately\n",
    "compacted, summaries = offload_to_summary(\n",
    "    large_context, memory_manager, client, threshold_percent=1.0\n",
    ")\n",
    "\n",
    "usage_after = calculate_context_usage(compacted)\n",
    "print(f\"AFTER offload:\")\n",
    "print(f\"  Tokens:  {usage_after['tokens']:,}\")\n",
    "print(f\"  Usage:   {usage_after['percent']}%\")\n",
    "print(f\"  Length:  {len(compacted):,} chars\")\n",
    "print(f\"  Compacted to: {compacted}\")\n",
    "print()\n",
    "\n",
    "reduction = usage_before['tokens'] - usage_after['tokens']\n",
    "print(f\"Reduction: {reduction:,} tokens ({100 - (usage_after['tokens'] / usage_before['tokens'] * 100):.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
