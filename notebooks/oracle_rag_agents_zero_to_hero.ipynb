{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f3a21bc",
   "metadata": {},
   "source": [
    "# Oracle RAG Agents: Zero to Hero\n",
    "\n",
    "[![Open in Colab](https://img.shields.io/badge/Open%20in-Colab-F9AB00?style=flat-square&logo=googlecolab)](https://colab.research.google.com/github/oracle-devrel/oracle-ai-developer-hub/blob/main/notebooks/oracle_rag_agents_zero_to_hero.ipynb)\n",
    "\n",
    "-------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b7f4b9",
   "metadata": {},
   "source": [
    "## What You Will Learn\n",
    "\n",
    "In this notebook, you will learn how to:\n",
    "- Set up Oracle AI Database locally for AI application development.\n",
    "- Build a research dataset pipeline from ingestion to embeddings.\n",
    "- Implement keyword, vector, hybrid, and graph-based retrieval in Oracle.\n",
    "- Create a full RAG workflow and connect it to OpenAI models.\n",
    "- Build agentic workflows with tool use, orchestration, and session memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e322ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -Uq oracledb pandas sentence-transformers datasets einops \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010cf528",
   "metadata": {},
   "source": [
    "# Part 1. Oracle AI Database Local Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20816044",
   "metadata": {},
   "source": [
    "## 1.1 Local Installation of Oracle AI Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c687fd63",
   "metadata": {},
   "source": [
    "**Oracle AI Database 26ai** is a **converged database** built for AI developers.  \n",
    "It unifies **relational, document, graph and vector data** in one engine ‚Äî so you can build  \n",
    "**semantic search**, **RAG**, and **natural language to SQL** applications without leaving the database.  \n",
    "\n",
    "Store embeddings, run vector search, and apply AI directly where your data lives ‚Äî  \n",
    "**securely, efficiently, and at scale.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad10f4",
   "metadata": {},
   "source": [
    "For this notebook we will be using a local installation of [Oracle AI Database](https://www.oracle.com/database/free/get-started/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0308b61d",
   "metadata": {},
   "source": [
    "1. Install & start Docker. Docker Desktop (Mac/Windows) or Docker Engine (Linux). Make sure it‚Äôs running.\n",
    "    - If installed with Docker Enginer, run from terminal ```open /Applications/Docker.app```\n",
    "2. Pull [docker image](https://www.oracle.com/database/free/get-started/)\n",
    "3. Run a container with oracle image\n",
    "\n",
    "    ```\n",
    "    docker run -d \\\n",
    "      --name oracle-full \\\n",
    "      -p 1521:1521 -p 5500:5500 \\\n",
    "      -e ORACLE_PWD=YourStrongPassword \\\n",
    "      -e ORACLE_SID=FREE \\\n",
    "      -e ORACLE_PDB=FREEPDB1 \\\n",
    "      -v ~/oracle/full_data:/opt/oracle/oradata \\\n",
    "      container-registry.oracle.com/database/free:latest\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0da924",
   "metadata": {},
   "source": [
    "> üö´ **Troubleshoot**  \n",
    "> If you see the error:  \n",
    "> *`docker: Error response from daemon: Conflict. The container name \"/oracle-full\" is already in use by container ... You have to remove (or rename) that container to be able to reuse that name.`*  \n",
    ">\n",
    "> üß© **Fix:**  \n",
    "> - Remove the existing container:  \n",
    ">   ```bash\n",
    ">   docker rm oracle-full\n",
    ">   ```  \n",
    "> - Then re-run your Docker command from **Step 3** to start a new container.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c21e0d",
   "metadata": {},
   "source": [
    "After running the docker command above in your terminal, you should see the image below if you click into the container running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c1481",
   "metadata": {},
   "source": [
    "### 1.1.1 Connecting to Oracle AI Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de8defa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Granted READ ANY PROPERTY GRAPH to VECTOR.\n",
      "‚úì Granted CREATE PROPERTY GRAPH to VECTOR.\n",
      "Connection attempt 1/3...\n",
      "‚úì Connected successfully!\n",
      "\n",
      "Oracle AI Database 26ai Free Release 23.26.0.0.0 - Develop, Learn, and Run for Free\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import oracledb\n",
    "import time\n",
    "\n",
    "\n",
    "def ensure_property_graph_privileges(\n",
    "    target_user: str = \"VECTOR\",\n",
    "    admin_user: str = \"SYSTEM\",\n",
    "    admin_password: str | None = None,\n",
    "    dsn: str = \"localhost:1521/FREEPDB1\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Grant SQL Property Graph privileges required by this notebook.\n",
    "\n",
    "    Uses an admin session (SYSTEM by default) to grant:\n",
    "      - CREATE PROPERTY GRAPH (required)\n",
    "      - READ ANY PROPERTY GRAPH (optional; useful for cross-schema graph reads)\n",
    "\n",
    "    Set ORACLE_ADMIN_PWD in your environment if admin password differs.\n",
    "    \"\"\"\n",
    "    admin_password = (\n",
    "        admin_password\n",
    "        or os.environ.get(\"ORACLE_ADMIN_PWD\")\n",
    "        or os.environ.get(\"ORACLE_PASSWORD\")\n",
    "        or \"YourStrongPassword\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with oracledb.connect(user=admin_user, password=admin_password, dsn=dsn) as admin_conn:\n",
    "            with admin_conn.cursor() as cur:\n",
    "                # Required for CREATE PROPERTY GRAPH DDL\n",
    "                cur.execute(f\"GRANT CREATE PROPERTY GRAPH TO {target_user}\")\n",
    "\n",
    "                # Optional in some setups/versions; ignore if unsupported\n",
    "                try:\n",
    "                    cur.execute(f\"GRANT READ ANY PROPERTY GRAPH TO {target_user}\")\n",
    "                    print(f\"‚úì Granted READ ANY PROPERTY GRAPH to {target_user}.\")\n",
    "                except oracledb.DatabaseError as read_err:\n",
    "                    if \"ORA-00990\" in str(read_err):\n",
    "                        print(\"‚ÑπÔ∏è READ ANY PROPERTY GRAPH is not available in this setup; continuing.\")\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "            admin_conn.commit()\n",
    "\n",
    "        print(f\"‚úì Granted CREATE PROPERTY GRAPH to {target_user}.\")\n",
    "\n",
    "    except oracledb.DatabaseError as e:\n",
    "        print(f\"‚ö†Ô∏è Could not auto-grant property graph privileges as {admin_user}: {e}\")\n",
    "        print(\"   If you hit ORA-01031 later, run as admin:\")\n",
    "        print(f\"   GRANT CREATE PROPERTY GRAPH TO {target_user};\")\n",
    "        print(f\"   -- Optional if supported: GRANT READ ANY PROPERTY GRAPH TO {target_user};\")\n",
    "\n",
    "\n",
    "def connect_to_oracle(max_retries=3, retry_delay=5):\n",
    "    \"\"\"\n",
    "    Connect to Oracle database with retry logic and better error handling.\n",
    "\n",
    "    Args:\n",
    "        max_retries: Maximum number of connection attempts\n",
    "        retry_delay: Seconds to wait between retries\n",
    "    \"\"\"\n",
    "    user = \"VECTOR\"\n",
    "    password = \"VectorPwd_2025\"  # must match ORACLE_PWD from docker run\n",
    "    dsn = \"localhost:1521/FREEPDB1\"\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(f\"Connection attempt {attempt}/{max_retries}...\")\n",
    "            conn = oracledb.connect(\n",
    "                user=user,\n",
    "                password=password,\n",
    "                dsn=dsn\n",
    "            )\n",
    "            print(\"‚úì Connected successfully!\")\n",
    "\n",
    "            # Test the connection\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(\"SELECT banner FROM v$version WHERE banner LIKE 'Oracle%';\")\n",
    "                banner = cur.fetchone()[0]\n",
    "                print(f\"\\n{banner}\")\n",
    "\n",
    "            return conn\n",
    "\n",
    "        except oracledb.OperationalError as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"‚úó Connection failed (attempt {attempt}/{max_retries})\")\n",
    "\n",
    "            if \"DPY-4011\" in error_msg or \"Connection reset by peer\" in error_msg:\n",
    "                print(\"  ‚Üí This usually means:\")\n",
    "                print(\"    1. Database is still starting up (wait 2-3 minutes)\")\n",
    "                print(\"    2. Listener is not bound to 0.0.0.0 (run fix_oracle_listener())\")\n",
    "                print(\"    3. Container is not running (check with check_docker_container())\")\n",
    "\n",
    "                if attempt < max_retries:\n",
    "                    print(f\"\\n  Waiting {retry_delay} seconds before retry...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                else:\n",
    "                    print(\"\\n  üí° Try running:\")\n",
    "                    print(\"     1. check_docker_container() - verify container is running\")\n",
    "                    print(\"     2. fix_oracle_listener() - fix listener binding\")\n",
    "                    raise\n",
    "            else:\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Unexpected error: {e}\")\n",
    "            raise\n",
    "\n",
    "    raise ConnectionError(\"Failed to connect after all retries\")\n",
    "\n",
    "\n",
    "# Ensure VECTOR can create/read SQL Property Graph objects\n",
    "ensure_property_graph_privileges(\n",
    "    target_user=\"VECTOR\",\n",
    "    admin_user=\"SYSTEM\",\n",
    "    dsn=\"localhost:1521/FREEPDB1\",\n",
    ")\n",
    "\n",
    "# Connect to Oracle as VECTOR\n",
    "conn = connect_to_oracle()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31988f1d",
   "metadata": {},
   "source": [
    "> üö´ Troubleshoot: Oracle Database Free (Docker) ‚Äî Connection Fix\n",
    ">\n",
    "> If you see errors like:\n",
    ">\n",
    "> ```\n",
    "> OperationalError: DPY-6005: cannot connect to database\n",
    "> DPY-4011: the database or network closed the connection\n",
    "> TNS-12545: Connect failed because target host or object does not exist\n",
    "> ```\n",
    ">\n",
    "> It means the **Oracle listener** inside the container is binding to the **container hostname** instead of `0.0.0.0`, preventing host connections.\n",
    ">\n",
    ">\n",
    ">\n",
    "> üß© Fix\n",
    "> \n",
    "> Run this exact command to patch the listener and restart it:\n",
    "> \n",
    "> ```bash\n",
    "> docker exec -it oracle-full bash -lc '\n",
    ">   export ORACLE_HOME=${ORACLE_HOME:-/opt/oracle/product/26ai/dbhomeFree}\n",
    ">   export PATH=$ORACLE_HOME/bin:$PATH\n",
    ">   LISTENER_ORA=\"$ORACLE_HOME/network/admin/listener.ora\"\n",
    "> \n",
    ">   echo \"== Fixing listener to use HOST=0.0.0.0\"\n",
    ">   sed -i \"s/(HOST *= *[^)]*)/(HOST = 0.0.0.0)/\" \"$LISTENER_ORA\"\n",
    "> \n",
    ">   echo \"== Restarting listener\"\n",
    ">   lsnrctl stop || true\n",
    ">   lsnrctl start\n",
    "> \n",
    ">   echo \"== Re-registering services\"\n",
    ">   echo \"ALTER SYSTEM REGISTER;\" | sqlplus -s / as sysdba\n",
    "> \n",
    ">   echo \"== Listener status (first 20 lines):\"\n",
    ">   lsnrctl status | sed -n \"1,20p\"\n",
    "> '\n",
    "> ```\n",
    "> \n",
    "> This:\n",
    "> - Forces the listener to bind on all interfaces (`0.0.0.0`).\n",
    "> - Restarts the listener.\n",
    "> - Re-registers the PDB service (`FREEPDB1`) with the listener.\n",
    "> \n",
    "> ---\n",
    "> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d714bb2",
   "metadata": {},
   "source": [
    "## 1.2 Remote Access with FreeSQL.com (Coming Soon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c24fa8",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02032d60",
   "metadata": {},
   "source": [
    "# Part 2. Data Loading, Preparation and Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d8e19e",
   "metadata": {},
   "source": [
    "## 2.1 Data Loading From Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ddae6",
   "metadata": {},
   "source": [
    "**Streaming the ArXiv Papers Dataset with Hugging Face**\n",
    "\n",
    "The following code in the next cell demonstrates how to efficiently load and stream a large dataset using the **Hugging Face `datasets`** library ‚Äî a powerful tool for handling massive datasets that may not fit into memory all at once.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds_stream = load_dataset(\"nick007x/arxiv-papers\", split=\"train\", streaming=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6cc928",
   "metadata": {},
   "source": [
    "**Step-by-Step Explanation**\n",
    "1. **Importing dependencies**\n",
    "   - `load_dataset` is imported from the `datasets` library, giving access to thousands of open datasets hosted on the Hugging Face Hub.\n",
    "\n",
    "2. **Loading the dataset**\n",
    "   - The dataset `\"nick007x/arxiv-papers\"` refers to a public dataset on the Hugging Face Hub that contains metadata or text from research papers hosted on [arXiv.org](https://arxiv.org).\n",
    "   - The parameter `split=\"train\"` loads the training partition of the dataset (many datasets have `train`, `validation`, and `test` splits).\n",
    "   - The key argument `streaming=True` activates **streaming mode**, meaning the dataset is read progressively from the source without downloading it entirely to disk.\n",
    "\n",
    "3. **Why streaming mode matters**\n",
    "   - Traditional dataset loading downloads the full dataset into memory or disk, which can be slow and memory-intensive.  \n",
    "   - Streaming allows you to process examples **as they arrive**, ideal for very large datasets or limited-resource environments.\n",
    "   - You can iterate over the dataset like this:\n",
    "     ```python\n",
    "     for record in ds_stream:\n",
    "         print(record)\n",
    "         break\n",
    "     ```\n",
    "\n",
    "4. **Resulting object**\n",
    "   - The variable `ds_stream` is an instance of `datasets.IterableDataset`, not a static table.  \n",
    "   - You can convert a small sample into a Pandas DataFrame for inspection:\n",
    "     ```python\n",
    "     sample = [next(iter(ds_stream)) for _ in range(5)]\n",
    "     pd.DataFrame(sample)\n",
    "     ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b06e64",
   "metadata": {},
   "source": [
    "> **üí° Takeaway:**  \n",
    "> Using `load_dataset(..., streaming=True)` enables developers and data scientists to work with **large datasets efficiently** ‚Äî a perfect fit for machine learning pipelines, LLM training, or large document analysis workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ee654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds_stream = load_dataset(\"nick007x/arxiv-papers\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8e1adc",
   "metadata": {},
   "source": [
    "The code below streams the first 1,000 ArXiv papers(feel free to use more) from the dataset, extracts their titles and abstracts, combines them into a single text field, and stores the results as structured dictionaries for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5ebcc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Streamed 1000 papers.\n"
     ]
    }
   ],
   "source": [
    "sampled = []\n",
    "for i, item in enumerate(ds_stream):\n",
    "    if i >= 1000:\n",
    "        break\n",
    "    \n",
    "    arxiv_id = item.get(\"arxiv_id\", f\"unknown_{i}\")\n",
    "    title = item.get(\"title\", \"\").strip()\n",
    "    abstract = item.get(\"abstract\", \"\").strip()\n",
    "    authors = item.get(\"authors\", [])\n",
    "\n",
    "    if isinstance(authors, str):\n",
    "        authors = [a.strip() for a in authors.split(\",\") if a.strip()]\n",
    "    elif isinstance(authors, list):\n",
    "        authors = [str(a).strip() for a in authors if str(a).strip()]\n",
    "    else:\n",
    "        authors = []\n",
    "    \n",
    "    # Combine title + abstract for embedding text\n",
    "    text = f\"{title} ‚Äî {abstract}\"\n",
    "    \n",
    "    sampled.append({\n",
    "        \"id\": item.get(\"id\", f\"arxiv_{i}\"),\n",
    "        \"arxiv_id\": arxiv_id,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"authors\": authors,\n",
    "        \"text\": text\n",
    "    })\n",
    "\n",
    "print(f\"‚úÖ Streamed {len(sampled)} papers.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaae87a",
   "metadata": {},
   "source": [
    "The code below converts the collected list of sampled paper records into a Pandas DataFrame for easier analysis, prints how many rows were loaded, and displays the first few entries to preview the dataset‚Äôs structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b655b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 1000 rows into DataFrame.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arxiv_0</td>\n",
       "      <td>0902.3253</td>\n",
       "      <td>The gravitational wave background from star-ma...</td>\n",
       "      <td>Stars on eccentric orbits around a massive bla...</td>\n",
       "      <td>[Silvia Toonen, Clovis Hopman, Marc Freitag]</td>\n",
       "      <td>The gravitational wave background from star-ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arxiv_1</td>\n",
       "      <td>0902.0428</td>\n",
       "      <td>Dynamics of planets in retrograde mean motion ...</td>\n",
       "      <td>In a previous paper (Gayon &amp;amp; Bois 2008a), ...</td>\n",
       "      <td>[Julie Gayon, Eric Bois, Hans Scholl]</td>\n",
       "      <td>Dynamics of planets in retrograde mean motion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arxiv_2</td>\n",
       "      <td>0901.3401</td>\n",
       "      <td>Diurnal Thermal Tides in a Non-synchronized Ho...</td>\n",
       "      <td>We perform a linear analysis to investigate th...</td>\n",
       "      <td>[Pin-Gao Gu, Gordon I. Ogilvie]</td>\n",
       "      <td>Diurnal Thermal Tides in a Non-synchronized Ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arxiv_3</td>\n",
       "      <td>0901.1570</td>\n",
       "      <td>Intermittent turbulence, noisy fluctuations an...</td>\n",
       "      <td>Recent research has shown that distinct physic...</td>\n",
       "      <td>[Z. V√∂r√∂s, T. L. Zhang, M. P. Leubner, M. Volw...</td>\n",
       "      <td>Intermittent turbulence, noisy fluctuations an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arxiv_4</td>\n",
       "      <td>0901.2048</td>\n",
       "      <td>Falling Transiting Extrasolar Giant Planets</td>\n",
       "      <td>We revisit the tidal stability of extrasolar s...</td>\n",
       "      <td>[B. Levrard, C. Winisdoerffer, G. Chabrier]</td>\n",
       "      <td>Falling Transiting Extrasolar Giant Planets ‚Äî ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   arxiv_id                                              title  \\\n",
       "0  arxiv_0  0902.3253  The gravitational wave background from star-ma...   \n",
       "1  arxiv_1  0902.0428  Dynamics of planets in retrograde mean motion ...   \n",
       "2  arxiv_2  0901.3401  Diurnal Thermal Tides in a Non-synchronized Ho...   \n",
       "3  arxiv_3  0901.1570  Intermittent turbulence, noisy fluctuations an...   \n",
       "4  arxiv_4  0901.2048        Falling Transiting Extrasolar Giant Planets   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Stars on eccentric orbits around a massive bla...   \n",
       "1  In a previous paper (Gayon &amp; Bois 2008a), ...   \n",
       "2  We perform a linear analysis to investigate th...   \n",
       "3  Recent research has shown that distinct physic...   \n",
       "4  We revisit the tidal stability of extrasolar s...   \n",
       "\n",
       "                                             authors  \\\n",
       "0       [Silvia Toonen, Clovis Hopman, Marc Freitag]   \n",
       "1              [Julie Gayon, Eric Bois, Hans Scholl]   \n",
       "2                    [Pin-Gao Gu, Gordon I. Ogilvie]   \n",
       "3  [Z. V√∂r√∂s, T. L. Zhang, M. P. Leubner, M. Volw...   \n",
       "4        [B. Levrard, C. Winisdoerffer, G. Chabrier]   \n",
       "\n",
       "                                                text  \n",
       "0  The gravitational wave background from star-ma...  \n",
       "1  Dynamics of planets in retrograde mean motion ...  \n",
       "2  Diurnal Thermal Tides in a Non-synchronized Ho...  \n",
       "3  Intermittent turbulence, noisy fluctuations an...  \n",
       "4  Falling Transiting Extrasolar Giant Planets ‚Äî ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the list of tuples (id, text) into a DataFrame\n",
    "dataset_df = pd.DataFrame(sampled)\n",
    "\n",
    "# View shape and head\n",
    "print(f\"‚úÖ Loaded {len(dataset_df)} rows into DataFrame.\")\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac879d",
   "metadata": {},
   "source": [
    "## 2.2 Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d2c62",
   "metadata": {},
   "source": [
    "\n",
    "This code in the next cell below imports the **`SentenceTransformer`** class from the `sentence_transformers` library and loads a pretrained model called **`\"nomic-ai/nomic-embed-text-v1.5\"`** from the Hugging Face Hub.\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
    "```\n",
    "\n",
    "üîç What it does\n",
    "- **`SentenceTransformer`** extends transformer-based models (like BERT or RoBERTa) to produce **sentence-level embeddings** ‚Äî dense numerical vectors that capture the semantic meaning of text.\n",
    "- The model **`nomic-ai/nomic-embed-text-v1.5`** is optimized for general-purpose text embeddings and works well for tasks such as:\n",
    "  - Semantic search  \n",
    "  - Clustering and topic modeling  \n",
    "  - Retrieval-Augmented Generation (RAG)  \n",
    "  - Similarity ranking and recommendation systems\n",
    "- The parameter **`trust_remote_code=True`** allows the loader to execute custom code from the model‚Äôs repository, which is required for models that define specialized architectures or preprocessing logic.\n",
    "\n",
    "In short, this code prepares a powerful embedding model that can transform text (like paper titles or abstracts) into **high-dimensional semantic vectors**, making it easier to measure meaning-based similarity across documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "939be3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeb4f07",
   "metadata": {},
   "source": [
    "This code in the next cell below iterates through each document, encodes it into a normalized embedding vector while showing live progress, and prefixes each text with \"search_document: \" so that the Nomic embedding model correctly interprets it as a retrieval document, improving alignment with query embeddings during semantic search.\n",
    "\n",
    "The prefix `search_document`: \" tells the Nomic embedding model what kind of text it‚Äôs encoding ‚Äî in this case, a document intended for retrieval.\n",
    "\n",
    "Nomic models like nomic-embed-text-v1.5 are trained with instructional prefixes (e.g., \"search_query:\", \"search_document:\", \"classification:\"), which guide the model to generate embeddings suited for different purposes.\n",
    "\n",
    "**Why Use the `\"search_document:\"` Prefix with Nomic Embeddings**\n",
    "\n",
    "Nomic embedding models (like **`nomic-embed-text-v1.5`**) are **instruction-tuned**, meaning they were trained with specific **task prefixes** that tell the model *how* to interpret the text you‚Äôre embedding.  \n",
    "\n",
    "According to the [Nomic documentation](https://docs.nomic.ai/reference/api/embed-text-v-1-embedding-text-post) and the [Hugging Face model card](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5):\n",
    "\n",
    "> ‚ÄúImportant: the text prompt must include a task instruction prefix, instructing the model which task is being performed.\n",
    "> For example, if you are implementing a RAG application, you embed your documents as  \n",
    "> `search_document: <text>` and embed your user queries as `search_query: <text>`.‚Äù\n",
    "\n",
    "**üí° Why this matters**\n",
    "- The prefix tells the model whether a text is a **document** or a **query**, ensuring both are embedded into the **same semantic space**.\n",
    "- Using `search_document:` for document embeddings and `search_query:` for query embeddings **improves retrieval accuracy**, since the model optimizes for similarity between matching query‚Äìdocument pairs.\n",
    "- Omitting or mismatching prefixes can lead to weaker alignment and lower recall in search or RAG workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be75832a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 1000/1000 texts...\n",
      "‚úÖ Finished encoding 1000 texts.\n",
      "‚úÖ Embeddings shape: (1000, 768)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Prefix for retrieval-style embeddings\n",
    "dataset_df[\"text_prefixed\"] = dataset_df[\"text\"].apply(lambda x: f\"search_document: {x}\")\n",
    "\n",
    "# Convert to list for iteration\n",
    "texts = dataset_df[\"text_prefixed\"].tolist()\n",
    "embs = []\n",
    "\n",
    "# Encode one text at a time with a single-line progress display\n",
    "total = len(texts)\n",
    "for i, text in enumerate(texts, start=1):\n",
    "    embedding = embedding_model.encode(\n",
    "        text,\n",
    "        show_progress_bar=False,   # Disable SentenceTransformer progress\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    embs.append(embedding)\n",
    "\n",
    "    # Print progress on the same line\n",
    "    sys.stdout.write(f\"\\rEncoding {i}/{total} texts...\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# Final newline to avoid overwriting the last line\n",
    "print(f\"\\n‚úÖ Finished encoding {len(embs)} texts.\")\n",
    "\n",
    "# Convert list of vectors to NumPy array\n",
    "embs = np.vstack(embs)\n",
    "print(f\"‚úÖ Embeddings shape: {embs.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2986b5f5",
   "metadata": {},
   "source": [
    "One important detail to note is the embedding dimensionality ‚Äî the number of numerical features in each vector representation.\n",
    "This dimensionality determines the structure of your vector index and must remain consistent across all embeddings to ensure efficient similarity search and accurate retrieval performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5614e",
   "metadata": {},
   "source": [
    "This code in the next cell below converts each embedding into a list of 32-bit floating-point numbers (float32) so it can be stored in an Oracle VECTOR column, determines the embedding dimension (needed for defining the vector index), and we then displays the first two rows to confirm the data and embeddings were formatted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53954b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>text</th>\n",
       "      <th>text_prefixed</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arxiv_0</td>\n",
       "      <td>0902.3253</td>\n",
       "      <td>The gravitational wave background from star-ma...</td>\n",
       "      <td>Stars on eccentric orbits around a massive bla...</td>\n",
       "      <td>[Silvia Toonen, Clovis Hopman, Marc Freitag]</td>\n",
       "      <td>The gravitational wave background from star-ma...</td>\n",
       "      <td>search_document: The gravitational wave backgr...</td>\n",
       "      <td>[0.03310791030526161, 0.06245189532637596, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arxiv_1</td>\n",
       "      <td>0902.0428</td>\n",
       "      <td>Dynamics of planets in retrograde mean motion ...</td>\n",
       "      <td>In a previous paper (Gayon &amp;amp; Bois 2008a), ...</td>\n",
       "      <td>[Julie Gayon, Eric Bois, Hans Scholl]</td>\n",
       "      <td>Dynamics of planets in retrograde mean motion ...</td>\n",
       "      <td>search_document: Dynamics of planets in retrog...</td>\n",
       "      <td>[0.029524968937039375, 0.07589565962553024, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   arxiv_id                                              title  \\\n",
       "0  arxiv_0  0902.3253  The gravitational wave background from star-ma...   \n",
       "1  arxiv_1  0902.0428  Dynamics of planets in retrograde mean motion ...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Stars on eccentric orbits around a massive bla...   \n",
       "1  In a previous paper (Gayon &amp; Bois 2008a), ...   \n",
       "\n",
       "                                        authors  \\\n",
       "0  [Silvia Toonen, Clovis Hopman, Marc Freitag]   \n",
       "1         [Julie Gayon, Eric Bois, Hans Scholl]   \n",
       "\n",
       "                                                text  \\\n",
       "0  The gravitational wave background from star-ma...   \n",
       "1  Dynamics of planets in retrograde mean motion ...   \n",
       "\n",
       "                                       text_prefixed  \\\n",
       "0  search_document: The gravitational wave backgr...   \n",
       "1  search_document: Dynamics of planets in retrog...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.03310791030526161, 0.06245189532637596, -0....  \n",
       "1  [0.029524968937039375, 0.07589565962553024, -0...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store as float32 lists (ready for Oracle VECTOR column)\n",
    "dataset_df[\"embedding\"] = [e.astype(np.float32).tolist() for e in embs]\n",
    "\n",
    "dim = len(dataset_df[\"embedding\"].iloc[0])\n",
    "\n",
    "# View the first 2 rows of the dataset\n",
    "dataset_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58889766",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526f50c",
   "metadata": {},
   "source": [
    "# Part 3: Database Table Setup and Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9710e3",
   "metadata": {},
   "source": [
    "This code below safely resets the core paper table by first dropping any dependent graph edge tables (if they exist), then recreating `research_papers` with paper metadata and a `VECTOR` embedding column of size `dim`.\n",
    "\n",
    "This rerun-safe order avoids foreign-key drop errors when graph tables already reference `research_papers`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a52cfc9",
   "metadata": {},
   "source": [
    "## 3.1 Create Reseach Papers Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "833b3c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl = f\"\"\"\n",
    "BEGIN\n",
    "    EXECUTE IMMEDIATE 'DROP TABLE paper_similarities';\n",
    "EXCEPTION WHEN OTHERS THEN\n",
    "    IF SQLCODE != -942 THEN RAISE; END IF;\n",
    "END;\n",
    "/\n",
    "BEGIN\n",
    "    EXECUTE IMMEDIATE 'DROP TABLE paper_authors';\n",
    "EXCEPTION WHEN OTHERS THEN\n",
    "    IF SQLCODE != -942 THEN RAISE; END IF;\n",
    "END;\n",
    "/\n",
    "BEGIN\n",
    "    EXECUTE IMMEDIATE 'DROP TABLE authors';\n",
    "EXCEPTION WHEN OTHERS THEN\n",
    "    IF SQLCODE != -942 THEN RAISE; END IF;\n",
    "END;\n",
    "/\n",
    "BEGIN\n",
    "    EXECUTE IMMEDIATE 'DROP TABLE research_papers CASCADE CONSTRAINTS PURGE';\n",
    "EXCEPTION WHEN OTHERS THEN\n",
    "    IF SQLCODE != -942 THEN RAISE; END IF;\n",
    "END;\n",
    "/\n",
    "CREATE TABLE research_papers (\n",
    "    arxiv_id VARCHAR2(255) PRIMARY KEY,\n",
    "    title VARCHAR2(4000),\n",
    "    abstract VARCHAR2(4000),\n",
    "    text CLOB,\n",
    "    embedding VECTOR({dim}, FLOAT32)\n",
    ")\n",
    "TABLESPACE USERS\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f83f9e",
   "metadata": {},
   "source": [
    "Here, we‚Äôre taking the multi-statement SQL stored in `ddl`, splitting it by `/` so each command runs separately, and executing them one by one using the database cursor.  \n",
    "After all statements finish, we call `conn.commit()` to save the changes ‚Äî effectively creating the `RESEARCH_PAPERS` table ‚Äî and then print a confirmation message showing the vector dimension (`dim`) used for the `embedding` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d20eecfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table RESEARCH_PAPERS created with VECTOR dimension: 768\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as cur:\n",
    "    for stmt in ddl.split(\"/\"):\n",
    "        if stmt.strip():\n",
    "            cur.execute(stmt)\n",
    "\n",
    "conn.commit()\n",
    "print(\"‚úÖ Table RESEARCH_PAPERS created with VECTOR dimension:\", dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ab118",
   "metadata": {},
   "source": [
    "## 3.2 Create Indexes (Vector and Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e7ce7",
   "metadata": {},
   "source": [
    "This code in the next cell below creates a **vector index** on the `embedding` column of the `research_papers` table to enable fast similarity search.  \n",
    "The index, named `RP_VEC_HNSW`, uses Oracle‚Äôs `VECTOR` indexing with **HNSW (Hierarchical Navigable Small World)** organization, which stores vectors in a graph for efficient approximate nearest-neighbor traversal.  \n",
    "It‚Äôs configured to use **cosine distance** as the similarity metric and a **target accuracy of 90%**, balancing search speed and precision.  \n",
    "We also set HNSW build parameters (`NEIGHBORS`, `EFCONSTRUCTION`) for graph connectivity and index quality.  \n",
    "Finally, `conn.commit()` saves the index creation, and a confirmation message is printed once the index is successfully built.\n",
    "\n",
    "Note: In Oracle‚Äôs vector indexing syntax, **ORGANIZATION INMEMORY NEIGHBOR GRAPH** enables the HNSW graph-based index structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87b97672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector Index RP_VEC_HNSW (HNSW) created\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as cur:\n",
    "    # Drop old vector indexes if they exist (supports re-running this section)\n",
    "    for idx_name in (\"RP_VEC_HNSW\", \"RP_VEC_IVF\"):\n",
    "        try:\n",
    "            cur.execute(f\"DROP INDEX {idx_name}\")\n",
    "        except oracledb.DatabaseError as e:\n",
    "            if \"ORA-01418\" not in str(e):  # ignore \"index does not exist\"\n",
    "                raise\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE VECTOR INDEX RP_VEC_HNSW\n",
    "        ON research_papers(embedding)\n",
    "        ORGANIZATION INMEMORY NEIGHBOR GRAPH\n",
    "        DISTANCE COSINE\n",
    "        WITH TARGET ACCURACY 90\n",
    "        PARAMETERS (TYPE HNSW, NEIGHBORS 40, EFCONSTRUCTION 500)\n",
    "        TABLESPACE USERS\n",
    "    \"\"\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"‚úÖ Vector Index RP_VEC_HNSW (HNSW) created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106fa5b5",
   "metadata": {},
   "source": [
    "> **üí° Knowledge Checkpoint**\n",
    ">\n",
    "> HNSW (Hierarchical Navigable Small World) is a graph-based vector index that speeds up similarity search using layered small-world navigation.\n",
    ">\n",
    "> Instead of scanning all vectors, HNSW traverses graph links from coarse to fine levels to quickly approach nearest neighbors.\n",
    "> \n",
    "> In Oracle AI Database, the clause `ORGANIZATION INMEMORY NEIGHBOR GRAPH` activates this HNSW structure. Parameters like `NEIGHBORS` and `EFCONSTRUCTION` control graph connectivity and index build quality, while `WITH TARGET ACCURACY` controls ANN search quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6231597e",
   "metadata": {},
   "source": [
    "**Creating an Oracle Text Index for Full-Text Search**\n",
    "\n",
    "This code below creates a **full-text search index** on the `text` column of the `research_papers` table using Oracle Text.\n",
    "\n",
    "1. **Drop existing index** ‚Äî removes `rp_text_idx` if it already exists, ignoring the ‚Äúindex does not exist‚Äù error.  \n",
    "2. **Create new text index** ‚Äî builds a `CTXSYS.CONTEXT` index, which tokenizes and indexes the text for efficient keyword searches.  \n",
    "   - `SYNC (ON COMMIT)` keeps the index automatically updated whenever new data is committed.  \n",
    "3. **Commit and confirm** ‚Äî saves the changes and prints a success message.\n",
    "\n",
    "Once created, you can use the `CONTAINS()` operator with `SCORE()` for fast, ranked keyword searches, e.g.:\n",
    "\n",
    "```sql\n",
    "SELECT title, SCORE(1) AS relevance\n",
    "FROM research_papers\n",
    "WHERE CONTAINS(text, 'transformer architecture', 1) > 0\n",
    "ORDER BY SCORE(1) DESC;\n",
    "```\n",
    "\n",
    "This turns your text column into a search-optimized field, similar to how search engines handle full-text retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52720503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Oracle Text index (rp_text_idx) created successfully on TEXT column.\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as cur:\n",
    "    # Drop old index if it exists\n",
    "    try:\n",
    "        cur.execute(\"DROP INDEX rp_text_idx\")\n",
    "    except oracledb.DatabaseError as e:\n",
    "        if \"ORA-01418\" not in str(e):  # ignore \"index does not exist\"\n",
    "            raise\n",
    "\n",
    "    # Create a TEXT index on the 'text' column only\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE INDEX rp_text_idx\n",
    "        ON research_papers(text)\n",
    "        INDEXTYPE IS CTXSYS.CONTEXT\n",
    "        PARAMETERS ('SYNC (ON COMMIT)')\n",
    "    \"\"\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"‚úÖ Oracle Text index (rp_text_idx) created successfully on TEXT column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2e0bf1",
   "metadata": {},
   "source": [
    "### 3.2.1 Create Relational Tables for Graph Retrieval\n",
    "\n",
    "To support Oracle SQL Property Graph retrieval, we add normalized edge tables for authorship and paper similarity. This keeps the existing `research_papers` vector/text setup intact while adding graph traversal data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec534faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Graph tables created: AUTHORS, PAPER_AUTHORS, PAPER_SIMILARITIES\n"
     ]
    }
   ],
   "source": [
    "graph_tables_ddl = \"\"\"\n",
    "BEGIN\n",
    "    EXECUTE IMMEDIATE 'DROP TABLE paper_similarities';\n",
    "EXCEPTION WHEN OTHERS THEN\n",
    "    IF SQLCODE != -942 THEN RAISE; END IF;\n",
    "END;\n",
    "/\n",
    "BEGIN\n",
    "    EXECUTE IMMEDIATE 'DROP TABLE paper_authors';\n",
    "EXCEPTION WHEN OTHERS THEN\n",
    "    IF SQLCODE != -942 THEN RAISE; END IF;\n",
    "END;\n",
    "/\n",
    "BEGIN\n",
    "    EXECUTE IMMEDIATE 'DROP TABLE authors';\n",
    "EXCEPTION WHEN OTHERS THEN\n",
    "    IF SQLCODE != -942 THEN RAISE; END IF;\n",
    "END;\n",
    "/\n",
    "CREATE TABLE authors (\n",
    "    author_name VARCHAR2(512) PRIMARY KEY\n",
    ")\n",
    "TABLESPACE USERS\n",
    "/\n",
    "CREATE TABLE paper_authors (\n",
    "    arxiv_id VARCHAR2(255) NOT NULL,\n",
    "    author_name VARCHAR2(512) NOT NULL,\n",
    "    CONSTRAINT pk_paper_authors PRIMARY KEY (arxiv_id, author_name),\n",
    "    CONSTRAINT fk_pa_paper FOREIGN KEY (arxiv_id) REFERENCES research_papers(arxiv_id),\n",
    "    CONSTRAINT fk_pa_author FOREIGN KEY (author_name) REFERENCES authors(author_name)\n",
    ")\n",
    "TABLESPACE USERS\n",
    "/\n",
    "CREATE TABLE paper_similarities (\n",
    "    source_arxiv_id VARCHAR2(255) NOT NULL,\n",
    "    target_arxiv_id VARCHAR2(255) NOT NULL,\n",
    "    sim_score NUMBER(8,6) NOT NULL,\n",
    "    rank_no NUMBER(5) NOT NULL,\n",
    "    CONSTRAINT pk_paper_similarities PRIMARY KEY (source_arxiv_id, target_arxiv_id),\n",
    "    CONSTRAINT fk_ps_src FOREIGN KEY (source_arxiv_id) REFERENCES research_papers(arxiv_id),\n",
    "    CONSTRAINT fk_ps_tgt FOREIGN KEY (target_arxiv_id) REFERENCES research_papers(arxiv_id),\n",
    "    CONSTRAINT ck_ps_not_self CHECK (source_arxiv_id <> target_arxiv_id)\n",
    ")\n",
    "TABLESPACE USERS\n",
    "\"\"\"\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    for stmt in graph_tables_ddl.split(\"/\"):\n",
    "        if stmt.strip():\n",
    "            cur.execute(stmt)\n",
    "\n",
    "    cur.execute(\"CREATE INDEX idx_pa_author ON paper_authors(author_name)\")\n",
    "    cur.execute(\"CREATE INDEX idx_ps_source ON paper_similarities(source_arxiv_id)\")\n",
    "    cur.execute(\"CREATE INDEX idx_ps_target ON paper_similarities(target_arxiv_id)\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"‚úÖ Graph tables created: AUTHORS, PAPER_AUTHORS, PAPER_SIMILARITIES\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d74db",
   "metadata": {},
   "source": [
    "In the code below we‚Äôre taking all the research paper records (including their embeddings) from the DataFrame and inserting them into the `RESEARCH_PAPERS` table in Oracle.\n",
    "\n",
    "Let‚Äôs break down what‚Äôs happening:\n",
    "\n",
    "\n",
    "1. Convert each embedding for Oracle compatibility\n",
    "```python\n",
    "embedding_array = array.array('f', row.get(\"embedding\"))\n",
    "```\n",
    "Oracle‚Äôs `VECTOR` column expects the data as a compact binary float array, not a Python list.  \n",
    "So here we convert each embedding into an `array.array('f')` ‚Äî this ensures the data binds correctly and efficiently when inserting.\n",
    "\n",
    "\n",
    "2. Prepare all rows for insertion\n",
    "For every paper, we build a tuple containing its metadata (`arxiv_id`, `title`, `abstract`, `text`) and the formatted `embedding_array`.  \n",
    "These tuples are collected in a list called `rows`.\n",
    "\n",
    "3. Insert each row with a progress bar\n",
    "```python\n",
    "for row in tqdm(rows):\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO research_papers (arxiv_id, title, abstract, text, embedding)\n",
    "        VALUES (:1, :2, :3, :4, :5)\n",
    "    \"\"\", row)\n",
    "```\n",
    "We loop through each row, inserting it into the table using Oracle‚Äôs parameterized query syntax.  \n",
    "The `tqdm` progress bar gives real-time feedback on the insertion process ‚Äî helpful when inserting hundreds or thousands of embeddings.\n",
    "\n",
    "\n",
    "4. Commit everything\n",
    "Finally, `conn.commit()` saves all the inserted records permanently in the database.  You‚Äôll see a confirmation message once the operation completes successfully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08324270",
   "metadata": {},
   "source": [
    "## 3.3 Ingest Data into Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be6bdde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to insert 1000 rows into RESEARCH_PAPERS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 1046.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data inserted successfully\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import array\n",
    "\n",
    "\n",
    "rows = []\n",
    "for i, row in dataset_df.iterrows():\n",
    "    # Convert embedding list to array.array for proper VECTOR binding\n",
    "    embedding_array = array.array('f', row.get(\"embedding\"))\n",
    "    \n",
    "    rows.append((\n",
    "        row.get(\"arxiv_id\"),\n",
    "        row.get(\"title\"),\n",
    "        row.get(\"abstract\"),\n",
    "        row.get(\"text\"),\n",
    "        embedding_array\n",
    "    ))\n",
    "\n",
    "print(f\"Preparing to insert {len(rows)} rows into RESEARCH_PAPERS...\")\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    for row in tqdm(rows):\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO research_papers (arxiv_id, title, abstract, text, embedding)\n",
    "            VALUES (:1, :2, :3, :4, :5)\n",
    "            \"\"\", \n",
    "            row\n",
    "        )\n",
    "        \n",
    "conn.commit()\n",
    "print(\"‚úÖ Data inserted successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7a14d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 1000\n",
      "('0902.3253', 'The gravitational wave background from star-massive black hole fly-bys', 'Stars on eccentric orbits around a massive black hole (MBH) emit bursts of gravitational waves (GWs) at periapse. Such events may be directly resolvable in the Galactic centre. However, if the star does not spiral in, the emitted GWs are not resolvable for extra-galactic MBHs, but constitute a source of background noise. We estimate the power spectrum of this extreme mass ratio burst background (EMBB) and compare it to the anticipated instrumental noise of the Laser Interferometer Space Antenna (LISA). To this end, we model the regions close to a MBH, accounting for mass-segregation, and for processes that limit the presence of stars close to the MBH, such as GW inspiral and hydrodynamical collisions between stars. We find that the EMBB is dominated by GW bursts from stellar mass black holes, and the magnitude of the noise spectrum (f S_GW)^{1/2} is at least a factor ~10 smaller than the instrumental noise. As an additional result of our analysis, we show that LISA is unlikely to detect relativistic bursts in the Galactic centre.', <oracledb.lob.LOB object at 0x386c40350>)\n",
      "('0901.1570', 'Intermittent turbulence, noisy fluctuations and wavy structures in the Venusian magnetosheath and wake', 'Recent research has shown that distinct physical regions in the Venusian induced magnetosphere are recognizable from the variations of strength of the magnetic field and its wave/fluctuation activity. In this paper the statistical properties of magnetic fluctuations are investigated in the Venusian magnetosheath and wake regions. The main goal is to identify the characteristic scaling features of fluctuations along Venus Express (VEX) trajectory and to understand the specific circumstances of the occurrence of different types of scalings. For the latter task we also use the results of measurements from the previous missions to Venus. Our main result is that the changing character of physical interactions between the solar wind and the planetary obstacle is leading to different types of spectral scaling in the near-Venusian space. Noisy fluctuations are observed in the magnetosheath, wavy structures near the terminator and in the nightside near-planet wake. Multi-scale turbulence is observed at the magnetosheath boundary layer and near the quasi-parallel bow shock. Magnetosheath boundary layer turbulence is associated with an average magnetic field which is nearly aligned with the Sun-Venus line. Noisy magnetic fluctuations are well described with the Gaussian statistics. Both magnetosheath boundary layer and near shock turbulence statistics exhibit non-Gaussian features and intermittency over small spatio-temporal scales. The occurrence of turbulence near magnetosheath boundaries can be responsible for the local heating of plasma observed by previous missions.', <oracledb.lob.LOB object at 0x386bf99d0>)\n",
      "('0902.0428', 'Dynamics of planets in retrograde mean motion resonance', 'In a previous paper (Gayon &amp; Bois 2008a), we have shown the general efficiency of retrograde resonances for stabilizing compact planetary systems. Such retrograde resonances can be found when two-planets of a three-body planetary system are both in mean motion resonance and revolve in opposite directions. For a particular two-planet system, we have also obtained a new orbital fit involving such a counter-revolving configuration and consistent with the observational data. <br>In the present paper, we analytically investigate the three-body problem in this particular case of retrograde resonances. We therefore define a new set of canonical variables allowing to express correctly the resonance angles and obtain the Hamiltonian of a system harboring planets revolving in opposite directions. The acquiring of an analytical &#34;rail&#34; may notably contribute to a deeper understanding of our numerical investigation and provides the major structures related to the stability properties. A comparison between our analytical and numerical results is also carried out.', <oracledb.lob.LOB object at 0x380720350>)\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT COUNT(*) FROM RESEARCH_PAPERS\")\n",
    "    print(\"Row count:\", cur.fetchone()[0])\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT arxiv_id, title, abstract, text FROM RESEARCH_PAPERS\n",
    "        FETCH FIRST 3 ROWS ONLY\n",
    "    \"\"\")\n",
    "    for row in cur.fetchall():\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83ce7ad",
   "metadata": {},
   "source": [
    "## 3.4 Build and Register Graph Relationships\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de4e73b",
   "metadata": {},
   "source": [
    "### 3.4.1 Load Author Edges (`Author -> WROTE -> Paper`)\n",
    "\n",
    "Here we normalize the dataset `authors` field and populate `AUTHORS` + `PAPER_AUTHORS` so graph traversal can connect papers through shared authors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d79b490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 3410 authors and 6083 Author->WROTE->Paper edges.\n"
     ]
    }
   ],
   "source": [
    "def normalize_authors(raw_authors):\n",
    "    if raw_authors is None:\n",
    "        return []\n",
    "    if isinstance(raw_authors, str):\n",
    "        return [a.strip() for a in raw_authors.split(\",\") if a.strip()]\n",
    "    if isinstance(raw_authors, list):\n",
    "        cleaned = []\n",
    "        for author in raw_authors:\n",
    "            text = str(author).strip()\n",
    "            if text:\n",
    "                cleaned.append(text)\n",
    "        return cleaned\n",
    "    return []\n",
    "\n",
    "author_rows = set()\n",
    "paper_author_rows = set()\n",
    "\n",
    "for _, row in dataset_df.iterrows():\n",
    "    arxiv_id = row.get(\"arxiv_id\")\n",
    "    if not arxiv_id:\n",
    "        continue\n",
    "\n",
    "    for author_name in normalize_authors(row.get(\"authors\")):\n",
    "        author_rows.add((author_name,))\n",
    "        paper_author_rows.add((arxiv_id, author_name))\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"TRUNCATE TABLE paper_authors\")\n",
    "    cur.execute(\"TRUNCATE TABLE authors\")\n",
    "\n",
    "    if author_rows:\n",
    "        cur.executemany(\n",
    "            \"INSERT INTO authors (author_name) VALUES (:1)\",\n",
    "            sorted(author_rows)\n",
    "        )\n",
    "\n",
    "    if paper_author_rows:\n",
    "        cur.executemany(\n",
    "            \"INSERT INTO paper_authors (arxiv_id, author_name) VALUES (:1, :2)\",\n",
    "            sorted(paper_author_rows)\n",
    "        )\n",
    "\n",
    "conn.commit()\n",
    "print(f\"‚úÖ Loaded {len(author_rows)} authors and {len(paper_author_rows)} Author->WROTE->Paper edges.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235952fd",
   "metadata": {},
   "source": [
    "### 3.4.2 Load Similarity Edges (`Paper -> SIMILAR_TO -> Paper`)\n",
    "\n",
    "Using the normalized embeddings already computed for vector search, we add top-k nearest-neighbor paper links and persist them as directed similarity edges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8da9ea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 10000 Paper->SIMILAR_TO->Paper edges (top 10 per paper).\n"
     ]
    }
   ],
   "source": [
    "TOP_SIM_NEIGHBORS = 10\n",
    "\n",
    "paper_ids = dataset_df[\"arxiv_id\"].tolist()\n",
    "emb_matrix = np.vstack(dataset_df[\"embedding\"].values).astype(np.float32)\n",
    "\n",
    "neighbor_k = min(TOP_SIM_NEIGHBORS, max(len(paper_ids) - 1, 0))\n",
    "similarity_rows = []\n",
    "\n",
    "if neighbor_k > 0:\n",
    "    similarity_matrix = emb_matrix @ emb_matrix.T\n",
    "    np.fill_diagonal(similarity_matrix, -np.inf)\n",
    "\n",
    "    for i, source_arxiv_id in enumerate(paper_ids):\n",
    "        nn_idx = np.argpartition(similarity_matrix[i], -neighbor_k)[-neighbor_k:]\n",
    "        nn_idx = nn_idx[np.argsort(similarity_matrix[i][nn_idx])[::-1]]\n",
    "\n",
    "        for rank_no, j in enumerate(nn_idx, start=1):\n",
    "            similarity_rows.append((\n",
    "                source_arxiv_id,\n",
    "                paper_ids[j],\n",
    "                float(similarity_matrix[i][j]),\n",
    "                rank_no\n",
    "            ))\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"TRUNCATE TABLE paper_similarities\")\n",
    "    if similarity_rows:\n",
    "        cur.executemany(\n",
    "            \"\"\"\n",
    "            INSERT INTO paper_similarities (source_arxiv_id, target_arxiv_id, sim_score, rank_no)\n",
    "            VALUES (:1, :2, :3, :4)\n",
    "            \"\"\",\n",
    "            similarity_rows\n",
    "        )\n",
    "\n",
    "conn.commit()\n",
    "print(f\"‚úÖ Loaded {len(similarity_rows)} Paper->SIMILAR_TO->Paper edges (top {neighbor_k} per paper).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fdf463",
   "metadata": {},
   "source": [
    "### 3.4.3 Register the Oracle SQL Property Graph\n",
    "\n",
    "This cell creates a SQL Property Graph (`RESEARCH_GRAPH`) over `RESEARCH_PAPERS`, `AUTHORS`, `PAPER_AUTHORS`, and `PAPER_SIMILARITIES`, so we can use `GRAPH_TABLE` pattern matching for graph-native retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ca2ff21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SQL Property Graph RESEARCH_GRAPH created.\n"
     ]
    }
   ],
   "source": [
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM user_property_graphs\n",
    "        WHERE graph_name = 'RESEARCH_GRAPH'\n",
    "    \"\"\")\n",
    "    graph_exists = cur.fetchone()[0] > 0\n",
    "\n",
    "    if graph_exists:\n",
    "        cur.execute(\"DROP PROPERTY GRAPH research_graph\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE PROPERTY GRAPH research_graph\n",
    "        VERTEX TABLES (\n",
    "            research_papers\n",
    "                KEY (arxiv_id)\n",
    "                LABEL paper\n",
    "                PROPERTIES (arxiv_id, title, abstract),\n",
    "            authors\n",
    "                KEY (author_name)\n",
    "                LABEL author\n",
    "                PROPERTIES (author_name)\n",
    "        )\n",
    "        EDGE TABLES (\n",
    "            paper_authors\n",
    "                KEY (arxiv_id, author_name)\n",
    "                SOURCE KEY (author_name) REFERENCES authors (author_name)\n",
    "                DESTINATION KEY (arxiv_id) REFERENCES research_papers (arxiv_id)\n",
    "                LABEL wrote,\n",
    "            paper_similarities\n",
    "                KEY (source_arxiv_id, target_arxiv_id)\n",
    "                SOURCE KEY (source_arxiv_id) REFERENCES research_papers (arxiv_id)\n",
    "                DESTINATION KEY (target_arxiv_id) REFERENCES research_papers (arxiv_id)\n",
    "                LABEL similar_to\n",
    "                PROPERTIES (sim_score, rank_no)\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"‚úÖ SQL Property Graph RESEARCH_GRAPH created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9788fc",
   "metadata": {},
   "source": [
    "# Part 4. Retrieval Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab38912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_TEXT_KEYWORDS = \"optimization\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa03c53c",
   "metadata": {},
   "source": [
    "## 4.1 Text Based Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d31cd",
   "metadata": {},
   "source": [
    "The code below performs a full-text search over the text column of the research_papers table, using the Oracle Text index we just created earlier\n",
    "\n",
    "- CONTAINS(text, :keyword, 1) uses the Oracle Text index on the text column to find documents containing the given keyword or phrase.\n",
    "- SCORE(1) assigns a relevance score based on how well each document matches the search term.\n",
    "- SUBSTR(text, 1, 200) returns the first 200 characters as a short preview snippet.\n",
    "- FETCH FIRST 10 ROWS ONLY limits the results to the top 10 most relevant matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7fbb1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_search_research_papers(conn, keyword: str):\n",
    "    \"\"\"\n",
    "    Perform a full-text keyword search on the 'text' column \n",
    "    using the Oracle Text index (rp_text_idx).\n",
    "\n",
    "    Args:\n",
    "        conn: Oracle database connection object.\n",
    "        keyword (str): Keyword or phrase to search for.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (rows, columns)\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            arxiv_id, \n",
    "            title, \n",
    "            SUBSTR(text, 1, 200) AS text_snippet,\n",
    "            SCORE(1) AS relevance_score\n",
    "        FROM research_papers\n",
    "        WHERE CONTAINS(text, :keyword, 1) > 0\n",
    "        ORDER BY SCORE(1) DESC\n",
    "        FETCH FIRST 10 ROWS ONLY\n",
    "    \"\"\"\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query, keyword=keyword)\n",
    "        rows = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "\n",
    "    return rows, columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbbe04fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Keyword Search: 'optimization'\n",
      "üìä Results: 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARXIV_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>TEXT_SNIPPET</th>\n",
       "      <th>RELEVANCE_SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0912.1394</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0901.0846</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0904.4106</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ARXIV_ID                                              TITLE  \\\n",
       "0  0912.1394  Are Near Earth Objects the Key to Optimization...   \n",
       "1  0901.0846  A Metric and Optimisation Scheme for Microlens...   \n",
       "2  0904.4106  Is the HR 8799 extrasolar system destined for ...   \n",
       "\n",
       "                                        TEXT_SNIPPET  RELEVANCE_SCORE  \n",
       "0  Are Near Earth Objects the Key to Optimization...               42  \n",
       "1  A Metric and Optimisation Scheme for Microlens...               11  \n",
       "2  Is the HR 8799 extrasolar system destined for ...               11  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows, columns = keyword_search_research_papers(conn, SEARCH_TEXT_KEYWORDS)\n",
    "\n",
    "results_df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "print(f\"üîç Keyword Search: '{SEARCH_TEXT_KEYWORDS}'\")\n",
    "print(f\"üìä Results: {len(results_df)}\\n\")\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c352afa",
   "metadata": {},
   "source": [
    "As shown above, the returned rows represent documents whose indexed `text` content matched the full-text search query **\"Optimization\"**, as determined by the Oracle Text `CONTAINS()` operator using the `rp_text_idx` index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ff4f6b",
   "metadata": {},
   "source": [
    "## 4.2 Vector Based Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d0cc6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_QUERY = \"Get me papers related to planetary exploration\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023c96d4",
   "metadata": {},
   "source": [
    "**Vector Similarity Search in Oracle**\n",
    "\n",
    "This function below `vector_search_research_papers` performs a **semantic vector search** on the `research_papers` table, retrieving papers most similar to a given query using **Oracle‚Äôs native VECTOR search** feature.\n",
    "\n",
    "\n",
    "Step-by-step overview\n",
    "\n",
    "1. Encode the search query\n",
    "```python\n",
    "query_embedding = embedding_model.encode(\n",
    "    [f\"search_query: {search_query}\"],\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")[0].astype(np.float32).tolist()\n",
    "```\n",
    "- The query is embedded using the same model that generated the document embeddings.  \n",
    "- The prefix `\"search_query:\"` ensures embeddings align with `\"search_document:\"` vectors.  \n",
    "- Normalized and cast to `float32` for Oracle‚Äôs `VECTOR` type.\n",
    "\n",
    "\n",
    "2. Prepare for database binding\n",
    "```python\n",
    "query_embedding_array = array.array('f', query_embedding)\n",
    "```\n",
    "- Converts the embedding list into a binary float array (`array('f')`), required for Oracle‚Äôs vector binding.\n",
    "\n",
    "\n",
    "3. Perform vector search\n",
    "```sql\n",
    "SELECT arxiv_id, title, abstract,\n",
    "       SUBSTR(text, 1, 200) AS text_snippet,\n",
    "        ROUND(1 - VECTOR_DISTANCE(embedding, :q, COSINE), 4) AS similarity_score\n",
    "FROM research_papers\n",
    "ORDER BY similarity_score\n",
    "FETCH APPROX FIRST {top_k} ROWS ONLY WITH TARGET ACCURACY 90\n",
    "```\n",
    "- Computes **cosine distance** between the query vector and stored embeddings.  \n",
    "- Orders results by similarity, returning the closest matches.  \n",
    "- Uses **Approximate Nearest Neighbor (ANN)** search for fast retrieval at 90% target accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b46a3f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search_research_papers(conn, embedding_model, search_query: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Perform a vector similarity search on the research_papers table using a query embedding.\n",
    "    Returns cosine similarity scores (higher = more similar).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1Ô∏è‚É£ Encode the query into a vector\n",
    "    query_embedding = embedding_model.encode(\n",
    "        [f\"search_query: {search_query}\"], \n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )[0].astype(np.float32).tolist()\n",
    "\n",
    "    # 2Ô∏è‚É£ Prepare the vector for Oracle binding\n",
    "    query_embedding_array = array.array('f', query_embedding)\n",
    "\n",
    "    # 3Ô∏è‚É£ Run a vector similarity search using cosine similarity\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            arxiv_id, \n",
    "            title, \n",
    "            abstract, \n",
    "            SUBSTR(text, 1, 200) AS text_snippet,\n",
    "            ROUND(1 - VECTOR_DISTANCE(embedding, :q, COSINE), 4) AS similarity_score\n",
    "        FROM research_papers\n",
    "        ORDER BY similarity_score DESC\n",
    "        FETCH APPROX FIRST {top_k} ROWS ONLY WITH TARGET ACCURACY 90\n",
    "    \"\"\"\n",
    "\n",
    "    # 4Ô∏è‚É£ Execute and return results\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query, q=query_embedding_array)\n",
    "        rows = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "\n",
    "    return rows, columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dca16bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Vector Search: 'optimization'\n",
      "üìä Results: 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARXIV_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>TEXT_SNIPPET</th>\n",
       "      <th>SIMILARITY_SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0912.1394</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>This note suggests that near earth objects and...</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>0.7392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0901.0846</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>OGLE III and MOA II are discovering 600-1000 G...</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>0.6409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0911.2703</td>\n",
       "      <td>An Efficient Method for Modeling High Magnific...</td>\n",
       "      <td>I present a previously unpublished method for ...</td>\n",
       "      <td>An Efficient Method for Modeling High Magnific...</td>\n",
       "      <td>0.6136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001.0437</td>\n",
       "      <td>Kepler Science Operations</td>\n",
       "      <td>Kepler&amp;#39;s primary mission is a search for e...</td>\n",
       "      <td>Kepler Science Operations ‚Äî Kepler&amp;#39;s prima...</td>\n",
       "      <td>0.6019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0903.5139</td>\n",
       "      <td>Science-Operational Metrics and Issues for the...</td>\n",
       "      <td>A movement is underway to test the uniqueness ...</td>\n",
       "      <td>Science-Operational Metrics and Issues for the...</td>\n",
       "      <td>0.6009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ARXIV_ID                                              TITLE  \\\n",
       "0  0912.1394  Are Near Earth Objects the Key to Optimization...   \n",
       "1  0901.0846  A Metric and Optimisation Scheme for Microlens...   \n",
       "2  0911.2703  An Efficient Method for Modeling High Magnific...   \n",
       "3  1001.0437                          Kepler Science Operations   \n",
       "4  0903.5139  Science-Operational Metrics and Issues for the...   \n",
       "\n",
       "                                            ABSTRACT  \\\n",
       "0  This note suggests that near earth objects and...   \n",
       "1  OGLE III and MOA II are discovering 600-1000 G...   \n",
       "2  I present a previously unpublished method for ...   \n",
       "3  Kepler&#39;s primary mission is a search for e...   \n",
       "4  A movement is underway to test the uniqueness ...   \n",
       "\n",
       "                                        TEXT_SNIPPET  SIMILARITY_SCORE  \n",
       "0  Are Near Earth Objects the Key to Optimization...            0.7392  \n",
       "1  A Metric and Optimisation Scheme for Microlens...            0.6409  \n",
       "2  An Efficient Method for Modeling High Magnific...            0.6136  \n",
       "3  Kepler Science Operations ‚Äî Kepler&#39;s prima...            0.6019  \n",
       "4  Science-Operational Metrics and Issues for the...            0.6009  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows, columns = vector_search_research_papers(conn, embedding_model, SEARCH_TEXT_KEYWORDS, top_k=5)\n",
    "\n",
    "results_df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "print(f\"üîç Vector Search: '{SEARCH_TEXT_KEYWORDS}'\")\n",
    "print(f\"üìä Results: {len(results_df)}\\n\")\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c8bf3e",
   "metadata": {},
   "source": [
    "In this step, we used Oracle Database‚Äôs native vector search capabilities to perform a semantic similarity search over the research_papers dataset.\n",
    "\n",
    "The text query ‚Äî ‚ÄúGet me papers related to planetary exploration‚Äù ‚Äî was first transformed into a high-dimensional embedding vector using the same model that generated our document embeddings (nomic-embed-text-v1.5).\n",
    "This query vector captures the semantic meaning of the phrase rather than just the exact words.\n",
    "\n",
    "Using Oracle‚Äôs VECTOR_DISTANCE(..., COSINE) function (converted to 1 - distance), we then compared this query vector against the stored embeddings for each paper in the database.\n",
    "The result is a ranked list of research papers ordered by cosine similarity, where higher scores indicate stronger semantic alignment with the search query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93fd0c7",
   "metadata": {},
   "source": [
    "## 4.3 Hybrid Retrieval (Vector + Text Combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1972f31",
   "metadata": {},
   "source": [
    "#### 4.3.1 Pre Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e07236a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "import numpy as np\n",
    "\n",
    "def hybrid_search_research_papers_pre_filter(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    search_phrase: str,\n",
    "    top_k: int = 10,\n",
    "    show_explain: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform a hybrid search using Oracle Text + Vector Search.\n",
    "    Combines lexical filtering (CONTAINS) with semantic re-ranking via cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        conn: Oracle database connection object.\n",
    "        embedding_model: Model with `.encode()` method (e.g., SentenceTransformer).\n",
    "        search_phrase (str): User search phrase used for both text filtering and embedding.\n",
    "        top_k (int): Number of results to return (default = 10).\n",
    "        show_explain (bool): If True, prints the execution plan.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (rows, columns, exec_plan_text or None)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 1: Encode search phrase into normalized vector ---\n",
    "    query_embedding = embedding_model.encode(\n",
    "        [f\"search_query: {search_phrase}\"],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )[0].astype(np.float32).tolist()\n",
    "    query_embedding_array = array.array('f', query_embedding)\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        # Enable runtime stats if needed\n",
    "        if show_explain:\n",
    "            cur.execute(\"ALTER SESSION SET statistics_level = ALL\")\n",
    "\n",
    "        # --- Step 2: Hybrid query (Oracle Text + Vector) ---\n",
    "        sql = f\"\"\"\n",
    "            SELECT {\"/*+ GATHER_PLAN_STATISTICS */\" if show_explain else \"\"}\n",
    "                arxiv_id,\n",
    "                title,\n",
    "                abstract,\n",
    "                SUBSTR(text, 1, 200) AS text_snippet,\n",
    "                ROUND(1 - VECTOR_DISTANCE(embedding, :q, COSINE), 4) AS similarity_score\n",
    "            FROM research_papers\n",
    "            WHERE CONTAINS(text, :kw, 1) > 0\n",
    "            ORDER BY similarity_score DESC\n",
    "            FETCH APPROX FIRST {top_k} ROWS ONLY WITH TARGET ACCURACY 90\n",
    "        \"\"\"\n",
    "\n",
    "        cur.execute(sql, q=query_embedding_array, kw=search_phrase)\n",
    "        rows = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "\n",
    "    # --- Step 3: Execution plan (optional) ---\n",
    "    exec_plan_text = None\n",
    "    if show_explain:\n",
    "        with conn.cursor() as cur_plan:\n",
    "            cur_plan.execute(\"\"\"\n",
    "                SELECT plan_table_output\n",
    "                FROM TABLE(DBMS_XPLAN.DISPLAY_CURSOR(NULL, NULL, 'ALLSTATS LAST +PREDICATE'))\n",
    "            \"\"\")\n",
    "            exec_plan_text = \"\\n\".join(r[0] for r in cur_plan.fetchall())\n",
    "\n",
    "        print(\"\\n====== Execution Plan (DBMS_XPLAN.DISPLAY_CURSOR) ======\")\n",
    "        print(exec_plan_text)\n",
    "        print(\"========================================================\\n\")\n",
    "\n",
    "    return rows, columns, exec_plan_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91ba38db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Hybrid Search: 'optimization'\n",
      "üìä Found 3 results\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARXIV_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>TEXT_SNIPPET</th>\n",
       "      <th>SIMILARITY_SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0912.1394</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>This note suggests that near earth objects and...</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>0.7392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0901.0846</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>OGLE III and MOA II are discovering 600-1000 G...</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>0.6409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0904.4106</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>The recent discovery of a three-planet extraso...</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>0.5576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ARXIV_ID                                              TITLE  \\\n",
       "0  0912.1394  Are Near Earth Objects the Key to Optimization...   \n",
       "1  0901.0846  A Metric and Optimisation Scheme for Microlens...   \n",
       "2  0904.4106  Is the HR 8799 extrasolar system destined for ...   \n",
       "\n",
       "                                            ABSTRACT  \\\n",
       "0  This note suggests that near earth objects and...   \n",
       "1  OGLE III and MOA II are discovering 600-1000 G...   \n",
       "2  The recent discovery of a three-planet extraso...   \n",
       "\n",
       "                                        TEXT_SNIPPET  SIMILARITY_SCORE  \n",
       "0  Are Near Earth Objects the Key to Optimization...            0.7392  \n",
       "1  A Metric and Optimisation Scheme for Microlens...            0.6409  \n",
       "2  Is the HR 8799 extrasolar system destined for ...            0.5576  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows, columns, exec_plan = hybrid_search_research_papers_pre_filter(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    search_phrase=SEARCH_TEXT_KEYWORDS,\n",
    "    top_k=10,\n",
    "    show_explain=False\n",
    ")\n",
    "\n",
    "pre_filter_results_df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "print(f\"üîç Hybrid Search: '{SEARCH_TEXT_KEYWORDS}'\")\n",
    "print(f\"üìä Found {len(pre_filter_results_df)} results\\n\")\n",
    "\n",
    "pre_filter_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e5fda2",
   "metadata": {},
   "source": [
    "#### 4.3.2 Post Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae572091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "import numpy as np\n",
    "\n",
    "def hybrid_search_research_papers_postfilter(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    search_phrase: str,\n",
    "    top_k: int = 10,\n",
    "    candidate_k: int = 200,\n",
    "    show_explain: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform a hybrid search using Vector Search first, then Oracle Text filtering.\n",
    "    Returns top results ranked by semantic similarity but filtered by lexical match.\n",
    "\n",
    "    Args:\n",
    "        conn: Oracle database connection object.\n",
    "        embedding_model: Model with `.encode()` method (e.g., SentenceTransformer).\n",
    "        search_phrase (str): Search phrase used for both embedding and text filtering.\n",
    "        top_k (int): Number of top results to return (default = 10).\n",
    "        candidate_k (int): Number of initial vector candidates (default = 200).\n",
    "        show_explain (bool): If True, prints the execution plan.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (rows, columns, exec_plan_text or None)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 1: Encode search phrase into a normalized query vector ---\n",
    "    query_embedding = embedding_model.encode(\n",
    "        [f\"search_query: {search_phrase}\"],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )[0].astype(np.float32).tolist()\n",
    "    query_embedding_array = array.array('f', query_embedding)\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        # Enable runtime statistics if requested\n",
    "        if show_explain:\n",
    "            cur.execute(\"ALTER SESSION SET statistics_level = ALL\")\n",
    "\n",
    "        # --- Step 2: Hybrid query (Vector first ‚Üí Text filter) ---\n",
    "        sql = f\"\"\"\n",
    "            WITH vec_candidates AS (\n",
    "                SELECT\n",
    "                    arxiv_id,\n",
    "                    title,\n",
    "                    abstract,\n",
    "                    text,\n",
    "                    1 - VECTOR_DISTANCE(embedding, :q, COSINE) AS similarity_score\n",
    "                FROM research_papers\n",
    "                ORDER BY similarity_score DESC\n",
    "                FETCH APPROX FIRST {candidate_k} ROWS ONLY WITH TARGET ACCURACY 90\n",
    "            )\n",
    "            SELECT {\"/*+ GATHER_PLAN_STATISTICS */\" if show_explain else \"\"}\n",
    "                arxiv_id,\n",
    "                title,\n",
    "                SUBSTR(text, 1, 200) AS text_snippet,\n",
    "                ROUND(similarity_score, 4) AS similarity_score\n",
    "            FROM vec_candidates\n",
    "            WHERE CONTAINS(text, :kw, 1) > 0\n",
    "            ORDER BY similarity_score DESC\n",
    "            FETCH FIRST {top_k} ROWS ONLY\n",
    "        \"\"\"\n",
    "\n",
    "        cur.execute(sql, q=query_embedding_array, kw=search_phrase)\n",
    "        rows = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "\n",
    "    # --- Step 3: Fetch and display execution plan (optional) ---\n",
    "    exec_plan_text = None\n",
    "    if show_explain:\n",
    "        with conn.cursor() as cur_plan:\n",
    "            cur_plan.execute(\"\"\"\n",
    "                SELECT plan_table_output\n",
    "                FROM TABLE(DBMS_XPLAN.DISPLAY_CURSOR(NULL, NULL, 'ALLSTATS LAST +PREDICATE'))\n",
    "            \"\"\")\n",
    "            exec_plan_text = \"\\n\".join(r[0] for r in cur_plan.fetchall())\n",
    "\n",
    "        print(\"\\n====== Execution Plan (DBMS_XPLAN.DISPLAY_CURSOR) ======\")\n",
    "        print(exec_plan_text)\n",
    "        print(\"========================================================\\n\")\n",
    "\n",
    "    return rows, columns, exec_plan_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4048cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Hybrid Search: 'optimization'\n",
      "üìä Found 3 results\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARXIV_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>TEXT_SNIPPET</th>\n",
       "      <th>SIMILARITY_SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0912.1394</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>0.7392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0901.0846</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>0.6409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0904.4106</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>0.5576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ARXIV_ID                                              TITLE  \\\n",
       "0  0912.1394  Are Near Earth Objects the Key to Optimization...   \n",
       "1  0901.0846  A Metric and Optimisation Scheme for Microlens...   \n",
       "2  0904.4106  Is the HR 8799 extrasolar system destined for ...   \n",
       "\n",
       "                                        TEXT_SNIPPET  SIMILARITY_SCORE  \n",
       "0  Are Near Earth Objects the Key to Optimization...            0.7392  \n",
       "1  A Metric and Optimisation Scheme for Microlens...            0.6409  \n",
       "2  Is the HR 8799 extrasolar system destined for ...            0.5576  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows, columns, exec_plan = hybrid_search_research_papers_postfilter(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    search_phrase=SEARCH_TEXT_KEYWORDS,\n",
    "    top_k=10,\n",
    "    show_explain=False\n",
    ")\n",
    "\n",
    "post_filter_results_df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "print(f\"üîç Hybrid Search: '{SEARCH_TEXT_KEYWORDS}'\")\n",
    "print(f\"üìä Found {len(post_filter_results_df)} results\\n\")\n",
    "\n",
    "post_filter_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba61b5",
   "metadata": {},
   "source": [
    "Observe pre/post filtering technques in a table side by side "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5e521f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARXIV_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>TEXT_SNIPPET</th>\n",
       "      <th>SIMILARITY_SCORE</th>\n",
       "      <th>ARXIV_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>TEXT_SNIPPET</th>\n",
       "      <th>SIMILARITY_SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0912.1394</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>This note suggests that near earth objects and...</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>0.7392</td>\n",
       "      <td>0912.1394</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>0.7392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0901.0846</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>OGLE III and MOA II are discovering 600-1000 G...</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>0901.0846</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>0.6409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0904.4106</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>The recent discovery of a three-planet extraso...</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>0.5576</td>\n",
       "      <td>0904.4106</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>0.5576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ARXIV_ID                                              TITLE  \\\n",
       "0  0912.1394  Are Near Earth Objects the Key to Optimization...   \n",
       "1  0901.0846  A Metric and Optimisation Scheme for Microlens...   \n",
       "2  0904.4106  Is the HR 8799 extrasolar system destined for ...   \n",
       "\n",
       "                                            ABSTRACT  \\\n",
       "0  This note suggests that near earth objects and...   \n",
       "1  OGLE III and MOA II are discovering 600-1000 G...   \n",
       "2  The recent discovery of a three-planet extraso...   \n",
       "\n",
       "                                        TEXT_SNIPPET  SIMILARITY_SCORE  \\\n",
       "0  Are Near Earth Objects the Key to Optimization...            0.7392   \n",
       "1  A Metric and Optimisation Scheme for Microlens...            0.6409   \n",
       "2  Is the HR 8799 extrasolar system destined for ...            0.5576   \n",
       "\n",
       "    ARXIV_ID                                              TITLE  \\\n",
       "0  0912.1394  Are Near Earth Objects the Key to Optimization...   \n",
       "1  0901.0846  A Metric and Optimisation Scheme for Microlens...   \n",
       "2  0904.4106  Is the HR 8799 extrasolar system destined for ...   \n",
       "\n",
       "                                        TEXT_SNIPPET  SIMILARITY_SCORE  \n",
       "0  Are Near Earth Objects the Key to Optimization...            0.7392  \n",
       "1  A Metric and Optimisation Scheme for Microlens...            0.6409  \n",
       "2  Is the HR 8799 extrasolar system destined for ...            0.5576  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the results side by side\n",
    "pd.concat([pre_filter_results_df, post_filter_results_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b041cd8b",
   "metadata": {},
   "source": [
    "| Approach                          | Strength                            | Best For                                |\n",
    "| --------------------------------- | ----------------------------------- | --------------------------------------- |\n",
    "| **Pre-filter** (`CONTAINS` first) | Fast, keyword-strict                | Narrow keyword search                   |\n",
    "| **Post-filter** (this one)        | Semantically rich but still precise | Broader exploratory or research queries |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a008720",
   "metadata": {},
   "source": [
    "#### 4.3.3 Reciprocial Rank Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7aefbf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "import numpy as np\n",
    "import oracledb\n",
    "\n",
    "def hybrid_rrf_search(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    search_phrase: str,\n",
    "    top_k: int = 10,\n",
    "    per_list: int = 120,     # candidates from each list before fusion (>= 10x top_k is a good rule)\n",
    "    k: int = 60,             # RRF smoothing constant (60 is standard)\n",
    "    phrase_safe: bool = True,\n",
    "    show_explain: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Local-friendly RRF fusion of Vector + Oracle Text results on research_papers(text, embedding).\n",
    "\n",
    "    Prereqs (local/Docker Free OK):\n",
    "      - VECTOR column/index on research_papers(embedding)  -- HNSW\n",
    "      - Oracle Text index on research_papers(text)         -- e.g. CREATE SEARCH INDEX rp_text_idx ON research_papers(text);\n",
    "\n",
    "    RRF = 1/(k + r_vec) + 1/(k + r_txt), where r_vec and r_txt are ranks (1 = best).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Encode query for vector modality (align with your doc prefixing scheme)\n",
    "    qv = embedding_model.encode(\n",
    "        [f\"search_query: {search_phrase}\"],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )[0].astype(np.float32).tolist()\n",
    "    qv = array.array('f', qv)\n",
    "\n",
    "    # 2) Phrase-safe text query for Oracle Text (optional)\n",
    "    kw = f\"\\\"{search_phrase}\\\"\" if (phrase_safe and \" \" in search_phrase.strip()) else search_phrase\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        if show_explain:\n",
    "            cur.execute(\"ALTER SESSION SET statistics_level = ALL\")\n",
    "\n",
    "        sql = f\"\"\"\n",
    "            WITH\n",
    "            /* Vector top-N with ranks (higher similarity first) */\n",
    "            vec AS (\n",
    "              SELECT\n",
    "                arxiv_id,\n",
    "                title,\n",
    "                SUBSTR(text, 1, 200) AS text_snippet,\n",
    "                1 - VECTOR_DISTANCE(embedding, :q, COSINE) AS sim_vec,\n",
    "                ROW_NUMBER() OVER (ORDER BY 1 - VECTOR_DISTANCE(embedding, :q, COSINE) DESC) AS r_vec\n",
    "              FROM research_papers\n",
    "              FETCH APPROX FIRST {per_list} ROWS ONLY WITH TARGET ACCURACY 90\n",
    "            ),\n",
    "            /* Oracle Text top-N with ranks (higher SCORE(1) first) */\n",
    "            txt AS (\n",
    "              SELECT\n",
    "                arxiv_id,\n",
    "                title,\n",
    "                SUBSTR(text, 1, 200) AS text_snippet,\n",
    "                SCORE(1) AS score_txt,\n",
    "                ROW_NUMBER() OVER (ORDER BY SCORE(1) DESC) AS r_txt\n",
    "              FROM research_papers\n",
    "              WHERE CONTAINS(text, :kw, 1) > 0\n",
    "              FETCH FIRST {per_list} ROWS ONLY\n",
    "            ),\n",
    "            /* Fuse by arxiv_id; keep docs present in either list */\n",
    "            fused AS (\n",
    "              SELECT\n",
    "                COALESCE(v.arxiv_id, t.arxiv_id)           AS arxiv_id,\n",
    "                COALESCE(v.title,     t.title)             AS title,\n",
    "                COALESCE(v.text_snippet, t.text_snippet)   AS text_snippet,\n",
    "                NVL(v.r_vec,  999999) AS r_vec,\n",
    "                NVL(t.r_txt,  999999) AS r_txt,\n",
    "                NVL(v.sim_vec, 0)     AS sim_vec,\n",
    "                NVL(t.score_txt, 0)   AS score_txt\n",
    "              FROM vec v\n",
    "              FULL OUTER JOIN txt t\n",
    "                ON t.arxiv_id = v.arxiv_id\n",
    "            )\n",
    "            SELECT {\"/*+ GATHER_PLAN_STATISTICS */\" if show_explain else \"\"}\n",
    "              arxiv_id,\n",
    "              title,\n",
    "              text_snippet,\n",
    "              ROUND( (1.0/(:k + r_vec)) + (1.0/(:k + r_txt)), 6 ) AS rrf_score,\n",
    "              r_vec,\n",
    "              r_txt,\n",
    "              ROUND(sim_vec, 4)  AS sim_vec,\n",
    "              ROUND(score_txt,4) AS score_txt\n",
    "            FROM fused\n",
    "            ORDER BY rrf_score DESC, title\n",
    "            FETCH FIRST {top_k} ROWS ONLY\n",
    "        \"\"\"\n",
    "\n",
    "        cur.execute(sql, q=qv, kw=kw, k=k)\n",
    "        rows = cur.fetchall()\n",
    "        columns = [d[0] for d in cur.description]\n",
    "\n",
    "    exec_plan_text = None\n",
    "    if show_explain:\n",
    "        with conn.cursor() as cur_plan:\n",
    "            cur_plan.execute(\"\"\"\n",
    "                SELECT plan_table_output\n",
    "                FROM TABLE(DBMS_XPLAN.DISPLAY_CURSOR(NULL, NULL, 'ALLSTATS LAST +PREDICATE'))\n",
    "            \"\"\")\n",
    "            exec_plan_text = \"\\n\".join(r[0] for r in cur_plan.fetchall())\n",
    "            print(\"\\n====== Execution Plan (DBMS_XPLAN.DISPLAY_CURSOR) ======\")\n",
    "            print(exec_plan_text)\n",
    "            print(\"========================================================\\n\")\n",
    "\n",
    "    return rows, columns, exec_plan_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "128e687c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Hybrid Search: 'optimization'\n",
      "üìä Found 3 results\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARXIV_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>TEXT_SNIPPET</th>\n",
       "      <th>RRF_SCORE</th>\n",
       "      <th>R_VEC</th>\n",
       "      <th>R_TXT</th>\n",
       "      <th>SIM_VEC</th>\n",
       "      <th>SCORE_TXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0912.1394</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7392</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0901.0846</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>0.032002</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6409</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0904.4106</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>0.021811</td>\n",
       "      <td>116</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5576</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ARXIV_ID                                              TITLE  \\\n",
       "0  0912.1394  Are Near Earth Objects the Key to Optimization...   \n",
       "1  0901.0846  A Metric and Optimisation Scheme for Microlens...   \n",
       "2  0904.4106  Is the HR 8799 extrasolar system destined for ...   \n",
       "\n",
       "                                        TEXT_SNIPPET  RRF_SCORE  R_VEC  R_TXT  \\\n",
       "0  Are Near Earth Objects the Key to Optimization...   0.032787      1      1   \n",
       "1  A Metric and Optimisation Scheme for Microlens...   0.032002      2      3   \n",
       "2  Is the HR 8799 extrasolar system destined for ...   0.021811    116      2   \n",
       "\n",
       "   SIM_VEC  SCORE_TXT  \n",
       "0   0.7392         42  \n",
       "1   0.6409         11  \n",
       "2   0.5576         11  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows, columns, exec_plan = hybrid_rrf_search(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    search_phrase=SEARCH_TEXT_KEYWORDS,\n",
    "    top_k=3,\n",
    "    show_explain=False\n",
    ")\n",
    "\n",
    "rrf_results_df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "print(f\"üîç Hybrid Search: '{SEARCH_TEXT_KEYWORDS}'\")\n",
    "print(f\"üìä Found {len(rrf_results_df)} results\\n\")\n",
    "\n",
    "rrf_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a80ec43",
   "metadata": {},
   "source": [
    "## 4.4 Graph-Based Retrieval (Oracle SQL Property Graph)\n",
    "\n",
    "In this retrieval mode, we seed with vector similarity, then expand candidates through graph paths:\n",
    "- `Paper -> SIMILAR_TO -> Paper`\n",
    "- `Paper <- WROTE - Author - WROTE -> Paper`\n",
    "\n",
    "The final score blends seed vector relevance and graph-path evidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "250ef601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_search_research_papers(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    search_query: str,\n",
    "    top_k: int = 10,\n",
    "    seed_k: int = 25\n",
    "):\n",
    "    \"\"\"\n",
    "    Graph retrieval using Oracle SQL Property Graph + GRAPH_TABLE.\n",
    "    Seeds from vector similarity, then expands via SIMILAR_TO and shared-author paths.\n",
    "    \"\"\"\n",
    "\n",
    "    seed_k = max(seed_k, top_k)\n",
    "\n",
    "    query_embedding = embedding_model.encode(\n",
    "        [f\"search_query: {search_query}\"],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )[0].astype(np.float32).tolist()\n",
    "    \n",
    "    query_embedding_array = array.array('f', query_embedding)\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        WITH seed AS (\n",
    "            SELECT\n",
    "                arxiv_id,\n",
    "                1 - VECTOR_DISTANCE(embedding, :q, COSINE) AS seed_score\n",
    "            FROM research_papers\n",
    "            ORDER BY seed_score DESC\n",
    "            FETCH APPROX FIRST {seed_k} ROWS ONLY WITH TARGET ACCURACY 90\n",
    "        ),\n",
    "        seed_hits AS (\n",
    "            SELECT\n",
    "                arxiv_id AS source_arxiv_id,\n",
    "                arxiv_id AS candidate_arxiv_id,\n",
    "                seed_score,\n",
    "                'seed' AS relation_type,\n",
    "                seed_score AS edge_score\n",
    "            FROM seed\n",
    "        ),\n",
    "        sim_hops AS (\n",
    "            SELECT\n",
    "                s.arxiv_id AS source_arxiv_id,\n",
    "                gt.target_arxiv_id AS candidate_arxiv_id,\n",
    "                s.seed_score,\n",
    "                'similar_to' AS relation_type,\n",
    "                gt.edge_score AS edge_score\n",
    "            FROM seed s\n",
    "            JOIN GRAPH_TABLE(\n",
    "                research_graph\n",
    "                MATCH (src IS paper)-[e IS similar_to]->(dst IS paper)\n",
    "                COLUMNS (\n",
    "                    src.arxiv_id AS source_arxiv_id,\n",
    "                    dst.arxiv_id AS target_arxiv_id,\n",
    "                    e.sim_score AS edge_score\n",
    "                )\n",
    "            ) gt\n",
    "                ON gt.source_arxiv_id = s.arxiv_id\n",
    "        ),\n",
    "        author_hops AS (\n",
    "            SELECT\n",
    "                s.arxiv_id AS source_arxiv_id,\n",
    "                gt.target_arxiv_id AS candidate_arxiv_id,\n",
    "                s.seed_score,\n",
    "                'shared_author' AS relation_type,\n",
    "                1.0 AS edge_score\n",
    "            FROM seed s\n",
    "            JOIN GRAPH_TABLE(\n",
    "                research_graph\n",
    "                MATCH (src IS paper)<-[w1 IS wrote]-(a IS author)-[w2 IS wrote]->(dst IS paper)\n",
    "                COLUMNS (\n",
    "                    src.arxiv_id AS source_arxiv_id,\n",
    "                    dst.arxiv_id AS target_arxiv_id\n",
    "                )\n",
    "            ) gt\n",
    "                ON gt.source_arxiv_id = s.arxiv_id\n",
    "            WHERE gt.target_arxiv_id <> s.arxiv_id\n",
    "        ),\n",
    "        candidates AS (\n",
    "            SELECT * FROM seed_hits\n",
    "            UNION ALL\n",
    "            SELECT * FROM sim_hops\n",
    "            UNION ALL\n",
    "            SELECT * FROM author_hops\n",
    "        ),\n",
    "        scored AS (\n",
    "            SELECT\n",
    "                candidate_arxiv_id AS arxiv_id,\n",
    "                MAX(\n",
    "                    CASE relation_type\n",
    "                        WHEN 'seed' THEN seed_score\n",
    "                        WHEN 'similar_to' THEN (0.70 * seed_score) + (0.30 * edge_score)\n",
    "                        WHEN 'shared_author' THEN (0.85 * seed_score) + (0.15 * edge_score)\n",
    "                        ELSE seed_score\n",
    "                    END\n",
    "                ) AS graph_score\n",
    "            FROM candidates\n",
    "            GROUP BY candidate_arxiv_id\n",
    "        )\n",
    "        SELECT\n",
    "            rp.arxiv_id,\n",
    "            rp.title,\n",
    "            rp.abstract,\n",
    "            SUBSTR(rp.text, 1, 200) AS text_snippet,\n",
    "            ROUND(sc.graph_score, 4) AS graph_score\n",
    "        FROM scored sc\n",
    "        JOIN research_papers rp\n",
    "            ON rp.arxiv_id = sc.arxiv_id\n",
    "        ORDER BY graph_score DESC\n",
    "        FETCH FIRST {top_k} ROWS ONLY\n",
    "    \"\"\"\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(sql, q=query_embedding_array)\n",
    "        rows = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "\n",
    "    return rows, columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69a4d16",
   "metadata": {},
   "source": [
    "This quick check runs graph retrieval on the same sample query so you can compare graph-ranked results side-by-side with keyword/vector/hybrid modes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0256de57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Graph Search: 'optimization'\n",
      "üìä Results: 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARXIV_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>TEXT_SNIPPET</th>\n",
       "      <th>GRAPH_SCORE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0907.5062</td>\n",
       "      <td>Heating of near-Earth objects and meteoroids d...</td>\n",
       "      <td>It is known that near-Earth objects (NEOs) dur...</td>\n",
       "      <td>Heating of near-Earth objects and meteoroids d...</td>\n",
       "      <td>0.7453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0912.1394</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>This note suggests that near earth objects and...</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>0.7392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0912.3449</td>\n",
       "      <td>Galactic tide and secular orbital evolution</td>\n",
       "      <td>Equation of motion for the galactic tide is tr...</td>\n",
       "      <td>Galactic tide and secular orbital evolution ‚Äî ...</td>\n",
       "      <td>0.7321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004.3288</td>\n",
       "      <td>Pencil-Beam Surveys for Trans-Neptunian Object...</td>\n",
       "      <td>Two populations of minor bodies in the outer S...</td>\n",
       "      <td>Pencil-Beam Surveys for Trans-Neptunian Object...</td>\n",
       "      <td>0.7290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0910.1399</td>\n",
       "      <td>Mercury&amp;#39;s geochronology revised by applyin...</td>\n",
       "      <td>Model Production Function chronology uses dyna...</td>\n",
       "      <td>Mercury&amp;#39;s geochronology revised by applyin...</td>\n",
       "      <td>0.7284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ARXIV_ID                                              TITLE  \\\n",
       "0  0907.5062  Heating of near-Earth objects and meteoroids d...   \n",
       "1  0912.1394  Are Near Earth Objects the Key to Optimization...   \n",
       "2  0912.3449        Galactic tide and secular orbital evolution   \n",
       "3  1004.3288  Pencil-Beam Surveys for Trans-Neptunian Object...   \n",
       "4  0910.1399  Mercury&#39;s geochronology revised by applyin...   \n",
       "\n",
       "                                            ABSTRACT  \\\n",
       "0  It is known that near-Earth objects (NEOs) dur...   \n",
       "1  This note suggests that near earth objects and...   \n",
       "2  Equation of motion for the galactic tide is tr...   \n",
       "3  Two populations of minor bodies in the outer S...   \n",
       "4  Model Production Function chronology uses dyna...   \n",
       "\n",
       "                                        TEXT_SNIPPET  GRAPH_SCORE  \n",
       "0  Heating of near-Earth objects and meteoroids d...       0.7453  \n",
       "1  Are Near Earth Objects the Key to Optimization...       0.7392  \n",
       "2  Galactic tide and secular orbital evolution ‚Äî ...       0.7321  \n",
       "3  Pencil-Beam Surveys for Trans-Neptunian Object...       0.7290  \n",
       "4  Mercury&#39;s geochronology revised by applyin...       0.7284  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows, columns = graph_search_research_papers(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    search_query=SEARCH_TEXT_KEYWORDS,\n",
    "    top_k=5,\n",
    "    seed_k=20\n",
    ")\n",
    "\n",
    "graph_results_df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "print(f\"üîç Graph Search: '{SEARCH_TEXT_KEYWORDS}'\")\n",
    "print(f\"üìä Results: {len(graph_results_df)}\\n\")\n",
    "\n",
    "graph_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f0ef74",
   "metadata": {},
   "source": [
    "### 4.5 Compare Top Results Across Retrieval Strategies\n",
    "\n",
    "To make the retrieval behavior easy to inspect, the next cell builds a side-by-side table of titles.\n",
    "\n",
    "Each row is a retrieval strategy, and the columns `Top_1` to `Top_5` show the highest-ranked paper titles returned for the same query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86f9d8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrieval_strategy</th>\n",
       "      <th>Top_1</th>\n",
       "      <th>Top_2</th>\n",
       "      <th>Top_3</th>\n",
       "      <th>Top_4</th>\n",
       "      <th>Top_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>keyword</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vector</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>An Efficient Method for Modeling High Magnific...</td>\n",
       "      <td>Kepler Science Operations</td>\n",
       "      <td>Science-Operational Metrics and Issues for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hybrid_pre_filter</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hybrid_postfilter</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hybrid_rrf</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>A Metric and Optimisation Scheme for Microlens...</td>\n",
       "      <td>Is the HR 8799 extrasolar system destined for ...</td>\n",
       "      <td>An Efficient Method for Modeling High Magnific...</td>\n",
       "      <td>Kepler Science Operations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>graph</td>\n",
       "      <td>Heating of near-Earth objects and meteoroids d...</td>\n",
       "      <td>Are Near Earth Objects the Key to Optimization...</td>\n",
       "      <td>Galactic tide and secular orbital evolution</td>\n",
       "      <td>Pencil-Beam Surveys for Trans-Neptunian Object...</td>\n",
       "      <td>Mercury&amp;#39;s geochronology revised by applyin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  retrieval_strategy                                              Top_1  \\\n",
       "0            keyword  Are Near Earth Objects the Key to Optimization...   \n",
       "1             vector  Are Near Earth Objects the Key to Optimization...   \n",
       "2  hybrid_pre_filter  Are Near Earth Objects the Key to Optimization...   \n",
       "3  hybrid_postfilter  Are Near Earth Objects the Key to Optimization...   \n",
       "4         hybrid_rrf  Are Near Earth Objects the Key to Optimization...   \n",
       "5              graph  Heating of near-Earth objects and meteoroids d...   \n",
       "\n",
       "                                               Top_2  \\\n",
       "0  A Metric and Optimisation Scheme for Microlens...   \n",
       "1  A Metric and Optimisation Scheme for Microlens...   \n",
       "2  A Metric and Optimisation Scheme for Microlens...   \n",
       "3  A Metric and Optimisation Scheme for Microlens...   \n",
       "4  A Metric and Optimisation Scheme for Microlens...   \n",
       "5  Are Near Earth Objects the Key to Optimization...   \n",
       "\n",
       "                                               Top_3  \\\n",
       "0  Is the HR 8799 extrasolar system destined for ...   \n",
       "1  An Efficient Method for Modeling High Magnific...   \n",
       "2  Is the HR 8799 extrasolar system destined for ...   \n",
       "3  Is the HR 8799 extrasolar system destined for ...   \n",
       "4  Is the HR 8799 extrasolar system destined for ...   \n",
       "5        Galactic tide and secular orbital evolution   \n",
       "\n",
       "                                               Top_4  \\\n",
       "0                                                      \n",
       "1                          Kepler Science Operations   \n",
       "2                                                      \n",
       "3                                                      \n",
       "4  An Efficient Method for Modeling High Magnific...   \n",
       "5  Pencil-Beam Surveys for Trans-Neptunian Object...   \n",
       "\n",
       "                                               Top_5  \n",
       "0                                                     \n",
       "1  Science-Operational Metrics and Issues for the...  \n",
       "2                                                     \n",
       "3                                                     \n",
       "4                          Kepler Science Operations  \n",
       "5  Mercury&#39;s geochronology revised by applyin...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_top_titles(rows, columns, k=5):\n",
    "    \"\"\"Return top-k titles from retrieval results.\"\"\"\n",
    "    titles = []\n",
    "    for row in rows[:k]:\n",
    "        row_data = dict(zip(columns, row))\n",
    "        titles.append(row_data.get(\"TITLE\", \"Untitled\"))\n",
    "    return titles\n",
    "\n",
    "\n",
    "comparison_specs = [\n",
    "    (\"keyword\", lambda: keyword_search_research_papers(conn, SEARCH_TEXT_KEYWORDS)),\n",
    "    (\"vector\", lambda: vector_search_research_papers(conn, embedding_model, SEARCH_TEXT_KEYWORDS, top_k=5)),\n",
    "    (\n",
    "        \"hybrid_pre_filter\",\n",
    "        lambda: hybrid_search_research_papers_pre_filter(\n",
    "            conn=conn,\n",
    "            embedding_model=embedding_model,\n",
    "            search_phrase=SEARCH_TEXT_KEYWORDS,\n",
    "            top_k=5,\n",
    "            show_explain=False,\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"hybrid_postfilter\",\n",
    "        lambda: hybrid_search_research_papers_postfilter(\n",
    "            conn=conn,\n",
    "            embedding_model=embedding_model,\n",
    "            search_phrase=SEARCH_TEXT_KEYWORDS,\n",
    "            top_k=5,\n",
    "            candidate_k=200,\n",
    "            show_explain=False,\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"hybrid_rrf\",\n",
    "        lambda: hybrid_rrf_search(\n",
    "            conn=conn,\n",
    "            embedding_model=embedding_model,\n",
    "            search_phrase=SEARCH_TEXT_KEYWORDS,\n",
    "            top_k=5,\n",
    "            per_list=60,\n",
    "            k=60,\n",
    "            show_explain=False,\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"graph\",\n",
    "        lambda: graph_search_research_papers(\n",
    "            conn=conn,\n",
    "            embedding_model=embedding_model,\n",
    "            search_query=SEARCH_TEXT_KEYWORDS,\n",
    "            top_k=5,\n",
    "            seed_k=20,\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "comparison_rows = []\n",
    "for strategy_name, runner in comparison_specs:\n",
    "    result = runner()\n",
    "\n",
    "    if isinstance(result, tuple) and len(result) >= 2:\n",
    "        rows, columns = result[0], result[1]\n",
    "    else:\n",
    "        rows, columns = [], []\n",
    "\n",
    "    titles = extract_top_titles(rows, columns, k=5)\n",
    "\n",
    "    record = {\"retrieval_strategy\": strategy_name}\n",
    "    for i in range(5):\n",
    "        record[f\"Top_{i+1}\"] = titles[i] if i < len(titles) else \"\"\n",
    "\n",
    "    comparison_rows.append(record)\n",
    "\n",
    "retrieval_strategy_comparison_df = pd.DataFrame(comparison_rows)\n",
    "retrieval_strategy_comparison_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0832de",
   "metadata": {},
   "source": [
    "# Part 5: Building a RAG Pipeline\n",
    "\n",
    "In this section, we connect retrieval to generation end-to-end.\n",
    "\n",
    "Flow for this part:\n",
    "1. Configure API access and initialize the OpenAI client.\n",
    "2. Define a reusable RAG function that supports multiple retrieval modes.\n",
    "3. Retrieve evidence from Oracle and synthesize a grounded answer.\n",
    "4. Run a sample query to validate the pipeline behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c1c9a2",
   "metadata": {},
   "source": [
    "### 5.1 Configure API Access\n",
    "\n",
    "The next two cells define a small helper to securely set environment variables and prompt for `OPENAI_API_KEY`. This keeps credentials out of source code and notebook outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "741e36c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Function to securely get and set environment variables\n",
    "def set_env_securely(var_name, prompt):\n",
    "    value = getpass.getpass(prompt)\n",
    "    os.environ[var_name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1cf5406",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_env_securely(\"OPENAI_API_KEY\", \"Enter your OPEN API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a90d90f",
   "metadata": {},
   "source": [
    "### 5.2 Initialize and Smoke-Test the OpenAI Client\n",
    "\n",
    "Before building the full pipeline, we perform a lightweight client call to confirm model access and credential configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f41960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OpenAI Python client library\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Initialize the OpenAI client (API key read from env var OPENAI_API_KEY)\n",
    "openai_client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Use the Responses API\n",
    "response = openai_client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Hello! I‚Äôm a user!\",\n",
    "    instructions=\"You are a research paper assistant.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27805348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Great to meet you. How can I help with your research or writing today?\n",
      "\n",
      "I can assist with:\n",
      "- Choosing/refining a topic or research question\n",
      "- Finding and summarizing relevant literature\n",
      "- Building an outline or argument\n",
      "- Drafting, editing, and clarity improvements\n",
      "- Citations and formatting (APA/MLA/Chicago/IEEE)\n",
      "- Data interpretation and figure/table planning\n",
      "\n",
      "If you share your field, assignment goals, style guide, and timeline, I‚Äôll tailor a plan.\n"
     ]
    }
   ],
   "source": [
    "# Print the output text\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d97587f",
   "metadata": {},
   "source": [
    "### 5.3 Define the Reusable RAG Function\n",
    "\n",
    "This function is the core integration point between retrieval and generation. It:\n",
    "- selects retrieval strategy (`keyword`, `vector`, `hybrid`, or `graph`),\n",
    "- formats retrieved rows into citation-ready context,\n",
    "- calls the Responses API with grounding instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "005a01c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_paper_assistant_rag_pipeline(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    user_query: str,\n",
    "    top_k: int = 10,\n",
    "    retrieval_mode: str = \"hybrid\",\n",
    "    show_explain: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Research Paper Assistant ‚Äî Retrieval-Augmented Generation (RAG) pipeline\n",
    "    built on SQL-based retrieval functions and powered by the OpenAI Responses API.\n",
    "\n",
    "    Retrieval techniques available:\n",
    "        - 'keyword'  ‚Üí uses keyword_search_research_papers()\n",
    "        - 'vector'   ‚Üí uses vector_search_research_papers()\n",
    "        - 'hybrid'   ‚Üí uses hybrid_search_research_papers_pre_filter() [default]\n",
    "        - 'graph'    ‚Üí uses graph_search_research_papers() via SQL Property Graph\n",
    "\n",
    "    Args:\n",
    "        conn: Oracle database connection.\n",
    "        embedding_model: Embedding model (e.g., SentenceTransformer, Voyage).\n",
    "        user_query (str): Research question from the user.\n",
    "        top_k (int): Number of top documents to retrieve.\n",
    "        retrieval_mode (str): Retrieval method ('keyword', 'vector', 'hybrid', 'graph').\n",
    "        show_explain (bool): Whether to show the SQL execution plan.\n",
    "\n",
    "    Returns:\n",
    "        str: LLM-generated research synthesis with citations.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 1. Retrieve relevant research papers using the selected retrieval mode\n",
    "    # ----------------------------------------------------------------------\n",
    "    if retrieval_mode == \"keyword\":\n",
    "        rows, columns = keyword_search_research_papers(conn, user_query)\n",
    "        exec_plan_text = None\n",
    "\n",
    "    elif retrieval_mode == \"vector\":\n",
    "        rows, columns = vector_search_research_papers(conn, embedding_model, user_query, top_k)\n",
    "        exec_plan_text = None\n",
    "\n",
    "    elif retrieval_mode == \"graph\":\n",
    "        rows, columns = graph_search_research_papers(conn, embedding_model, user_query, top_k=top_k)\n",
    "        exec_plan_text = None\n",
    "\n",
    "    else:  # default: hybrid retrieval\n",
    "        rows, columns, exec_plan_text = hybrid_search_research_papers_pre_filter(\n",
    "            conn=conn,\n",
    "            embedding_model=embedding_model,\n",
    "            search_phrase=user_query,\n",
    "            top_k=top_k,\n",
    "            show_explain=show_explain\n",
    "        )\n",
    "\n",
    "    retrieved_count = len(rows) if rows else 0\n",
    "    print(f\"üìä Retrieved {retrieved_count} papers using {retrieval_mode.upper()} retrieval.\")\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 2. Convert retrieved rows to formatted LLM context\n",
    "    # ----------------------------------------------------------------------\n",
    "    formatted_context = \"\"\n",
    "    if retrieved_count > 0:\n",
    "        formatted_context += f\"\\n\\nüìö {retrieved_count} relevant research papers retrieved:\\n\\n\"\n",
    "        for i, row in enumerate(rows):\n",
    "            row_data = dict(zip(columns, row))\n",
    "            title = row_data.get(\"TITLE\", \"Untitled Paper\")\n",
    "            abstract = row_data.get(\"ABSTRACT\", \"No abstract available.\")\n",
    "            snippet = row_data.get(\"TEXT_SNIPPET\", \"\")\n",
    "            score = (\n",
    "                row_data.get(\"GRAPH_SCORE\")\n",
    "                or row_data.get(\"SIMILARITY_SCORE\")\n",
    "                or row_data.get(\"RELEVANCE_SCORE\")\n",
    "                or row_data.get(\"TEXT_RELEVANCE_SCORE\")\n",
    "                or \"N/A\"\n",
    "            )\n",
    "            formatted_context += (\n",
    "                f\"[{i+1}] **{title}**\\n\"\n",
    "                f\"Abstract: {abstract}\\n\"\n",
    "                f\"Snippet: {snippet}\\n\"\n",
    "                f\"Relevance Score: {score}\\n\\n\"\n",
    "            )\n",
    "    else:\n",
    "        formatted_context = \"\\n\\n‚ö†Ô∏è No relevant papers were retrieved from the database.\\n\"\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 3. Construct the prompt for the Responses API\n",
    "    # ----------------------------------------------------------------------\n",
    "    prompt = f\"\"\"\n",
    "            You are a **Research Paper Assistant** that synthesizes academic literature to help answer user questions.\n",
    "\n",
    "            User Query: {user_query}\n",
    "\n",
    "            Number of retrieved papers: {retrieved_count}\n",
    "            {formatted_context}\n",
    "\n",
    "            Please:\n",
    "            - Summarize the findings most relevant to the query.\n",
    "            - Use citation numbers [X] to support claims.\n",
    "            - Highlight consensus, innovation, or research gaps.\n",
    "            - If there is insufficient context, clearly say so.\n",
    "            \"\"\"\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 4. Call the OpenAI Responses API\n",
    "    # ----------------------------------------------------------------------\n",
    "    response = openai_client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        input=prompt,\n",
    "        instructions=\"You are a scientific research assistant. Use only the provided context to answer. Always cite papers [1], [2], etc.\",\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 5. Optionally print SQL execution plan (if hybrid)\n",
    "    # ----------------------------------------------------------------------\n",
    "    if show_explain and exec_plan_text:\n",
    "        print(\"\\n====== SQL Execution Plan ======\")\n",
    "        print(exec_plan_text)\n",
    "        print(\"================================\\n\")\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 6. Return the LLM‚Äôs output text\n",
    "    # ----------------------------------------------------------------------\n",
    "    return response.output_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e8a3df",
   "metadata": {},
   "source": [
    "### 5.4 Run an End-to-End RAG Example\n",
    "\n",
    "This invocation exercises the full path from user query to retrieval to synthesized answer, so you can validate retrieval quality and response grounding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f4b00b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Retrieved 3 papers using HYBRID retrieval.\n",
      "The retrieved papers discuss different aspects of optimization in various contexts:\n",
      "\n",
      "1. **Central Force Optimization and Near Earth Objects**: The paper suggests a novel connection between Near Earth Objects (NEOs) and Central Force Optimization (CFO). It posits that NEO theory might help address challenges in deterministic optimization, such as local trapping and proof of convergence. The paper highlights the similarity between CFO's Davg and an NEO's Delta-V curves, suggesting that CFO's metaphorical \"gravity\" behaves like real gravity, potentially offering insights into optimization problems [1].\n",
      "\n",
      "2. **Microlens Planet Searches**: This research focuses on optimizing the search for planets using microlensing events. It proposes an automatic prioritization algorithm that optimizes the detection of planet anomalies by considering various observational parameters. The scheme aims to maximize the planet detection capability by recommending which targets to observe and the optimal exposure times, thus enhancing the efficiency of microlens observations [2].\n",
      "\n",
      "3. **Stability of the HR 8799 Extrasolar System**: The study investigates the dynamical stability of the HR 8799 extrasolar system using the GAMP method, which incorporates stability constraints into the optimization algorithm. The research finds limited stable regions for the system's planetary configurations and suggests that stability might be maintained through mean motion resonances. However, most configurations eventually become unstable, indicating potential planetary scattering [3].\n",
      "\n",
      "**Consensus and Innovation**:\n",
      "- The papers collectively highlight the application of optimization techniques in diverse fields, from astrophysics to algorithmic development.\n",
      "- The innovative connection between NEOs and optimization theory in [1] presents a novel perspective that could open new research avenues.\n",
      "- The optimization scheme for microlens planet searches in [2] demonstrates practical applications of optimization algorithms in enhancing observational strategies.\n",
      "\n",
      "**Research Gaps**:\n",
      "- Further exploration of the NEO-CFO connection could provide deeper insights into deterministic optimization challenges [1].\n",
      "- The long-term stability of extrasolar systems like HR 8799 requires more comprehensive modeling to understand the role of resonances and potential scattering events [3].\n",
      "\n",
      "Overall, the papers illustrate the broad applicability of optimization techniques and highlight areas for further research and development.\n"
     ]
    }
   ],
   "source": [
    "summary = research_paper_assistant_rag_pipeline(\n",
    "    conn=conn,\n",
    "    embedding_model=embedding_model,\n",
    "    user_query=SEARCH_TEXT_KEYWORDS,\n",
    "    top_k=5,\n",
    "    retrieval_mode=\"hybrid\",  # options: 'keyword', 'vector', 'hybrid', 'graph'\n",
    "    show_explain=False\n",
    ")\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104b31b8",
   "metadata": {},
   "source": [
    "# Part 6: AI Agents with OpenAI and Oracle AI Database\n",
    "\n",
    "In this section, we move from a single RAG call to an agentic system.\n",
    "\n",
    "You will build the flow incrementally:\n",
    "1. Start with a base agent.\n",
    "2. Add retrieval tools backed by Oracle SQL.\n",
    "3. Upgrade to multi-tool routing and orchestration.\n",
    "4. Add conversation history and persistent session memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c4bd7f",
   "metadata": {},
   "source": [
    "### 6.1 Install Agent Runtime and Verify Package Compatibility\n",
    "\n",
    "Use the notebook-native `%pip` command so installation happens in the **active kernel environment**.\n",
    "\n",
    "We install `openai` and `openai-agents` together to avoid version mismatches, then print resolved versions before importing `agents`.\n",
    "\n",
    "If you still see import errors after install, restart the kernel and run this section again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74defe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq --no-cache-dir openai openai-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d87a8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /opt/homebrew/Caskroom/miniconda/base/envs/playground/bin/python\n",
      "openai version: 2.20.0\n",
      "openai-agents version: 0.8.4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import importlib.metadata as md\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"openai version:\", md.version(\"openai\"))\n",
    "print(\"openai-agents version:\", md.version(\"openai-agents\"))\n",
    "\n",
    "OPENAI_MODEL = \"gpt-5\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63cabf2",
   "metadata": {},
   "source": [
    "### 6.2 Create a Baseline Agent (No Tools Yet)\n",
    "\n",
    "Start simple: define a research-focused assistant and run one direct query. This establishes baseline behavior before enabling external tool use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1222a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner\n",
    "\n",
    "research_paper_assistant = Agent(\n",
    "    name=\"Research Paper Assistant\",\n",
    "    model=OPENAI_MODEL,\n",
    "    instructions=\"\"\"\n",
    "      You are a Research Paper Assistant focused on helping users explore, analyze, and summarize\n",
    "      academic research.\n",
    "\n",
    "      Maintain a professional, concise, and scholarly tone appropriate for research discussions.\n",
    "    \"\"\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1ab0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_result = await Runner.run(\n",
    "    starting_agent=research_paper_assistant,\n",
    "    input=\"Summarize recent research on optimization techniques for planetary exploration.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2eda5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a concise synthesis of recent (roughly past 5‚Äì7 years) research on optimization techniques for planetary exploration, spanning mission design, guidance and control, surface operations, autonomy, and campaign-level logistics.\n",
      "\n",
      "1) Interplanetary and campaign-level mission design\n",
      "- Multi-objective trajectory optimization: Continued advances in low-thrust and multi‚Äìgravity-assist design use hybrid global‚Äìlocal methods (e.g., evolutionary algorithms with gradient-based refinement) and constraint-handling for mission risk, time, and propellant. Surrogate models and machine learning‚Äìaccelerated evaluators (e.g., neural surrogates trained on high-fidelity propagations) reduce search time by orders of magnitude in preliminary design.\n",
      "- Robust/stochastic optimization: Chance constraints and scenario-based formulations handle ephemeris, maneuver execution, and system uncertainty, producing trajectories and campaign plans with explicit risk bounds.\n",
      "- Large-scale campaign optimization: Coupled design of orbits, launch windows, and asset interactions (e.g., Mars Sample Return, lunar surface campaigns) increasingly uses mixed-integer nonlinear programming (MINLP), decomposition (Benders/cutting planes), and multi-fidelity optimization to co-optimize cost, schedule, and science return.\n",
      "- Tools and ecosystems: Open-source astrodynamics and optimization toolchains (e.g., PyKEP for MGA/low-thrust, Tudat, GMAT, CasADi, Pyomo/OR-Tools, OpenMDAO for MDAO workflows) are widely used for rapid trade studies and surrogate-assisted design.\n",
      "\n",
      "2) Entry, descent, and landing (EDL) guidance and site selection\n",
      "- Real-time convex optimization: Successive convexification and state-triggered constraints enable onboard, fuel/time-optimal powered descent with hard safety constraints (thrust limits, plume impingement, keep-out zones). These methods have matured from simulation to hardware-in-the-loop tests and relevant field trials.\n",
      "- Risk-aware landing: Integration of hazard maps (from terrain-relative navigation) with chance-constrained landing guidance selects divert trajectories that maximize landing safety subject to propellant, time, and sensor constraints.\n",
      "- Aerocapture and skip-entry design: Optimization under uncertainty balances thermal loads, g-limits, and targeting accuracy using stochastic optimal control and robust MPC.\n",
      "\n",
      "3) Rover mobility, path planning, and navigation\n",
      "- Multi-objective surface path optimization: Graph search and sampling-based planners coupled to high-resolution digital terrain models optimize energy, slip risk, time, and science value. Energy-aware planners incorporate solar incidence, shadowing, thermal limits, and wheel‚Äìsoil interaction models.\n",
      "- Learning-enhanced traversability: Self-supervised vision and terrain segmentation feed optimization with calibrated cost maps; learning-to-optimize strategies prune search and bias sampling toward feasible, low-risk corridors.\n",
      "- Trajectory optimization and MPC: Kinodynamic planners and nonlinear MPC optimize rover speed profiles and steering with constraints from traction and actuator limits; receding-horizon updates manage uncertainty and emerging hazards.\n",
      "- Global‚Äìlocal coupling: Hierarchical schemes solve global routing for macro energy/time budgets while local optimizers refine drivable micro-trajectories, maintaining consistency with updated maps and power forecasts.\n",
      "\n",
      "4) Informative path planning and science autonomy\n",
      "- Bayesian optimization and Gaussian processes: Active-sensing planners choose measurement locations and rover/rotorcraft paths that maximize expected information gain about environmental fields (e.g., mineralogy, volatiles) under energy and time budgets.\n",
      "- Adaptive sampling under uncertainty: POMDPs and risk-bounded planners trade sampling value against navigation risk; online learning updates belief maps and replans to focus on high-yield science targets.\n",
      "- Multi-objective science‚Äìoperations trade: Methods balance science return with resource expenditure and operational safety using scalarization, Œµ-constraint, or Pareto-front exploration.\n",
      "\n",
      "5) Multi-robot cooperation and logistics\n",
      "- Distributed task allocation: Market-based algorithms (e.g., CBBA variants) and distributed constraint optimization solve allocation of sampling, transport, scouting, and construction tasks with limited communication and delayed updates.\n",
      "- Coverage, mapping, and routing: Vehicle routing with time windows and energy constraints (VRP variants), often formulated as MILP/MINLP with heuristics, plans multi-rover surveys and caching strategies for sample return.\n",
      "- Surface logistics for lunar/Martian bases: Optimization of ISRU site selection, haul routes, power beaming/cable layouts, and depot placement uses multi-criteria decision analysis and robust network design, accounting for terrain and operational uncertainty.\n",
      "\n",
      "6) Spacecraft and surface resource management\n",
      "- Energy/thermal/data scheduling: MILP and CP-SAT schedulers co-optimize power generation/consumption, thermal control, and downlink windows; robust scheduling handles visibility and fault contingencies.\n",
      "- Autonomy-aware planning: Hierarchical planners (e.g., EUROPA/ASPEN-style) integrate optimization with constraint-based reasoning for long-horizon activity plans, increasingly augmented with learning-based duration and power predictors.\n",
      "\n",
      "7) Communications and navigation infrastructure\n",
      "- Relay/constellation optimization: Design of lunar and Martian relay networks (e.g., NRHOs, frozen orbits, smallsat constellations) uses multi-objective optimization for coverage, dilution of precision, and Œîv/cost; robust formulations handle ephemeris and station-keeping uncertainty.\n",
      "- Opportunistic scheduling: Joint optimization of relay usage, downlink windows, and rover operations increases data return without violating power/thermal limits.\n",
      "\n",
      "8) Robustness, verification, and onboard implementability\n",
      "- Chance-constrained and distributionally robust optimization: Adopted across landing, routing, and scheduling to provide probabilistic safety guarantees with limited distributional knowledge.\n",
      "- Real-time feasibility: Embedded optimization via tailored convex solvers, code generation, and warm-starting enables flight-ready runtimes; anytime algorithms and certified bounds support decisions under hard deadlines.\n",
      "- Safety and verification: Control barrier functions and formal methods are being combined with optimization to provide verifiable safety envelopes for autonomous actions.\n",
      "\n",
      "Emerging directions\n",
      "- Learning to optimize and differentiable simulation: Differentiable physics and trajectory layers enable gradient-based end-to-end design; reinforcement learning is increasingly constrained by model-based safety filters or MPC backstops.\n",
      "- Cross-domain co-design: Joint optimization of structures, power, autonomy, and operations (MDAO) is moving from concept studies toward flight-relevant fidelity.\n",
      "- Heterogeneous teams: Coordinated optimization across orbiters, landers, rovers, and rotorcraft (e.g., scout-to-rover handoffs) promises large science gains per unit energy.\n",
      "- Explainable and certifiable autonomy: Bridging high performance with traceability and certification remains an active research frontier.\n",
      "\n",
      "Representative venues and resources\n",
      "- Venues: AIAA SciTech and GNC/Astrodynamics conferences; IEEE Aerospace; i-SAIRAS; ICRA/IROS/RSS; Journal of Guidance, Control, and Dynamics; Acta Astronautica; Autonomous Robots; IEEE Transactions on Robotics; Journal of Field Robotics.\n",
      "- Tools: PyKEP (ESA ACT) for interplanetary trajectories; Tudat and GMAT for astrodynamics; CasADi/ACADO for optimal control; Pyomo/OR-Tools for MILP/CP-SAT; OMPL for motion planning; OpenMDAO for MDAO workflows.\n",
      "\n",
      "Key open problems\n",
      "- Unified uncertainty quantification across physics, environment, and autonomy to support end-to-end robust optimization.\n",
      "- Trusted autonomy with formal guarantees under long delays and sparse communications.\n",
      "- High-fidelity yet fast models of terrain‚Äìrobot interaction for planning at scale.\n",
      "- Campaign-level optimization that co-designs infrastructure, logistics, and science strategy over multi-year horizons.\n",
      "\n",
      "If you‚Äôd like, I can tailor this survey to a specific mission class (e.g., lunar surface operations, Mars sample return, Titan rotorcraft) and provide a focused reading list with recent, citable papers.\n"
     ]
    }
   ],
   "source": [
    "print(run_result.final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ef7c",
   "metadata": {},
   "source": [
    "### 6.3 Expose Oracle Retrieval as a Callable Tool\n",
    "\n",
    "Here we wrap SQL retrieval in a `@function_tool`, attach it to the assistant, and run a query that should trigger tool usage. Inspecting raw responses helps verify tool-call arguments and execution order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd18f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.tool import function_tool\n",
    "\n",
    "@function_tool\n",
    "def get_research_papers(user_query: str, retrieval_mode: str = \"hybrid\", top_k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves academic research papers relevant to the user's query.\n",
    "\n",
    "    This tool queries the research_papers SQL table using one of four retrieval techniques:\n",
    "        - 'keyword'  ‚Üí lexical search via Oracle Text\n",
    "        - 'vector'   ‚Üí semantic similarity search\n",
    "        - 'hybrid'   ‚Üí combines keyword prefiltering + vector similarity (default)\n",
    "        - 'graph'    ‚Üí graph expansion via SQL Property Graph + GRAPH_TABLE\n",
    "\n",
    "    Use this tool when analyzing or summarizing scientific literature.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): Research topic or question to search for.\n",
    "        retrieval_mode (str): 'keyword', 'vector', 'hybrid', or 'graph'. Default is 'hybrid'.\n",
    "        top_k (int): Number of top papers to retrieve (default=5).\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted summary of the most relevant research papers.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Perform retrieval using SQL-based functions (defined earlier)\n",
    "    # ------------------------------------------------------------------\n",
    "    if retrieval_mode == \"keyword\":\n",
    "        rows, columns = keyword_search_research_papers(conn, user_query)\n",
    "    elif retrieval_mode == \"vector\":\n",
    "        rows, columns = vector_search_research_papers(conn, embedding_model, user_query, top_k)\n",
    "    elif retrieval_mode == \"graph\":\n",
    "        rows, columns = graph_search_research_papers(conn, embedding_model, user_query, top_k=top_k)\n",
    "    else:\n",
    "        rows, columns, _ = hybrid_search_research_papers_pre_filter(\n",
    "            conn=conn,\n",
    "            embedding_model=embedding_model,\n",
    "            search_phrase=user_query,\n",
    "            top_k=top_k,\n",
    "            show_explain=False\n",
    "        )\n",
    "\n",
    "    retrieved_count = len(rows) if rows else 0\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Format the output into a readable string\n",
    "    # ------------------------------------------------------------------\n",
    "    if retrieved_count == 0:\n",
    "        return f\"No research papers found related to '{user_query}'.\"\n",
    "\n",
    "    formatted_results = [f\"üìö {retrieved_count} papers retrieved for query: '{user_query}'\\n\"]\n",
    "    for i, row in enumerate(rows):\n",
    "        row_data = dict(zip(columns, row))\n",
    "        title = row_data.get(\"TITLE\", \"Untitled Paper\")\n",
    "        abstract = row_data.get(\"ABSTRACT\", \"No abstract available.\")\n",
    "        score = (\n",
    "            row_data.get(\"GRAPH_SCORE\")\n",
    "            or row_data.get(\"SIMILARITY_SCORE\")\n",
    "            or row_data.get(\"RELEVANCE_SCORE\")\n",
    "            or row_data.get(\"TEXT_RELEVANCE_SCORE\")\n",
    "            or \"N/A\"\n",
    "        )\n",
    "        formatted_results.append(\n",
    "            f\"[{i+1}] {title}\\n\"\n",
    "            f\"Abstract: {abstract}\\n\"\n",
    "            f\"Relevance Score: {score}\\n\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(formatted_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "625c3b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_paper_assistant.tools.append(get_research_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3bdca1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_result_with_tool = await Runner.run(\n",
    "    starting_agent=research_paper_assistant,\n",
    "    input=\"Get me information on rover navigation, planetary data collection, mission planning, resource allocation, or other related fields\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "92279dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a concise guide to key topics, core methods, and representative literature across rover navigation, planetary data collection, mission planning, and resource allocation. If you tell me which body (Mars/Moon), platform (flight heritage vs. new concepts), and time horizon you care about, I can deliver a focused, up-to-date reading list and summaries.\n",
      "\n",
      "Rover navigation (autonomy, perception, planning, mobility)\n",
      "- Core problems: stereo/monocular perception; visual odometry and SLAM; traversability and hazard assessment; slip estimation/terramechanics; local/global path planning under uncertainty; risk/energy-aware navigation; online mapping.\n",
      "- Canonical methods and systems:\n",
      "  - Visual odometry and stereo navigation on Mars: feature tracking and bundle-adjustment with outlier-robust pose estimation; grid-based traversability (e.g., GESTALT); autonomy for blind/guarded driving; recent speed-ups via FPGA/ASIC and improved path evaluators.\n",
      "  - Path planning: incremental graph search and any-angle replanning (e.g., D*, D* Lite, Field D*), plus risk/energy terms in cost functions.\n",
      "  - Terramechanics and slip: proprioceptive slip estimation, wheel‚Äìsoil interaction modeling, learning-based slip prediction and soil classification.\n",
      "  - Learning: self-/weakly supervised terrain classification and semantic segmentation with limited labels (transfer from Earth analogs + human-in-the-loop labeling).\n",
      "- Representative starting points:\n",
      "  - Maimone, Cheng, and Matthies, Two Years of Visual Odometry on the Mars Exploration Rovers, Journal of Field Robotics, 2007.\n",
      "  - Ferguson and Stentz, Field D*: An Interpolation-Based Path Planner and Replanner, Journal of Field Robotics, 2006.\n",
      "  - Iagnemma and Dubowsky, Mobile Robots in Rough Terrain: Estimation, Motion Planning, and Control, Springer, 2004.\n",
      "  - Angelova et al., Learning and Prediction of Slip from Proprioception, ICRA, 2007 (and follow-on work on self-supervised terrain classification).\n",
      "  - Biesiadecki, Verma, et al., Mars 2020/Perseverance surface navigation and AutoNav enhancements, 2021‚Äì2023 (flight reports and conference papers).\n",
      "\n",
      "Planetary data collection (in-situ sensing, science autonomy, adaptive sampling)\n",
      "- Core problems: selecting and prioritizing science targets; instrument placement under kinematic/geometry constraints; autonomous retargeting; adaptive sampling for limited time/energy/downlink.\n",
      "- Science autonomy and targeting:\n",
      "  - AEGIS: onboard detection, ranking, and autonomous retargeting for remote-sensing instruments (MER, Curiosity, and extended on Perseverance).\n",
      "- Instrument/data pipeline references (Mars 2020, Space Science Reviews special issue):\n",
      "  - SuperCam (Wiens et al., 2021), Mastcam-Z (Bell et al., 2021), PIXL (Allwood et al., 2020), SHERLOC (Bhartia et al., 2021), RIMFAX ground-penetrating radar (Hamran et al., 2020).\n",
      "- Adaptive sampling/informative path planning:\n",
      "  - Hollinger and Singh, Sampling-based informative path planning for environmental sensing, IJRR, 2012.\n",
      "  - Gaussian process‚Äìbased and Bayesian optimization approaches to maximize information gain under motion/energy constraints (e.g., Singh et al., 2009; Marchant & Ramos, 2014).\n",
      "\n",
      "Mission planning (mixed-initiative planning/scheduling, operations automation)\n",
      "- Core problems: daily/weekly tactical planning; constraint-based activity scheduling; resource/time-window reasoning; mixed-initiative workflows; robustness to execution uncertainty.\n",
      "- Heritage systems and frameworks:\n",
      "  - MAPGEN: mixed-initiative planning for the Mars Exploration Rovers (human‚ÄìAI collaboration to build daily plans under tight resource/temporal constraints).\n",
      "  - ASPEN/CASPER: model-based planning and continuous replanning (used operationally for spacecraft; basis for autonomy concepts on rovers/landers).\n",
      "  - EUROPA: constraint reasoning and temporal planning engine; SPIFe: user-facing planning/scheduling environment used across missions.\n",
      "- Representative references:\n",
      "  - Bresina, J√≥nsson, Morris, Rajan, MAPGEN: Mixed-Initiative Planning and Scheduling for MER, 2004‚Äì2005 (AI Magazine / I-SAIRAS).\n",
      "  - Rabideau, Chien, et al., Using ASPEN to automate spacecraft operations; CASPER: A Continuous Planning and Execution Framework, 1999‚Äì2005.\n",
      "  - Frank and J√≥nsson, Constraint-based modeling and planning with EUROPA, 2003.\n",
      "  - McCurdy et al., SPIFe for mission planning and ops, I-SAIRAS, 2012.\n",
      "\n",
      "Resource allocation (energy, data volume, computation, comms/DSN)\n",
      "- Core problems: multi-resource temporal planning (time, power, data, consumables); energy-aware routing; duty-cycling compute and instruments; downlink prioritization; DSN scheduling under conflicts.\n",
      "- Approaches:\n",
      "  - Constraint/optimization formulations for temporally extended activities with continuous resources (MILP, CP, heuristic search with resource envelopes).\n",
      "  - Energy-/risk-aware navigation (solar incidence, terrain grade, thermal limits) embedded in motion-planning objectives.\n",
      "  - Communications: onboard prioritization and compression; campaign-level DSN scheduling as large-scale CP/MILP with preferences and fairness.\n",
      "- Representative references:\n",
      "  - Benton, Coles, and Coles, Temporal planning with continuous time and resources, ICAPS, 2012.\n",
      "  - Rabideau et al., Resource-constrained planning in ASPEN for spacecraft/rover operations, 1999‚Äì2010.\n",
      "  - Johnston and Clement, The Deep Space Network scheduling problem (constraint-based approaches), 2005‚Äì2007.\n",
      "  - Otsu, Ono, et al., Risk- and energy-aware path planning for planetary rovers (JPL field robotics; e.g., IROS 2018‚Äì2020).\n",
      "\n",
      "Closely related fields you may want to include\n",
      "- Entry/Descent/Landing terrain-relative navigation: Johnson et al., Mars 2020 Lander Vision System and TRN, Space Science Reviews, 2022.\n",
      "- Multi-robot planetary exploration and cave/lava-tube scouting under comm constraints.\n",
      "- Datasets and benchmarks for perception:\n",
      "  - AI4Mars: human-labeled terrain semantics for Mars rover images (Wagstaff et al., 2021), used for modern segmentation/classification models.\n",
      "\n",
      "How I can tailor this further\n",
      "- If you specify one or two subtopics (e.g., ‚ÄúPerseverance AutoNav internals,‚Äù ‚Äúenergy-aware path planning for solar rovers,‚Äù ‚ÄúMAPGEN/ASPEN models for power and data,‚Äù or ‚ÄúAEGIS target selection and performance‚Äù), I can:\n",
      "  - Produce an annotated bibliography with summaries and takeaways.\n",
      "  - Map methods to flight heritage vs. research-only.\n",
      "  - Highlight 2021‚Äì2025 papers and provide links/preprints where available.\n"
     ]
    }
   ],
   "source": [
    "print(run_result_with_tool.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9afe21b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelResponse(output=[ResponseReasoningItem(id='rs_0764abc738cd65b200698d50c78bc48194b2b6bb1ec23c354b', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
      "                       ResponseFunctionToolCall(arguments='{\"user_query\":\"autonomous rover navigation AND planetary robotics AND SLAM OR terrain hazard avoidance OR path planning OR visual odometry\",\"retrieval_mode\":\"hybrid\",\"top_k\":5}', call_id='call_YSTKzm8avatRXv9vgQp3ix63', name='get_research_papers', type='function_call', id='fc_0764abc738cd65b200698d50cfda448194b194d6e6c8016533', status='completed'),\n",
      "                       ResponseFunctionToolCall(arguments='{\"user_query\":\"planetary data collection AND in-situ sensing OR remote sensing OR rover instruments OR sample caching OR planetary science field experiments\",\"retrieval_mode\":\"hybrid\",\"top_k\":5}', call_id='call_Ec92vP6SHmtIZ6pZTPKGI5Ep', name='get_research_papers', type='function_call', id='fc_0764abc738cd65b200698d50cfda548194a4f68197d6f7c734', status='completed'),\n",
      "                       ResponseFunctionToolCall(arguments='{\"user_query\":\"mission planning AND planetary rovers OR Mars exploration AND autonomous task planning OR scheduling OR mixed-initiative planning\",\"retrieval_mode\":\"hybrid\",\"top_k\":5}', call_id='call_icZe3xY66wGH2RUk84GiMEkR', name='get_research_papers', type='function_call', id='fc_0764abc738cd65b200698d50cfda6081949cdfe133a44adfa0', status='completed'),\n",
      "                       ResponseFunctionToolCall(arguments='{\"user_query\":\"resource allocation AND planetary missions OR rover energy management OR computation scheduling OR communications bandwidth optimization\",\"retrieval_mode\":\"hybrid\",\"top_k\":5}', call_id='call_Qifb4umHDI28X0ntDAG5y3Yp', name='get_research_papers', type='function_call', id='fc_0764abc738cd65b200698d50cfda688194af91d15a1986f49b', status='completed')],\n",
      "               usage=Usage(requests=1,\n",
      "                           input_tokens=198,\n",
      "                           input_tokens_details=InputTokensDetails(cached_tokens=0),\n",
      "                           output_tokens=575,\n",
      "                           output_tokens_details=OutputTokensDetails(reasoning_tokens=320),\n",
      "                           total_tokens=773,\n",
      "                           request_usage_entries=[]),\n",
      "               response_id='resp_0764abc738cd65b200698d50c7336081949b0ffc5948284a07'),\n",
      " ModelResponse(output=[ResponseReasoningItem(id='rs_0764abc738cd65b200698d50d1eec48194a44a2ad98421645a', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
      "                       ResponseFunctionToolCall(arguments='{\"user_query\":\"Mars rover navigation SLAM visual odometry hazard detection avoidance path planning Field D* Curiosity Perseverance AutoNav\",\"retrieval_mode\":\"hybrid\",\"top_k\":5}', call_id='call_wnqOeSK1Q2hdZplA5MI23Wdl', name='get_research_papers', type='function_call', id='fc_0764abc738cd65b200698d50dc4d88819481266f3d2cdc42c2', status='completed'),\n",
      "                       ResponseFunctionToolCall(arguments='{\"user_query\":\"in-situ planetary data collection rover instruments sampling strategies adaptive sampling Mars rover science autonomy\",\"retrieval_mode\":\"hybrid\",\"top_k\":5}', call_id='call_wYLdM0lAIVZTRp8fMcNSVqhg', name='get_research_papers', type='function_call', id='fc_0764abc738cd65b200698d50dc4d9c8194b7aa9bfb3f3a04ae', status='completed'),\n",
      "                       ResponseFunctionToolCall(arguments='{\"user_query\":\"autonomous planning and scheduling for planetary rovers CASPER ASPEN MAPGEN mixed-initiative Mars Exploration Rover operations\",\"retrieval_mode\":\"hybrid\",\"top_k\":5}', call_id='call_qLeEptObfcrwuv1VVLuIWEte', name='get_research_papers', type='function_call', id='fc_0764abc738cd65b200698d50dc4da88194aaa148dd410339b4', status='completed'),\n",
      "                       ResponseFunctionToolCall(arguments='{\"user_query\":\"resource-constrained planning rover energy management power-aware scheduling planetary rover operations communications bandwidth optimization DSN\",\"retrieval_mode\":\"hybrid\",\"top_k\":5}', call_id='call_fi9B6lMXildpCRTMx1qU98R0', name='get_research_papers', type='function_call', id='fc_0764abc738cd65b200698d50dc4db08194b4b210b9ac3df800', status='completed')],\n",
      "               usage=Usage(requests=1,\n",
      "                           input_tokens=3224,\n",
      "                           input_tokens_details=InputTokensDetails(cached_tokens=0),\n",
      "                           output_tokens=565,\n",
      "                           output_tokens_details=OutputTokensDetails(reasoning_tokens=320),\n",
      "                           total_tokens=3789,\n",
      "                           request_usage_entries=[]),\n",
      "               response_id='resp_0764abc738cd65b200698d50d16c788194a178440ebd8a3be6'),\n",
      " ModelResponse(output=[ResponseReasoningItem(id='rs_0764abc738cd65b200698d50dd63088194810be3583f50dee1', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
      "                       ResponseOutputMessage(id='msg_0764abc738cd65b200698d5150b57081949f4f7d6b1c07d803', content=[ResponseOutputText(annotations=[], text='Below is a concise guide to key topics, core methods, and representative literature across rover navigation, planetary data collection, mission planning, and resource allocation. If you tell me which body (Mars/Moon), platform (flight heritage vs. new concepts), and time horizon you care about, I can deliver a focused, up-to-date reading list and summaries.\\n\\nRover navigation (autonomy, perception, planning, mobility)\\n- Core problems: stereo/monocular perception; visual odometry and SLAM; traversability and hazard assessment; slip estimation/terramechanics; local/global path planning under uncertainty; risk/energy-aware navigation; online mapping.\\n- Canonical methods and systems:\\n  - Visual odometry and stereo navigation on Mars: feature tracking and bundle-adjustment with outlier-robust pose estimation; grid-based traversability (e.g., GESTALT); autonomy for blind/guarded driving; recent speed-ups via FPGA/ASIC and improved path evaluators.\\n  - Path planning: incremental graph search and any-angle replanning (e.g., D*, D* Lite, Field D*), plus risk/energy terms in cost functions.\\n  - Terramechanics and slip: proprioceptive slip estimation, wheel‚Äìsoil interaction modeling, learning-based slip prediction and soil classification.\\n  - Learning: self-/weakly supervised terrain classification and semantic segmentation with limited labels (transfer from Earth analogs + human-in-the-loop labeling).\\n- Representative starting points:\\n  - Maimone, Cheng, and Matthies, Two Years of Visual Odometry on the Mars Exploration Rovers, Journal of Field Robotics, 2007.\\n  - Ferguson and Stentz, Field D*: An Interpolation-Based Path Planner and Replanner, Journal of Field Robotics, 2006.\\n  - Iagnemma and Dubowsky, Mobile Robots in Rough Terrain: Estimation, Motion Planning, and Control, Springer, 2004.\\n  - Angelova et al., Learning and Prediction of Slip from Proprioception, ICRA, 2007 (and follow-on work on self-supervised terrain classification).\\n  - Biesiadecki, Verma, et al., Mars 2020/Perseverance surface navigation and AutoNav enhancements, 2021‚Äì2023 (flight reports and conference papers).\\n\\nPlanetary data collection (in-situ sensing, science autonomy, adaptive sampling)\\n- Core problems: selecting and prioritizing science targets; instrument placement under kinematic/geometry constraints; autonomous retargeting; adaptive sampling for limited time/energy/downlink.\\n- Science autonomy and targeting:\\n  - AEGIS: onboard detection, ranking, and autonomous retargeting for remote-sensing instruments (MER, Curiosity, and extended on Perseverance).\\n- Instrument/data pipeline references (Mars 2020, Space Science Reviews special issue):\\n  - SuperCam (Wiens et al., 2021), Mastcam-Z (Bell et al., 2021), PIXL (Allwood et al., 2020), SHERLOC (Bhartia et al., 2021), RIMFAX ground-penetrating radar (Hamran et al., 2020).\\n- Adaptive sampling/informative path planning:\\n  - Hollinger and Singh, Sampling-based informative path planning for environmental sensing, IJRR, 2012.\\n  - Gaussian process‚Äìbased and Bayesian optimization approaches to maximize information gain under motion/energy constraints (e.g., Singh et al., 2009; Marchant & Ramos, 2014).\\n\\nMission planning (mixed-initiative planning/scheduling, operations automation)\\n- Core problems: daily/weekly tactical planning; constraint-based activity scheduling; resource/time-window reasoning; mixed-initiative workflows; robustness to execution uncertainty.\\n- Heritage systems and frameworks:\\n  - MAPGEN: mixed-initiative planning for the Mars Exploration Rovers (human‚ÄìAI collaboration to build daily plans under tight resource/temporal constraints).\\n  - ASPEN/CASPER: model-based planning and continuous replanning (used operationally for spacecraft; basis for autonomy concepts on rovers/landers).\\n  - EUROPA: constraint reasoning and temporal planning engine; SPIFe: user-facing planning/scheduling environment used across missions.\\n- Representative references:\\n  - Bresina, J√≥nsson, Morris, Rajan, MAPGEN: Mixed-Initiative Planning and Scheduling for MER, 2004‚Äì2005 (AI Magazine / I-SAIRAS).\\n  - Rabideau, Chien, et al., Using ASPEN to automate spacecraft operations; CASPER: A Continuous Planning and Execution Framework, 1999‚Äì2005.\\n  - Frank and J√≥nsson, Constraint-based modeling and planning with EUROPA, 2003.\\n  - McCurdy et al., SPIFe for mission planning and ops, I-SAIRAS, 2012.\\n\\nResource allocation (energy, data volume, computation, comms/DSN)\\n- Core problems: multi-resource temporal planning (time, power, data, consumables); energy-aware routing; duty-cycling compute and instruments; downlink prioritization; DSN scheduling under conflicts.\\n- Approaches:\\n  - Constraint/optimization formulations for temporally extended activities with continuous resources (MILP, CP, heuristic search with resource envelopes).\\n  - Energy-/risk-aware navigation (solar incidence, terrain grade, thermal limits) embedded in motion-planning objectives.\\n  - Communications: onboard prioritization and compression; campaign-level DSN scheduling as large-scale CP/MILP with preferences and fairness.\\n- Representative references:\\n  - Benton, Coles, and Coles, Temporal planning with continuous time and resources, ICAPS, 2012.\\n  - Rabideau et al., Resource-constrained planning in ASPEN for spacecraft/rover operations, 1999‚Äì2010.\\n  - Johnston and Clement, The Deep Space Network scheduling problem (constraint-based approaches), 2005‚Äì2007.\\n  - Otsu, Ono, et al., Risk- and energy-aware path planning for planetary rovers (JPL field robotics; e.g., IROS 2018‚Äì2020).\\n\\nClosely related fields you may want to include\\n- Entry/Descent/Landing terrain-relative navigation: Johnson et al., Mars 2020 Lander Vision System and TRN, Space Science Reviews, 2022.\\n- Multi-robot planetary exploration and cave/lava-tube scouting under comm constraints.\\n- Datasets and benchmarks for perception:\\n  - AI4Mars: human-labeled terrain semantics for Mars rover images (Wagstaff et al., 2021), used for modern segmentation/classification models.\\n\\nHow I can tailor this further\\n- If you specify one or two subtopics (e.g., ‚ÄúPerseverance AutoNav internals,‚Äù ‚Äúenergy-aware path planning for solar rovers,‚Äù ‚ÄúMAPGEN/ASPEN models for power and data,‚Äù or ‚ÄúAEGIS target selection and performance‚Äù), I can:\\n  - Produce an annotated bibliography with summaries and takeaways.\\n  - Map methods to flight heritage vs. research-only.\\n  - Highlight 2021‚Äì2025 papers and provide links/preprints where available.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')],\n",
      "               usage=Usage(requests=1,\n",
      "                           input_tokens=4172,\n",
      "                           input_tokens_details=InputTokensDetails(cached_tokens=3456),\n",
      "                           output_tokens=5811,\n",
      "                           output_tokens_details=OutputTokensDetails(reasoning_tokens=4288),\n",
      "                           total_tokens=9983,\n",
      "                           request_usage_entries=[]),\n",
      "               response_id='resp_0764abc738cd65b200698d50dce3b881948b9758ceb6350285')]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(run_result_with_tool.raw_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a451b",
   "metadata": {},
   "source": [
    "### 6.4 Add a Second Tool and Enable Multi-Tool Access\n",
    "\n",
    "The first tool retrieves current literature. In this block, we add a second tool for past research conversations so the assistant can combine:\n",
    "- fresh evidence from the paper corpus, and\n",
    "- prior analytical context from earlier runs.\n",
    "\n",
    "This is the point where the agent starts making tool-selection decisions based on intent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "785c0266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.tool import function_tool\n",
    "\n",
    "@function_tool\n",
    "def get_past_research_conversations(user_query: str, top_k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves relevant past research-related conversations or analyses related to the query.\n",
    "\n",
    "    This tool searches a SQL database of prior research assistant conversations, \n",
    "    literature discussions, or synthesis sessions to find relevant context. \n",
    "    It allows the research assistant to recall previous analyses or summaries \n",
    "    that addressed similar topics, providing continuity and richer insights.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The research topic, concept, or question to search for.\n",
    "        top_k (int): Number of top past discussions to retrieve (default=5).\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted examples of relevant past research discussions.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Perform retrieval using the SQL-based hybrid search (vector + keyword)\n",
    "    # ------------------------------------------------------------------\n",
    "    rows, columns, _ = hybrid_search_research_papers_pre_filter(\n",
    "        conn=conn,\n",
    "        embedding_model=embedding_model,\n",
    "        search_phrase=user_query,\n",
    "        top_k=top_k,\n",
    "        show_explain=False\n",
    "    )\n",
    "\n",
    "    retrieved_count = len(rows) if rows else 0\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Format results for readability\n",
    "    # ------------------------------------------------------------------\n",
    "    if retrieved_count == 0:\n",
    "        return f\"No past research discussions found related to '{user_query}'.\"\n",
    "\n",
    "    formatted_results = [f\"üß† {retrieved_count} past research discussions retrieved for query: '{user_query}'\\n\"]\n",
    "    for i, row in enumerate(rows):\n",
    "        row_data = dict(zip(columns, row))\n",
    "        title = row_data.get(\"TITLE\", \"Untitled Discussion\")\n",
    "        abstract = row_data.get(\"ABSTRACT\", \"No summary available.\")\n",
    "        snippet = row_data.get(\"TEXT_SNIPPET\", \"\")\n",
    "        score = (\n",
    "            row_data.get(\"SIMILARITY_SCORE\")\n",
    "            or row_data.get(\"TEXT_RELEVANCE_SCORE\")\n",
    "            or \"N/A\"\n",
    "        )\n",
    "        formatted_results.append(\n",
    "            f\"[{i+1}] **{title}**\\n\"\n",
    "            f\"Summary: {abstract}\\n\"\n",
    "            f\"Snippet: {snippet}\\n\"\n",
    "            f\"Relevance Score: {score}\\n\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(formatted_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db0e96",
   "metadata": {},
   "source": [
    "### 6.5 Strengthen Agent Instructions for Tool Routing\n",
    "\n",
    "Tool availability alone is not enough. The instruction policy below makes routing explicit so the assistant knows:\n",
    "- when to call paper retrieval,\n",
    "- when to call conversation retrieval,\n",
    "- and when to combine both for broader synthesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f602932",
   "metadata": {},
   "outputs": [],
   "source": [
    "upgraded_research_paper_assistant = Agent(\n",
    "    name=\"Research Paper Assistant\",\n",
    "    model=OPENAI_MODEL,\n",
    "    instructions=\"\"\"\n",
    "    Always maintain an academic, evidence-based tone.\n",
    "    Your purpose is to help users explore, synthesize, and connect research insights ‚Äî\n",
    "    not to speculate or fabricate information.\n",
    "    \"\"\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fecf68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach research retrieval tools to the upgraded research assistant\n",
    "upgraded_research_paper_assistant.tools.append(get_research_papers)\n",
    "upgraded_research_paper_assistant.tools.append(get_past_research_conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a66d1b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FunctionTool(name='get_research_papers',\n",
      "              description='Retrieves academic research papers relevant to the '\n",
      "                          \"user's query.\",\n",
      "              params_json_schema={'additionalProperties': False,\n",
      "                                  'properties': {'retrieval_mode': {'default': 'hybrid',\n",
      "                                                                    'description': \"'keyword', \"\n",
      "                                                                                   \"'vector', \"\n",
      "                                                                                   \"'hybrid', \"\n",
      "                                                                                   'or '\n",
      "                                                                                   \"'graph'. \"\n",
      "                                                                                   'Default '\n",
      "                                                                                   'is '\n",
      "                                                                                   \"'hybrid'.\",\n",
      "                                                                    'title': 'Retrieval '\n",
      "                                                                             'Mode',\n",
      "                                                                    'type': 'string'},\n",
      "                                                 'top_k': {'default': 5,\n",
      "                                                           'description': 'Number '\n",
      "                                                                          'of '\n",
      "                                                                          'top '\n",
      "                                                                          'papers '\n",
      "                                                                          'to '\n",
      "                                                                          'retrieve '\n",
      "                                                                          '(default=5).',\n",
      "                                                           'title': 'Top K',\n",
      "                                                           'type': 'integer'},\n",
      "                                                 'user_query': {'description': 'Research '\n",
      "                                                                               'topic '\n",
      "                                                                               'or '\n",
      "                                                                               'question '\n",
      "                                                                               'to '\n",
      "                                                                               'search '\n",
      "                                                                               'for.',\n",
      "                                                                'title': 'User '\n",
      "                                                                         'Query',\n",
      "                                                                'type': 'string'}},\n",
      "                                  'required': ['user_query',\n",
      "                                               'retrieval_mode',\n",
      "                                               'top_k'],\n",
      "                                  'title': 'get_research_papers_args',\n",
      "                                  'type': 'object'},\n",
      "              on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x400d92e80>,\n",
      "              strict_json_schema=True,\n",
      "              is_enabled=True,\n",
      "              tool_input_guardrails=None,\n",
      "              tool_output_guardrails=None,\n",
      "              needs_approval=False),\n",
      " FunctionTool(name='get_past_research_conversations',\n",
      "              description='Retrieves relevant past research-related '\n",
      "                          'conversations or analyses related to the query.\\n'\n",
      "                          '\\n'\n",
      "                          'This tool searches a SQL database of prior research '\n",
      "                          'assistant conversations, \\n'\n",
      "                          'literature discussions, or synthesis sessions to '\n",
      "                          'find relevant context. \\n'\n",
      "                          'It allows the research assistant to recall previous '\n",
      "                          'analyses or summaries \\n'\n",
      "                          'that addressed similar topics, providing continuity '\n",
      "                          'and richer insights.',\n",
      "              params_json_schema={'additionalProperties': False,\n",
      "                                  'properties': {'top_k': {'default': 5,\n",
      "                                                           'description': 'Number '\n",
      "                                                                          'of '\n",
      "                                                                          'top '\n",
      "                                                                          'past '\n",
      "                                                                          'discussions '\n",
      "                                                                          'to '\n",
      "                                                                          'retrieve '\n",
      "                                                                          '(default=5).',\n",
      "                                                           'title': 'Top K',\n",
      "                                                           'type': 'integer'},\n",
      "                                                 'user_query': {'description': 'The '\n",
      "                                                                               'research '\n",
      "                                                                               'topic, '\n",
      "                                                                               'concept, '\n",
      "                                                                               'or '\n",
      "                                                                               'question '\n",
      "                                                                               'to '\n",
      "                                                                               'search '\n",
      "                                                                               'for.',\n",
      "                                                                'title': 'User '\n",
      "                                                                         'Query',\n",
      "                                                                'type': 'string'}},\n",
      "                                  'required': ['user_query', 'top_k'],\n",
      "                                  'title': 'get_past_research_conversations_args',\n",
      "                                  'type': 'object'},\n",
      "              on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x3fd54d120>,\n",
      "              strict_json_schema=True,\n",
      "              is_enabled=True,\n",
      "              tool_input_guardrails=None,\n",
      "              tool_output_guardrails=None,\n",
      "              needs_approval=False)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(upgraded_research_paper_assistant.tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b54a82ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_result_with_tools = await Runner.run(\n",
    "    starting_agent=upgraded_research_paper_assistant,\n",
    "    input=(\n",
    "        \"Get me information on rover navigation, planetary data collection, mission planning, resource allocation, or other related fields \"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b61f988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelResponse(output=[ResponseReasoningItem(id='rs_09d496f62f49549700698d516f9e048190ad151d3fcb89e052', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseFunctionToolCall(arguments='{\"user_query\":\"Autonomous rover navigation for planetary exploration: terrain assessment, visual odometry/SLAM, hazard detection, path planning (Mars, Moon)\",\"retrieval_mode\":\"hybrid\",\"top_k\":5}', call_id='call_25kev6IP64s3GbfOcxJYyGlp', name='get_research_papers', type='function_call', id='fc_09d496f62f49549700698d5184f368819094a6252f3d80b0f1', status='completed'), ResponseFunctionToolCall(arguments='{\"user_query\":\"Planetary data collection and onboard science autonomy: AEGIS, autonomous target selection, in-situ instrument operations, data compression/prioritization on Mars rovers\",\"retrieval_mode\":\"hybrid\",\"top_k\":5}', call_id='call_OPE66gJMfhosRL5NHb8h1oSx', name='get_research_papers', type='function_call', id='fc_09d496f62f49549700698d5184f3788190a63e88a1b8cba268', status='completed'), ResponseFunctionToolCall(arguments='{\"user_query\":\"Mission planning and scheduling for planetary rovers: MAPGEN, CASPER, constraint-based planning, mixed-initiative operations\",\"retrieval_mode\":\"hybrid\",\"top_k\":5}', call_id='call_dxGb4GoVk3DyRXuQacWTbQc0', name='get_research_papers', type='function_call', id='fc_09d496f62f49549700698d5184f380819080268adac8716172', status='completed'), ResponseFunctionToolCall(arguments='{\"user_query\":\"Resource allocation and energy-aware planning for planetary rovers: power/thermal constraints, multi-objective optimization, opportunistic science\",\"retrieval_mode\":\"hybrid\",\"top_k\":5}', call_id='call_dXHgQOi4FRg05802n0ajdr6M', name='get_research_papers', type='function_call', id='fc_09d496f62f49549700698d5184f388819092c3b3f64103f3ac', status='completed'), ResponseFunctionToolCall(arguments='{\"user_query\":\"Multi-rover coordination and exploration strategies: cooperative mapping, task allocation, comms-constrained planning for planetary surfaces\",\"retrieval_mode\":\"hybrid\",\"top_k\":5}', call_id='call_1OfhcFLXbsHUoe0i6eUDGziT', name='get_research_papers', type='function_call', id='fc_09d496f62f49549700698d5184f3908190b84d1053873a279e', status='completed')], usage=Usage(requests=1, input_tokens=331, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=745, output_tokens_details=OutputTokensDetails(reasoning_tokens=448), total_tokens=1076, request_usage_entries=[]), response_id='resp_09d496f62f49549700698d516f39b48190b0ea84ce5112cafc'), ModelResponse(output=[ResponseReasoningItem(id='rs_09d496f62f49549700698d518668d081908ea3c9db9e10f860', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_09d496f62f49549700698d51ccfc7c81908b282b403e13c67a', content=[ResponseOutputText(annotations=[], text='Below is a concise, research-grounded overview of key areas in planetary surface exploration‚Äîrover navigation, planetary data collection, mission planning, and resource allocation‚Äîplus representative, well-cited systems and papers to get you oriented.\\n\\nRover navigation\\n- Core challenges: precise localization with sparse/no GNSS; visual/terrain hazards (slopes, rocks, sand); limited compute/power; uncertain traction and slip; long communication latency.\\n- Perception and localization:\\n  - Stereo-based terrain modeling and hazard detection (flight heritage on MER/MSL/M2020).\\n  - Visual odometry (VO) and visual-inertial odometry (VIO) for slip-aware localization on Mars.\\n  - Learning-based terrain segmentation and traversability estimation increasingly used for risk-aware planning.\\n  - Representative references:\\n    - Maimone, Cheng, Matthies (2007), ‚ÄúTwo Years of Visual Odometry on Mars,‚Äù Journal of Field Robotics.\\n    - Goldberg et al. (2002‚Äì2004), GESTALT reactive navigation and hazard detection for MER (JPL technical reports/IEEE Aerospace).\\n    - Angelova et al. (2007), slip prediction from imagery for planetary rovers, Journal of Field Robotics.\\n    - AI4Mars dataset (2020‚Äì2021), NASA/JPL: crowd-labeled Navcam images for terrain segmentation (public dataset and papers).\\n- Planning and control:\\n  - Local reactive planning (e.g., GESTALT) vs. global map-based replanning; D* variants and risk-aware costmaps.\\n  - Energy-/risk-aware path planning that trades distance vs slip vs power/thermal margins.\\n  - Mars 2020 added higher-throughput AutoNav with FPGA-accelerated vision and faster on-the-move hazard evaluation.\\n  - Representative references:\\n    - Ono et al. (2021), ‚ÄúMars 2020 Autonomous Navigation,‚Äù IEEE Aerospace Conference.\\n    - Iagnemma & Dubowsky (2004), Terramechanics-informed mobility/traction modeling for planetary rovers (Springer monograph and related papers).\\n\\nPlanetary data collection and onboard science autonomy\\n- Goals: target the best science under bandwidth/power/time limits; adapt to context while driving; triage/prioritize data.\\n- Flight-proven autonomy:\\n  - AEGIS on MER/Curiosity: onboard detection and targeting of scientifically interesting rocks/veins; used to automatically retarget the mast instruments during ‚Äúdrive-by‚Äù science.\\n  - Mars 2020 ‚ÄúAutonomous Target Selection‚Äù (ATS) extends AEGIS concepts for SuperCam/Mastcam-Z.\\n- Data management:\\n  - Onboard prioritization (thumbnailing, novelty detection) and compression (e.g., ICER wavelet compressor used on MER/MSL/M2020).\\n- Representative references:\\n  - Estlin et al. (2012), ‚ÄúAEGIS: Autonomous Targeting for the MER and MSL Rovers,‚Äù Journal of Field Robotics.\\n  - Burl et al. (2017‚Äì2021), Autonomous Target Selection for SuperCam (conference and mission ops papers).\\n  - Kiely & Klimesh (2003‚Äì2005), ‚ÄúICER: A Progressive Wavelet Image Compressor for Spacecraft,‚Äù JPL IPN Progress Reports/CCSDS 122.\\n\\nMission planning and operations (ground and onboard)\\n- Operational structure: strategic planning (weeks‚Äìmonths), campaign planning (sols‚Äìweeks), and tactical planning (daily) with mixed-initiative tools; onboard executives for robust execution.\\n- Ground tools:\\n  - MAPGEN: mixed-initiative activity planning and constraint reasoning for MER daily plans.\\n  - ASPEN/APGEN/SPIFe: constraint-based planning/scheduling, resource modeling, timeline editing, and validation (used across Mars missions and ISS).\\n- Onboard planning/execution:\\n  - CASPER/ASE: continuous planning and goal-driven replanning; flight heritage on Earth Observing-1 (demonstrated approach applicable to planetary missions).\\n  - MEXEC: flight executive used at JPL for resource-checked, time-tagged command execution.\\n- Representative references:\\n  - Bresina, J√≥nsson, Morris, Rajan (2005), ‚ÄúMAPGEN: Mixed-Initiative Planning and Scheduling for the Mars Exploration Rovers,‚Äù IEEE Intelligent Systems.\\n  - Chien et al. (2000‚Äì2016), CASPER/ASPEN and Autonomous Sciencecraft Experiment (Acta Astronautica; J. Aerospace Information Systems; mission reports).\\n  - Frank et al. (2014‚Äì2016), SPIFe scheduling for space operations (AIAA/IEEE/AIAA Space Ops).\\n\\nResource allocation and constraint management\\n- Key constraints: power (solar/RTG), thermal limits and warm-up times, data volume/downlink windows, time-on-target, mobility wear, and fault protection.\\n- Methods:\\n  - Constraint-based scheduling with resource envelopes; quantitative uncertainty margins.\\n  - Energy-aware traverse planning (costs incorporate slope, slip risk, soil type, thermal load).\\n  - Opportunistic science under resource guards (e.g., trigger extra observations only if power/thermal margins allow).\\n- Representative references:\\n  - Chien et al. (2012‚Äì2019), resource-aware planning with ASPEN/CASPER in space missions.\\n  - Rabideau et al. (2004‚Äì2010), automated scheduling for multi-constraint space operations (AIAA/AIAA Space Ops).\\n\\nMulti-robot and emerging architectures\\n- Multi-agent exploration:\\n  - Task allocation, decentralized mapping, communication-constrained planning; relevant for future lunar/Martian teams (rovers + aerial scouts).\\n  - Representative references:\\n    - Gerkey & Mataric (2004), ‚ÄúA formal analysis and taxonomy of task allocation in multi-robot systems,‚Äù International Journal of Robotics Research.\\n    - Cunningham et al. (2013), ‚ÄúFully Distributed Scalable Smoothing and Mapping (DDF-SAM),‚Äù RSS/IROS (multi-robot SLAM under limited comms).\\n- Heterogeneous teams:\\n  - Ingenuity-class flyers scouting for rovers; cooperative target confirmation and path scouting are active research directions.\\n\\nData, software, and testbeds\\n- Data sources:\\n  - NASA Planetary Data System (PDS): complete navcam, hazcam, mastcam products; rover telemetry and derived map products.\\n  - AI4Mars and AI4Mars 2.0 datasets for terrain segmentation; associated code baselines.\\n- Software:\\n  - F¬¥ (F Prime) flight software framework (JPL; used on Ingenuity and small spacecraft; relevant to future rover avionics).\\n  - Open-source SLAM/estimation libraries (GTSAM, ORB-SLAM derivatives) adapted in research prototypes for planetary-like environments.\\n- Field testbeds:\\n  - JPL Mars Yard campaigns; ExoMars/ESA ExoTeR; analog sites (deserts, volcanic terrains) for end-to-end autonomy trials.\\n\\nTrends and open problems\\n- Robust learning onboard: certifiable ML for perception/traversability; self-supervision from proprioception to reduce annotation burden.\\n- Compute acceleration: FPGA/ASIC pipelines for real-time stereo, VO/VIO, and segmentation under tight power budgets (as on Mars 2020).\\n- Risk- and science-aware integrated planning: unified objectives that trade traverse efficiency, safety, energy/thermal, and expected science value.\\n- Multi-agent operations: scalable tasking and mapping with severely intermittent links and time delays; cross-asset planning (orbiters as comm relays).\\n\\nIf you share your specific focus (e.g., ‚Äúlatest VO/VIO methods for offworld rovers,‚Äù ‚Äúonboard science autonomy for geochemical targets,‚Äù or ‚Äúconstraint-based daily planning for Mars-class missions‚Äù), I can assemble a targeted reading pack with seminal and recent papers, links to datasets/code, and a compact comparison of approaches.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], usage=Usage(requests=1, input_tokens=4799, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=4620, output_tokens_details=OutputTokensDetails(reasoning_tokens=2944), total_tokens=9419, request_usage_entries=[]), response_id='resp_09d496f62f49549700698d5185dcf481909932cec036b19cbc')]\n"
     ]
    }
   ],
   "source": [
    "print(run_result_with_tools.raw_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a57b24",
   "metadata": {},
   "source": [
    "## Agent-as-Tools Orchestration\n",
    "\n",
    "Now we compose specialists:\n",
    "- a research-paper retrieval specialist,\n",
    "- a past-conversation retrieval specialist,\n",
    "- an orchestrator that delegates,\n",
    "- and a final synthesizer that produces a cohesive response.\n",
    "\n",
    "This mirrors a production pattern where narrow agents perform focused work and a coordinator merges outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef7841",
   "metadata": {},
   "source": [
    "### Orchestration Flow\n",
    "\n",
    "Execution sequence in the next cells:\n",
    "1. Define specialized agents with clear scope.\n",
    "2. Register them as tools on an orchestrator agent.\n",
    "3. Run the orchestrator to gather relevant evidence.\n",
    "4. Pass gathered evidence to a synthesizer for final answer generation.\n",
    "\n",
    "The benefit is separation of concerns: retrieval logic is modular, and synthesis is centralized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a3dafca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define specialized agents for different research retrieval tasks\n",
    "research_paper_agent = Agent(\n",
    "    name=\"research_paper_agent\",\n",
    "    instructions=\"\"\"\n",
    "        You specialize in retrieving and summarizing academic research papers.\n",
    "        Use the get_research_papers tool to find relevant literature based on the user's query.\n",
    "        Always cite sources using [1], [2], etc., and focus on summarizing key findings,\n",
    "        methodologies, and implications of the studies retrieved.\n",
    "    \"\"\",\n",
    "    handoff_description=\"A research retrieval specialist with access to academic papers and literature databases.\",\n",
    "    tools=[get_research_papers],\n",
    ")\n",
    "\n",
    "research_conversation_agent = Agent(\n",
    "    name=\"research_conversation_agent\",\n",
    "    instructions=\"\"\"\n",
    "        You specialize in retrieving and summarizing past research discussions and analyses.\n",
    "        Use the get_past_research_conversations tool to surface relevant prior sessions\n",
    "        or summaries that relate to the user's current topic of inquiry.\n",
    "        Present these as context and examples of prior analytical reasoning.\n",
    "    \"\"\",\n",
    "    handoff_description=\"A research memory specialist with access to prior academic discussions and analyses.\",\n",
    "    tools=[get_past_research_conversations],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "304cac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an orchestrator agent that can coordinate both research retrieval agents\n",
    "orchestrator_agent = Agent(\n",
    "    name=\"research_assistant_orchestrator\",\n",
    "    instructions=(\n",
    "        \"You are a Research Orchestrator Assistant responsible for coordinating information retrieval \"\n",
    "        \"across multiple specialized research tools.\\n\\n\"\n",
    "        \"Your role is to help users explore, analyze, and synthesize academic research efficiently.\\n\\n\"\n",
    "        \"IMPORTANT RULES:\\n\"\n",
    "        \"1. ALWAYS use translate_to_research_papers when a query mentions research papers, studies, or findings.\\n\"\n",
    "        \"2. ALWAYS use translate_to_research_conversations when a query mentions previous discussions, analyses, or summaries.\\n\"\n",
    "        \"3. If a query requests BOTH new research and past discussions, use BOTH tools in sequence.\\n\"\n",
    "        \"4. NEVER attempt to provide research summaries without using your tools.\\n\"\n",
    "        \"5. Each tool provides complementary context ‚Äî use all appropriate tools for a comprehensive academic response.\\n\\n\"\n",
    "        \"After retrieving relevant results, synthesize them into a cohesive summary:\\n\"\n",
    "        \"- Clearly distinguish between newly retrieved research and recalled past discussions.\\n\"\n",
    "        \"- Cite sources using [1], [2], etc.\\n\"\n",
    "        \"- Identify key insights, trends, and research gaps.\\n\"\n",
    "        \"- Maintain an academic and objective tone.\"\n",
    "    ),\n",
    "    tools=[\n",
    "        research_paper_agent.as_tool(\n",
    "            tool_name=\"translate_to_research_papers\",\n",
    "            tool_description=\"Retrieve and summarize relevant academic research papers and literature findings.\",\n",
    "        ),\n",
    "        research_conversation_agent.as_tool(\n",
    "            tool_name=\"translate_to_research_conversations\",\n",
    "            tool_description=\"Retrieve and summarize past research discussions or analyses related to the topic.\",\n",
    "        ),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "62aab9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final agent to synthesize information from all sources (Research use case)\n",
    "synthesizer_agent = Agent(\n",
    "    name=\"research_response_synthesizer\",\n",
    "    instructions=(\n",
    "        \"You create comprehensive, well-organized research summaries by combining information from multiple sources.\\n\\n\"\n",
    "        \"When organizing your response:\\n\"\n",
    "        \"1) Start with a concise abstract-style overview (3‚Äì5 sentences) highlighting key findings and takeaways.\\n\"\n",
    "        \"2) Clearly separate NEW LITERATURE FINDINGS from PAST RESEARCH DISCUSSIONS.\\n\"\n",
    "        \"3) Cite sources using bracketed numbers [1], [2], etc., aligned with the retrieved items.\\n\"\n",
    "        \"4) Emphasize methods, evidence strength, and limitations; avoid speculation beyond the provided context.\\n\"\n",
    "        \"5) Use clear, scannable formatting (short paragraphs, bullet points where appropriate).\\n\"\n",
    "        \"6) Conclude with open questions, gaps, or future work suggested by the literature.\\n\"\n",
    "        \"7) If evidence is sparse, state this explicitly and avoid overgeneralization.\\n\"\n",
    "        \"Tone: academic, objective, and precise.\"\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2784d60",
   "metadata": {},
   "source": [
    "### 6.6 Run the Asynchronous Orchestration Workflow\n",
    "\n",
    "The next cells define async workflow execution, patch the notebook event loop, and provide a synchronous wrapper for interactive use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9f02800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import ItemHelpers, MessageOutputItem, trace\n",
    "from agents import Runner  # assuming Runner is imported elsewhere; include here for clarity\n",
    "\n",
    "\n",
    "async def research_assistant_workflow(user_query: str):\n",
    "    \"\"\"Run the complete research assistant workflow (orchestrate retrieval + synthesize).\"\"\"\n",
    "    # 1) Have the research orchestrator decide which tools to invoke\n",
    "    with trace(\"Research Orchestrator\"):\n",
    "        orchestrator_result = await Runner.run(orchestrator_agent, user_query)\n",
    "\n",
    "        # Debug/transparency: print intermediate orchestration steps\n",
    "        print(\"\\n--- Research Orchestration Steps ---\")\n",
    "        for item in orchestrator_result.new_items:\n",
    "            if isinstance(item, MessageOutputItem):\n",
    "                text = ItemHelpers.text_message_output(item)\n",
    "                if text:\n",
    "                    print(f\"  - Retrieval step: {text}\")\n",
    "\n",
    "        # 2) Synthesize all gathered information into a cohesive research summary\n",
    "        synthesizer_result = await Runner.run(\n",
    "            synthesizer_agent, orchestrator_result.to_input_list()\n",
    "        )\n",
    "\n",
    "        print(f\"\\n\\n--- Final Research Synthesis ---\\n{synthesizer_result.final_output}\\n\")\n",
    "\n",
    "    return synthesizer_result.final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cc193e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio to patch the event loop\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8bc86489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_virtual_research_assistant(query):\n",
    "    # Create a new event loop\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "\n",
    "    # Run the async function and get the result\n",
    "    result = loop.run_until_complete(research_assistant_workflow(query))\n",
    "\n",
    "    # Clean up\n",
    "    loop.close()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b705b630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Research Orchestration Steps ---\n",
      "  - Retrieval step: Understood. If you wish to end this session, you can simply close the window or tab. If you have further research questions or need assistance in the future, feel free to return. Thank you!\n",
      "\n",
      "\n",
      "--- Final Research Synthesis ---\n",
      "Session ending acknowledged. If you have additional questions or require future research summaries, feel free to request at any time.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Session ending acknowledged. If you have additional questions or require future research summaries, feel free to request at any time.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now call the function this way\n",
    "query = input(\"What research topic can I help you with today? \")\n",
    "run_virtual_research_assistant(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67e6c26",
   "metadata": {},
   "source": [
    "## Agentic Chat System\n",
    "\n",
    "The next block adds thread-aware conversation handling on top of orchestration.\n",
    "\n",
    "For each user turn, the system:\n",
    "1. Stores the new message in Oracle.\n",
    "2. Reconstructs conversation context by `thread_id`.\n",
    "3. Runs orchestration + synthesis with that context.\n",
    "4. Saves the assistant response back to Oracle.\n",
    "\n",
    "This pattern supports long-running research sessions with continuity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "05741d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table chat_history created successfully with index.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import uuid\n",
    "\n",
    "# Create chat_history table in Oracle\n",
    "with conn.cursor() as cur:\n",
    "    # Drop table if exists (for development)\n",
    "    cur.execute(\"\"\"\n",
    "        BEGIN\n",
    "            EXECUTE IMMEDIATE 'DROP TABLE chat_history';\n",
    "        EXCEPTION WHEN OTHERS THEN\n",
    "            IF SQLCODE != -942 THEN RAISE; END IF;\n",
    "        END;\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create chat_history table\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE chat_history (\n",
    "            id VARCHAR2(100) PRIMARY KEY,\n",
    "            thread_id VARCHAR2(100) NOT NULL,\n",
    "            role VARCHAR2(20) NOT NULL,\n",
    "            message CLOB NOT NULL,\n",
    "            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "        TABLESPACE USERS\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create index on thread_id and timestamp for efficient retrieval\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE INDEX idx_thread_timestamp \n",
    "        ON chat_history(thread_id, timestamp)\n",
    "        TABLESPACE USERS\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    print(\"‚úÖ Table chat_history created successfully with index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "34c470ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def research_assistant_chat(user_query, thread_id=None):\n",
    "    \"\"\"\n",
    "    Run the complete research assistant workflow with conversation history.\n",
    "    For each conversation turn:\n",
    "      - Stores the user's input and the assistant's output in Oracle along with a timestamp and thread_id.\n",
    "      - Retrieves and appends previous conversation history (ordered by timestamp) to the agent's input.\n",
    "    \n",
    "    If no thread_id is provided, a new conversation session is started.\n",
    "    \n",
    "    Returns:\n",
    "      tuple: (final_output, thread_id) where thread_id is the session identifier.\n",
    "    \"\"\"\n",
    "    # Generate a new thread id if not provided\n",
    "    if thread_id is None:\n",
    "        thread_id = str(uuid.uuid4())\n",
    "        print(f\"üìù New research conversation started with thread ID: {thread_id}\")\n",
    "    else:\n",
    "        print(f\"üìù Continuing research conversation with thread ID: {thread_id}\")\n",
    "    \n",
    "    # --- Step 1: Store the new user query in Oracle ---\n",
    "    message_id = str(uuid.uuid4())\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO chat_history (id, thread_id, role, message, timestamp)\n",
    "            VALUES (:id, :thread_id, :role, :message, CURRENT_TIMESTAMP)\n",
    "        \"\"\", {\n",
    "            'id': message_id,\n",
    "            'thread_id': thread_id,\n",
    "            'role': 'user',\n",
    "            'message': user_query\n",
    "        })\n",
    "        conn.commit()\n",
    "    \n",
    "    # --- Step 2: Retrieve full conversation history for context ---\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT role, message, timestamp\n",
    "            FROM chat_history\n",
    "            WHERE thread_id = :thread_id\n",
    "            ORDER BY timestamp ASC\n",
    "        \"\"\", {'thread_id': thread_id})\n",
    "        \n",
    "        chat_history = cur.fetchall()\n",
    "    \n",
    "    conversation_context = \"\"\n",
    "    for entry in chat_history:\n",
    "        role, message, timestamp = entry\n",
    "        if role == \"user\":\n",
    "            conversation_context += f\"User: {message}\\n\"\n",
    "        else:\n",
    "            conversation_context += f\"Assistant: {message}\\n\"\n",
    "    \n",
    "    # --- Step 3: Run the orchestrator agent with the conversation context ---\n",
    "    with trace(\"Research Orchestrator\"):\n",
    "        orchestrator_result = await Runner.run(orchestrator_agent, conversation_context)\n",
    "    \n",
    "    # Print intermediate processing steps for debugging/transparency\n",
    "    print(\"\\n--- Research Orchestrator Processing Steps ---\")\n",
    "    for item in orchestrator_result.new_items:\n",
    "        if isinstance(item, MessageOutputItem):\n",
    "            text = ItemHelpers.text_message_output(item)\n",
    "            if text:\n",
    "                print(f\"  - Information gathering step: {text}\")\n",
    "    \n",
    "    # --- Step 4: Run the synthesizer agent to produce a cohesive response ---\n",
    "    synthesizer_result = await Runner.run(\n",
    "        synthesizer_agent, orchestrator_result.to_input_list()\n",
    "    )\n",
    "    \n",
    "    # --- Step 5: Store the assistant's final output in Oracle ---\n",
    "    response_id = str(uuid.uuid4())\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO chat_history (id, thread_id, role, message, timestamp)\n",
    "            VALUES (:id, :thread_id, :role, :message, CURRENT_TIMESTAMP)\n",
    "        \"\"\", {\n",
    "            'id': response_id,\n",
    "            'thread_id': thread_id,\n",
    "            'role': 'assistant',\n",
    "            'message': synthesizer_result.final_output\n",
    "        })\n",
    "        conn.commit()\n",
    "    \n",
    "    print(f\"\\n\\n--- Final Research Response ---\\n{synthesizer_result.final_output}\\n\")\n",
    "    \n",
    "    return synthesizer_result.final_output, thread_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b6250b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_research_assistant_chat(query, thread_id=None):\n",
    "    \"\"\"\n",
    "    Run the research assistant synchronously.\n",
    "    Optionally, a thread_id can be provided to continue an existing conversation.\n",
    "    Returns a tuple (final_output, thread_id).\n",
    "    \"\"\"\n",
    "    # Create a new event loop\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "    \n",
    "    # Run the async function and get the result\n",
    "    result, thread_id = loop.run_until_complete(\n",
    "        research_assistant_chat(query, thread_id=thread_id)\n",
    "    )\n",
    "    \n",
    "    # Clean up the loop\n",
    "    loop.close()\n",
    "    \n",
    "    return result, thread_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e7cc560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_chat_session():\n",
    "    \"\"\"\n",
    "    Launches a research chat session that continues until the user enters 'q', 'exit', or 'quit'.\n",
    "    The session uses a persistent thread_id to preserve conversation history.\n",
    "    \"\"\"\n",
    "    print(\"üî¨ Starting Research Paper Assistant Chat\")\n",
    "    print(\"Type 'q', 'exit' or 'quit' to exit.\\n\")\n",
    "    \n",
    "    session_thread_id = None\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"What research topic can I help you with today? \")\n",
    "        \n",
    "        if query.lower() in [\"q\", \"exit\", \"quit\"]:\n",
    "            print(\"Exiting research chat session.\")\n",
    "            break\n",
    "        \n",
    "        response, session_thread_id = run_research_assistant_chat(\n",
    "            query, thread_id=session_thread_id\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüìö Assistant: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cf336fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Starting Research Paper Assistant Chat\n",
      "Type 'q', 'exit' or 'quit' to exit.\n",
      "\n",
      "Exiting research chat session.\n"
     ]
    }
   ],
   "source": [
    "# Start the research chat session\n",
    "research_chat_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5af5bfe",
   "metadata": {},
   "source": [
    "## Session Memory with Oracle AI Database\n",
    "\n",
    "This section implements a custom `OracleSession` adapter compatible with agent session APIs.\n",
    "\n",
    "Key capabilities:\n",
    "- persistent storage of message items,\n",
    "- retrieval in chronological order,\n",
    "- controlled trimming (`pop_item`) to manage memory budget,\n",
    "- full session reset (`clear_session`) when needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e20956c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union\n",
    "from datetime import datetime\n",
    "import oracledb\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "class OracleSession:\n",
    "    \"\"\"Custom Oracle session implementation following the Session protocol\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        session_id: str, \n",
    "        connection,\n",
    "        table_name: str = \"chat_history\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize Oracle session storage.\n",
    "        \n",
    "        Args:\n",
    "            session_id: Unique identifier for this conversation session\n",
    "            connection: Active oracledb connection object\n",
    "            table_name: Name of the Oracle table storing session data\n",
    "        \"\"\"\n",
    "        self.session_id = session_id\n",
    "        self.conn = connection\n",
    "        self.table_name = table_name\n",
    "    \n",
    "    async def get_items(self, limit: Optional[int] = None) -> List[dict]:\n",
    "        \"\"\"Retrieve conversation history for this session\"\"\"\n",
    "        try:\n",
    "            with self.conn.cursor() as cur:\n",
    "                if limit:\n",
    "                    cur.execute(f\"\"\"\n",
    "                        SELECT message\n",
    "                        FROM {self.table_name}\n",
    "                        WHERE thread_id = :session_id\n",
    "                        ORDER BY timestamp ASC\n",
    "                        FETCH FIRST :limit ROWS ONLY\n",
    "                    \"\"\", {'session_id': self.session_id, 'limit': limit})\n",
    "                else:\n",
    "                    cur.execute(f\"\"\"\n",
    "                        SELECT message\n",
    "                        FROM {self.table_name}\n",
    "                        WHERE thread_id = :session_id\n",
    "                        ORDER BY timestamp ASC\n",
    "                    \"\"\", {'session_id': self.session_id})\n",
    "                \n",
    "                rows = cur.fetchall()\n",
    "                \n",
    "                items = []\n",
    "                for row in rows:\n",
    "                    # Deserialize JSON from CLOB\n",
    "                    message_clob = row[0]\n",
    "                    if message_clob:\n",
    "                        message_str = message_clob.read() if hasattr(message_clob, 'read') else str(message_clob)\n",
    "                        items.append(json.loads(message_str))\n",
    "                \n",
    "                return items\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving items: {e}\")\n",
    "            return []\n",
    "    \n",
    "    async def add_items(self, items: List[dict]) -> None:\n",
    "        \"\"\"Store new items for this session\"\"\"\n",
    "        try:\n",
    "            with self.conn.cursor() as cur:\n",
    "                for item in items:\n",
    "                    item_id = str(uuid.uuid4())\n",
    "                    \n",
    "                    # Serialize the entire item as JSON\n",
    "                    message_json = json.dumps(item)\n",
    "                    \n",
    "                    # Extract role if available, otherwise default to 'system'\n",
    "                    role = item.get('role', 'system')\n",
    "                    \n",
    "                    cur.execute(f\"\"\"\n",
    "                        INSERT INTO {self.table_name} (id, thread_id, role, message, timestamp)\n",
    "                        VALUES (:id, :session_id, :role, :message, CURRENT_TIMESTAMP)\n",
    "                    \"\"\", {\n",
    "                        'id': item_id,\n",
    "                        'session_id': self.session_id,\n",
    "                        'role': role,\n",
    "                        'message': message_json\n",
    "                    })\n",
    "                \n",
    "                self.conn.commit()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding items: {e}\")\n",
    "            self.conn.rollback()\n",
    "    \n",
    "    async def pop_item(self, limit: Optional[int] = None) -> Optional[Union[dict, List[dict]]]:\n",
    "        \"\"\"\n",
    "        Remove and return the most recent item(s) for this session.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.conn.cursor() as cur:\n",
    "                # Pop a single most-recent item\n",
    "                if not limit or limit <= 1:\n",
    "                    cur.execute(f\"\"\"\n",
    "                        SELECT id, message\n",
    "                        FROM {self.table_name}\n",
    "                        WHERE thread_id = :session_id\n",
    "                        ORDER BY timestamp DESC\n",
    "                        FETCH FIRST 1 ROW ONLY\n",
    "                    \"\"\", {'session_id': self.session_id})\n",
    "                    \n",
    "                    row = cur.fetchone()\n",
    "                    \n",
    "                    if row:\n",
    "                        item_id, message_clob = row\n",
    "                        message_str = message_clob.read() if hasattr(message_clob, 'read') else str(message_clob)\n",
    "                        item = json.loads(message_str)\n",
    "                        \n",
    "                        # Delete the item\n",
    "                        cur.execute(f\"\"\"\n",
    "                            DELETE FROM {self.table_name}\n",
    "                            WHERE id = :id\n",
    "                        \"\"\", {'id': item_id})\n",
    "                        \n",
    "                        self.conn.commit()\n",
    "                        return item\n",
    "                    \n",
    "                    return None\n",
    "                \n",
    "                # Pop multiple most-recent items\n",
    "                cur.execute(f\"\"\"\n",
    "                    SELECT id, message\n",
    "                    FROM {self.table_name}\n",
    "                    WHERE thread_id = :session_id\n",
    "                    ORDER BY timestamp DESC\n",
    "                    FETCH FIRST :limit ROWS ONLY\n",
    "                \"\"\", {'session_id': self.session_id, 'limit': limit})\n",
    "                \n",
    "                rows = cur.fetchall()\n",
    "                \n",
    "                if not rows:\n",
    "                    return []\n",
    "                \n",
    "                items = []\n",
    "                ids_to_delete = []\n",
    "                \n",
    "                for row in rows:\n",
    "                    item_id, message_clob = row\n",
    "                    message_str = message_clob.read() if hasattr(message_clob, 'read') else str(message_clob)\n",
    "                    items.append(json.loads(message_str))\n",
    "                    ids_to_delete.append(item_id)\n",
    "                \n",
    "                # Delete all items\n",
    "                for item_id in ids_to_delete:\n",
    "                    cur.execute(f\"\"\"\n",
    "                        DELETE FROM {self.table_name}\n",
    "                        WHERE id = :id\n",
    "                    \"\"\", {'id': item_id})\n",
    "                \n",
    "                self.conn.commit()\n",
    "                return items\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error popping item(s): {e}\")\n",
    "            self.conn.rollback()\n",
    "            return None if (not limit or limit <= 1) else []\n",
    "    \n",
    "    async def clear_session(self) -> None:\n",
    "        \"\"\"Clear all items for this session\"\"\"\n",
    "        try:\n",
    "            with self.conn.cursor() as cur:\n",
    "                cur.execute(f\"\"\"\n",
    "                    DELETE FROM {self.table_name}\n",
    "                    WHERE thread_id = :session_id\n",
    "                \"\"\", {'session_id': self.session_id})\n",
    "                \n",
    "                self.conn.commit()\n",
    "                print(f\"‚úÖ Session {self.session_id} cleared successfully.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error clearing session: {e}\")\n",
    "            self.conn.rollback()\n",
    "    \n",
    "    def close(self) -> None:\n",
    "        \"\"\"\n",
    "        Note: Connection is managed externally, so we don't close it here.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079aa0f1",
   "metadata": {},
   "source": [
    "### Basic Example: Agent with Persistent Session Memory\n",
    "\n",
    "These turns demonstrate memory behavior explicitly:\n",
    "1. Introduce a user identity and topic.\n",
    "2. Ask follow-up questions that depend on prior context.\n",
    "3. Remove selected items to test forgetting behavior.\n",
    "4. Clear the session and verify memory reset.\n",
    "\n",
    "This gives a practical template for evaluating memory quality in your own assistants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ef7a559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent\n",
    "research_agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"Research the topic and return the most relevant information.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fb1433a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Oracle session instance\n",
    "session = OracleSession(\n",
    "    session_id=\"conversation_123\", \n",
    "    connection=conn,\n",
    "    table_name=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f4cc72f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hello, Richmond! Great to meet you. As an AI Memory Engineer focusing on LLMs (Large Language Models) and Agent Memory, you‚Äôre working in an exciting and rapidly evolving area. Here‚Äôs a brief summary of the most relevant information and current research directions related to LLMs and Agent Memory:\n",
      "\n",
      "---\n",
      "\n",
      "## Key Concepts\n",
      "\n",
      "### 1. **LLM Memory Limitations**\n",
      "- **Context Window Constraints:** Transformers (BERT, GPT, etc.) have a fixed context window, typically ranging from a few thousand to over 100,000 tokens in cutting-edge models (e.g., Claude 3 or Gemini 1.5 Flash), but still limited.\n",
      "- **Long-term Memory:** LLMs do not have persistent, editable long-term memory. All memory is in parameters or transient context.\n",
      "\n",
      "### 2. **Agent Memory Architectures**\n",
      "- **Episodic Memory:** Storing and recalling distinct past interactions or events.\n",
      "- **Semantic Memory:** General knowledge an agent acquires over time.\n",
      "- **Working Memory:** Short-term storage, e.g., the context window or buffer.\n",
      "- **External/Tool-Augmented Memory:** Storing knowledge in external databases, knowledge graphs, or retrievable document stores (e.g., RAG‚ÄîRetrieval-Augmented Generation).\n",
      "\n",
      "### 3. **Key Techniques**\n",
      "- **RAG (Retrieval-Augmented Generation):** Supplements LLMs by retrieving relevant documents from an external store at inference time.\n",
      "- **Vector Databases:** Using embeddings to index and retrieve past interactions or knowledge.\n",
      "- **Memory Networks & Episodic Buffers:** Neural methods for integrating memory modules into architectures.\n",
      "- **Agent Frameworks:** LangChain, LlamaIndex, and OpenAI‚Äôs function calling support integrating memory into LLM-driven agents.\n",
      "\n",
      "## Notable Research & Applications\n",
      "\n",
      "1. **LLMs with Tool-use & Persistent Memory**\n",
      "   - Combining LLMs with knowledge bases or user profiles.\n",
      "   - *Example:* OpenAI‚Äôs ‚ÄúChatGPT with Memory‚Äù feature for enterprise and custom GPTs.\n",
      "\n",
      "2. **Agentic LLMs**\n",
      "   - Autonomous AI agents with memory systems to support reasoning over time.\n",
      "   - *Example:* AutoGPT, BabyAGI, and recent work from Microsoft (‚ÄúTaskWeaver‚Äù) and DeepMind (‚ÄúGenerative Agents‚Äù).\n",
      "\n",
      "3. **Long Context Transformers**\n",
      "   - New architectural breakthroughs (e.g., Mamba, RWKV, Longformer) facilitating much larger context windows, supporting more explicit context retention.\n",
      "\n",
      "4. **Memory-augmented LLM Components**\n",
      "   - **Prompt Engineering:** Chunking memory into prompts, summarizing past interactions.\n",
      "   - **Retrieval Systems:** Leveraging semantic search to augment memory.\n",
      "   - **Event Summarization & Compression:** Summarizing and archiving previous conversations for re-insertion.\n",
      "\n",
      "5. **Evolving Standards**\n",
      "   - **LlamaIndex Memory Interfaces:** Standardized APIs for episodic, summary, and semantic memory.\n",
      "   - **LangChain Memory Classes:** Extensible memory strategies‚Äîsimple ‚ÄòConversationBuffer‚Äô or sophisticated retriever-based memory.\n",
      "\n",
      "## Open Challenges\n",
      "\n",
      "- Scaling memory retrieval as the dataset grows (latency, relevance).\n",
      "- Preventing memory drift, hallucinations, or privacy violations.\n",
      "- Memory consolidation and summarization without losing context.\n",
      "- Interleaving short-term ‚Äúworking memory‚Äù with large-scale ‚Äúepisodic/semantic‚Äù recall.\n",
      "\n",
      "## Start Here for Research:\n",
      "\n",
      "- Papers:\n",
      "  - [Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442)\n",
      "  - [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)\n",
      "  - [LongNet: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/abs/2307.02486)\n",
      "\n",
      "- Toolkits:\n",
      "  - [LangChain Memory Docs](https://python.langchain.com/docs/modules/memory/)\n",
      "  - [LlamaIndex Memory Strategies](https://docs.llamaindex.ai/en/stable/module_guides/storing_data/memory/)\n",
      "\n",
      "- Community:\n",
      "  - arXiv preprints with keywords *LLM memory*, *episodic memory in agents*, *retrieval-augmented LLMs*.\n",
      "  - OpenAI and DeepMind recent blog posts.\n",
      "\n",
      "---\n",
      "\n",
      "If you have a specific aspect of LLM memory or agent memory (architecture, implementation, evaluation, benchmarks, etc.) you want to dive deeper into, let me know!\n"
     ]
    }
   ],
   "source": [
    "# First turn\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"Hi my name is Richmond, and I am a AI Memory Engineer researching LLMs and Agent Memory\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f1ea86e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The seminal paper that **introduces the attention mechanism**‚Äîwhich forms the foundational architecture for LLMs (Large Language Models)‚Äîis:\n",
      "\n",
      "---\n",
      "\n",
      "### \"**Attention Is All You Need**\"\n",
      "- **Authors:** Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin\n",
      "- **Conference:** NeurIPS 2017 (formerly NIPS)\n",
      "- **arXiv:** [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)\n",
      "\n",
      "---\n",
      "\n",
      "#### **Summary:**\n",
      "This paper introduces the **Transformer architecture**, which relies solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Self-attention as described in this paper is the backbone for all modern LLMs, such as BERT, GPT, T5, and others.\n",
      "\n",
      "#### **Key Contributions:**\n",
      "- **Self-Attention:** Mechanism that allows the model to weigh the importance of different tokens for each token being processed.\n",
      "- **Multi-Head Attention:** Parallelizes attention mechanisms to allow the model to focus on different parts of the input.\n",
      "- **Scalability:** The architecture scales effectively with more data and compute, leading to massive LLMs.\n",
      "\n",
      "---\n",
      "\n",
      "If you‚Äôre researching LLM memory or architectures, this is the foundational text to cite and review.\n"
     ]
    }
   ],
   "source": [
    "# Second turn\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"What is a paper that introduces the attention mechanism in LLMs?\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "144c2920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The authors of \"**Attention Is All You Need**\" are:\n",
      "\n",
      "1. **Ashish Vaswani**\n",
      "2. **Noam Shazeer**\n",
      "3. **Niki Parmar**\n",
      "4. **Jakob Uszkoreit**\n",
      "5. **Llion Jones**\n",
      "6. **Aidan N. Gomez**\n",
      "7. **Lukasz Kaiser**\n",
      "8. **Illia Polosukhin**\n",
      "\n",
      "This research was primarily conducted at Google Brain and Google Research.\n"
     ]
    }
   ],
   "source": [
    "# Third turn, the agent will remember the previous conversation\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"Who were the authors of the paper?\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "22f955bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The year of publication for the paper **\"Attention Is All You Need\"** was **2017**.\n"
     ]
    }
   ],
   "source": [
    "# Fourth turn - continuing the conversation\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"What was the year of publication?\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e1d64efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'msg_03f8fa5fd09e295800698d53049bf8819596e31265565416e6',\n",
       "  'content': [{'annotations': [],\n",
       "    'text': 'The year of publication for the paper **\"Attention Is All You Need\"** was **2017**.',\n",
       "    'type': 'output_text',\n",
       "    'logprobs': []}],\n",
       "  'role': 'assistant',\n",
       "  'status': 'completed',\n",
       "  'type': 'message'},\n",
       " {'content': 'What was the year of publication?', 'role': 'user'},\n",
       " {'id': 'msg_03f8fa5fd09e295800698d5302993081959ae2cd9ffeb065a8',\n",
       "  'content': [{'annotations': [],\n",
       "    'text': 'The authors of \"**Attention Is All You Need**\" are:\\n\\n1. **Ashish Vaswani**\\n2. **Noam Shazeer**\\n3. **Niki Parmar**\\n4. **Jakob Uszkoreit**\\n5. **Llion Jones**\\n6. **Aidan N. Gomez**\\n7. **Lukasz Kaiser**\\n8. **Illia Polosukhin**\\n\\nThis research was primarily conducted at Google Brain and Google Research.',\n",
       "    'type': 'output_text',\n",
       "    'logprobs': []}],\n",
       "  'role': 'assistant',\n",
       "  'status': 'completed',\n",
       "  'type': 'message'},\n",
       " {'content': 'Who were the authors of the paper?', 'role': 'user'},\n",
       " {'id': 'msg_03f8fa5fd09e295800698d52fc24a481959baeb3d823eaa266',\n",
       "  'content': [{'annotations': [],\n",
       "    'text': 'The seminal paper that **introduces the attention mechanism**‚Äîwhich forms the foundational architecture for LLMs (Large Language Models)‚Äîis:\\n\\n---\\n\\n### \"**Attention Is All You Need**\"\\n- **Authors:** Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin\\n- **Conference:** NeurIPS 2017 (formerly NIPS)\\n- **arXiv:** [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)\\n\\n---\\n\\n#### **Summary:**\\nThis paper introduces the **Transformer architecture**, which relies solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Self-attention as described in this paper is the backbone for all modern LLMs, such as BERT, GPT, T5, and others.\\n\\n#### **Key Contributions:**\\n- **Self-Attention:** Mechanism that allows the model to weigh the importance of different tokens for each token being processed.\\n- **Multi-Head Attention:** Parallelizes attention mechanisms to allow the model to focus on different parts of the input.\\n- **Scalability:** The architecture scales effectively with more data and compute, leading to massive LLMs.\\n\\n---\\n\\nIf you‚Äôre researching LLM memory or architectures, this is the foundational text to cite and review.',\n",
       "    'type': 'output_text',\n",
       "    'logprobs': []}],\n",
       "  'role': 'assistant',\n",
       "  'status': 'completed',\n",
       "  'type': 'message'},\n",
       " {'content': 'What is a paper that introduces the attention mechanism in LLMs?',\n",
       "  'role': 'user'},\n",
       " {'id': 'msg_03f8fa5fd09e295800698d52e8d18c81959e4adad41923ff36',\n",
       "  'content': [{'annotations': [],\n",
       "    'text': 'Hello, Richmond! Great to meet you. As an AI Memory Engineer focusing on LLMs (Large Language Models) and Agent Memory, you‚Äôre working in an exciting and rapidly evolving area. Here‚Äôs a brief summary of the most relevant information and current research directions related to LLMs and Agent Memory:\\n\\n---\\n\\n## Key Concepts\\n\\n### 1. **LLM Memory Limitations**\\n- **Context Window Constraints:** Transformers (BERT, GPT, etc.) have a fixed context window, typically ranging from a few thousand to over 100,000 tokens in cutting-edge models (e.g., Claude 3 or Gemini 1.5 Flash), but still limited.\\n- **Long-term Memory:** LLMs do not have persistent, editable long-term memory. All memory is in parameters or transient context.\\n\\n### 2. **Agent Memory Architectures**\\n- **Episodic Memory:** Storing and recalling distinct past interactions or events.\\n- **Semantic Memory:** General knowledge an agent acquires over time.\\n- **Working Memory:** Short-term storage, e.g., the context window or buffer.\\n- **External/Tool-Augmented Memory:** Storing knowledge in external databases, knowledge graphs, or retrievable document stores (e.g., RAG‚ÄîRetrieval-Augmented Generation).\\n\\n### 3. **Key Techniques**\\n- **RAG (Retrieval-Augmented Generation):** Supplements LLMs by retrieving relevant documents from an external store at inference time.\\n- **Vector Databases:** Using embeddings to index and retrieve past interactions or knowledge.\\n- **Memory Networks & Episodic Buffers:** Neural methods for integrating memory modules into architectures.\\n- **Agent Frameworks:** LangChain, LlamaIndex, and OpenAI‚Äôs function calling support integrating memory into LLM-driven agents.\\n\\n## Notable Research & Applications\\n\\n1. **LLMs with Tool-use & Persistent Memory**\\n   - Combining LLMs with knowledge bases or user profiles.\\n   - *Example:* OpenAI‚Äôs ‚ÄúChatGPT with Memory‚Äù feature for enterprise and custom GPTs.\\n\\n2. **Agentic LLMs**\\n   - Autonomous AI agents with memory systems to support reasoning over time.\\n   - *Example:* AutoGPT, BabyAGI, and recent work from Microsoft (‚ÄúTaskWeaver‚Äù) and DeepMind (‚ÄúGenerative Agents‚Äù).\\n\\n3. **Long Context Transformers**\\n   - New architectural breakthroughs (e.g., Mamba, RWKV, Longformer) facilitating much larger context windows, supporting more explicit context retention.\\n\\n4. **Memory-augmented LLM Components**\\n   - **Prompt Engineering:** Chunking memory into prompts, summarizing past interactions.\\n   - **Retrieval Systems:** Leveraging semantic search to augment memory.\\n   - **Event Summarization & Compression:** Summarizing and archiving previous conversations for re-insertion.\\n\\n5. **Evolving Standards**\\n   - **LlamaIndex Memory Interfaces:** Standardized APIs for episodic, summary, and semantic memory.\\n   - **LangChain Memory Classes:** Extensible memory strategies‚Äîsimple ‚ÄòConversationBuffer‚Äô or sophisticated retriever-based memory.\\n\\n## Open Challenges\\n\\n- Scaling memory retrieval as the dataset grows (latency, relevance).\\n- Preventing memory drift, hallucinations, or privacy violations.\\n- Memory consolidation and summarization without losing context.\\n- Interleaving short-term ‚Äúworking memory‚Äù with large-scale ‚Äúepisodic/semantic‚Äù recall.\\n\\n## Start Here for Research:\\n\\n- Papers:\\n  - [Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442)\\n  - [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)\\n  - [LongNet: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/abs/2307.02486)\\n\\n- Toolkits:\\n  - [LangChain Memory Docs](https://python.langchain.com/docs/modules/memory/)\\n  - [LlamaIndex Memory Strategies](https://docs.llamaindex.ai/en/stable/module_guides/storing_data/memory/)\\n\\n- Community:\\n  - arXiv preprints with keywords *LLM memory*, *episodic memory in agents*, *retrieval-augmented LLMs*.\\n  - OpenAI and DeepMind recent blog posts.\\n\\n---\\n\\nIf you have a specific aspect of LLM memory or agent memory (architecture, implementation, evaluation, benchmarks, etc.) you want to dive deeper into, let me know!',\n",
       "    'type': 'output_text',\n",
       "    'logprobs': []}],\n",
       "  'role': 'assistant',\n",
       "  'status': 'completed',\n",
       "  'type': 'message'}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the conversation subject and ensure the agent does't remember the previous conversation\n",
    "# Specifiying pop without limit will remove the last item in the session\n",
    "await session.pop_item(limit=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3a7d9dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hi Richmond! As of now, you haven't mentioned a specific paper in our conversation. If you have a particular paper in mind regarding **LLMs (Large Language Models)** and **Agent Memory**, please provide the title or a description. I can then discuss its content, relevance, and key findings, or help you find important papers in this research area.\n",
      "\n",
      "If you‚Äôre looking for suggestions, some influential papers related to LLM memory and agent memory include:\n",
      "\n",
      "- **\"Memory in Language Models: An Empirical Study of Long-Context Retention\"** (NeurIPS 2023)  \n",
      "- **\"Toolformer: Language Models Can Teach Themselves to Use Tools\"** (Schick et al., 2023)  \n",
      "- **\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"** (Lewis et al., 2020)  \n",
      "- **\"LongNet: Scaling LLMs to 1 Billion Tokens\"** (Ding et al., 2023)  \n",
      "- **\"Memorizing Transformers\"** (Wu et al., 2022)\n",
      "\n",
      "Let me know what you‚Äôd like to focus on, and I can dive deeper!\n"
     ]
    }
   ],
   "source": [
    "# Fifth turn: The agent should not remember the conversations about the paper\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"What paper have we been talking about?\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c06a4705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Yes, your name is Richmond. If you‚Äôd like, I can continue to use your name in our conversation. Let me know how I can assist you further in your research on LLMs and agent memory!\n"
     ]
    }
   ],
   "source": [
    "# Because we limited the session to a few items, the agent should still remember our name at the introduction\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"Do you still remember my name?\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3694ed8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Session conversation_123 cleared successfully.\n"
     ]
    }
   ],
   "source": [
    "# Clear the session\n",
    "await session.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cd2169c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: I don‚Äôt have access to previous conversations or personal data unless you share that information during our chat. So unless you've just told me your name in this conversation, I don't know it. If you'd like me to use your name, feel free to tell me!\n"
     ]
    }
   ],
   "source": [
    "# Because we limited the session to 3 items, the agent should still remember our name at the introduction\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"Do you still remember my name?\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
